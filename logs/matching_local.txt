2022-07-12 12:59:22.005921: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2022-07-12 12:59:22.006227: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2022-07-12 12:59:22.006387: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2022-07-12 12:59:22.006557: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2022-07-12 12:59:22.006707: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2022-07-12 12:59:22.006876: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2022-07-12 12:59:22.007028: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2022-07-12 12:59:22.007090: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0712 12:59:22.007672 139911399675712 train.py:67] ===========Config Dict============
I0712 12:59:22.007965 139911399675712 train.py:68] available_devices:
- 0
- 1
- 2
- 3
base_type: dual_encoder
batch_size: 4
checkpoint_freq: 10000
data_dir: google_datasets/doc_retrieval/tsv_data/
emb_dim: 128
eval_frequency: 200
factors: constant * linear_warmup * rsqrt_decay
learning_rate: 0.05
max_eval_target_length: 200
max_length: 4000
max_predict_token_length: 50
max_target_length: 200
mlp_dim: 512
model_type: local
num_eval_steps: 50
num_heads: 4
num_layers: 4
num_train_steps: 1001
pooling_mode: CLS
prompt: ''
qkv_dim: 128
random_seed: 0
restore_checkpoints: true
sampling_temperature: 0.6
sampling_top_k: 20
save_checkpoints: true
task_name: aan_pairs
tokenizer: char
trial: 0
vocab_file_path: google_datasets/doc_retrieval/aan/
warmup: 1000
weight_decay: 0.1

I0712 12:59:22.019876 139911399675712 xla_bridge.py:340] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0712 12:59:23.250025 139911399675712 xla_bridge.py:340] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0712 12:59:23.250691 139911399675712 xla_bridge.py:340] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0712 12:59:23.250912 139911399675712 train.py:80] GPU devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)]
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
I0712 12:59:23.251066 139911399675712 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
I0712 12:59:23.330035 139911399675712 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 12:59:23.359238 139911399675712 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 12:59:25.709290 139911399675712 input_pipeline.py:60] Data sample: OrderedDict([('label', 0.0), ('id1', b'J98-2002'), ('id2', b'P12-1090'), ('text1', b'b\'1. Introduction  We address the problem of automatically acquiring case frame patterns (selectional  patterns, subcategorization patterns) from large corpora. A satisfactory solution to this  problem would have a great impact on various tasks in natural anguage processing,  including the structural disambiguation problem in parsing. The acquired knowledge  would also be helpful for building a lexicon, as it would provide lexicographers with  word usage descriptions.  In our view, the problem of acquiring case frame patterns involves the following  two issues: (a) acquiring patterns of individual case frame slots; and (b) learning  dependencies that may exist between different slots. In this paper, we confine ourselves  to the former issue, and refer the interested reader to Li and Abe (1996), which deals  with the latter issue.  The case frame (case slot) pattern acquisition process consists of two phases: extrac-  tion of case frame instances from corpus data, and generalization of those instances to  case frame patterns. The generalization step is needed in order to represent the input  case frame instances more compactly as well as to judge the (degree of) acceptability  of unseen case frame instances. For the extraction problem, there have been various  methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grish-  man and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent  1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization  problem, in contrast, is a more challenging one and has not been solved completely. A  number of methods for generalizing values of a case frame slot for a verb have been  * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan.  E-mail: { |ihang,abe }@ccm.cl.nec.co.jp  @ 1998 Association for Computational Linguistics  Computational Linguistics Volume 24, Number 2  proposed. Some of these methods make use of prior knowledge in the form of an  existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al 1994; Tanaka  1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowl-  edge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this  paper, we propose a new generalization method, belonging to the first of these two  categories, which is both theoretically well-motivated and computationally efficient.  Specifically, we formalize the problem of generalizing values of a case frame slot  for a given verb as that of estimating a conditional probability distribution over a  partition of words, and propose a new generalization method based on the Minimum  Description Length principle (MDL): a principle of data compression and statistical  estimation from information theory. 1 In order to assist with efficiency, our method  makes use of an existing thesaurus and restricts its attention on those partitions that  are present as "cuts" in the thesaurus tree, thus reducing the generalization problem  to that of estimating a "tree cut model" of the thesaurus tree. We then give an efficient algorithm that provably obtains the optimal tree cut model for the given frequency data  of a case slot, in the sense of MDL. In order to test the effectiveness of our method, we  conducted PP-attachment disambiguation experiments using the case frame patterns  obtained by our method. Our experimental results indicate that the proposed method  improves upon or is at least comparable to existing methods.  The remainder of this paper is organized as follows: In Section 2, we formalize the  problem of generalizing values of a case frame slot as that of estimating a conditional  distribution. In Section 3, we describe our MDL-based generalization method. In Sec-  tion 4, we present our experimental results. We then give some concluding remarks  in Section 5.  2. The Problem  2.1 The Data Sparseness Problem  Suppose that the data available to us are of the type shown in Table 1, which are slot  values for a given verb (verb,slot_name,slot_value triples) automatically extracted from  a corpus using existing techniques. By counting the frequency of occurrence of each  noun at a given slot of a verb, the frequency data shown in Figure 1 can be obtained.  We will refer to this type of data as co-occurrence data. The problem of generalizing  values of a case frame slot for a verb (or, in general, a head) can be viewed as the  problem of learning the underlying conditional probability distribution that gives  rise to such co-occurrence data. Such a conditional distribution can be represented by  a probability model that specifies the conditional probability P(n I v, r) for each n in  the set of nouns .M = {nl, n2 . . . . .  nN}, V in the set of verbs V = {vl, v2 . . . . .  Vv}, and r  in the set of slot names T~ = {rl, r2 . . . . .  rR}, satisfying:  P(n Iv, r) = 1. (1)  nGM  This type of probability model is often referred to as a word-based model. Since the  number of probability parameters in word-based models is large (O(N. V. R)), accurate  1 Recently, MDL and related techniques have become popular in corpus-based natural anguage  processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and  Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright  1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern  acquisition.  218  Li and Abe Generalizing Case Frames  Table 1  Example (verb, slot_name,  slot_value) triple data.  verb slot_name slot_value  fly argl bee  fly argl bird  fly argl bird  fly argl crow  fly argl bird  fly argl eagle  fly argl bee  fly argl eagle  fly argl bird  fly argl crow  "Freq." - -   swallow crow eaglebird bug bee insect  Figure 1  Frequency data for the subject slot of verb fly.  estimation of a word-based model is difficult with the data size that is available in  practice--a problem usually referred to as the data sparseness problem. For example,  suppose that we employ the maximum-likelihood estimation (or MLE for short) to  estimate the probability parameters of a conditional probability distribution, as de-  scribed above, given the co-occurrence data in Figure 1. In this case, MLE amounts  to estimating the parameters by simply normalizing the frequencies so that they sum  to one, giving, for example, the estimated probabilities of 0, 0.2, and 0.4 for swallow,  eagle, and bird, respectively (see Figure 2). Since in general the number of parameters  exceeds the size of data that is typically available, MLE will result in estimating most  of the probability parameters to be zero.  To address this problem, Grishman and Sterling (1994) proposed a method of  smoothing conditional probabilities using the probability values of similar words,  where the similarity between words is judged based on co-occurrence data (see also  Dagan, Marcus, and Makovitch \\\\[1992\\\\] and Dagan, Pereira, and Lee \\\\[1994\\\\]). More  specifically, conditional probabilities of words are smoothed by taking the weighted  average of those of similar words using the similarity measure as the weights. The  advantage of this approach is that it does not rely on any prior knowledge, but it  appears difficult to find a smoothing method that is both efficient and theoretically  sound. As an alternative, a number of authors have proposed the use of class-based  219  Computational Linguistics Volume 24, Number 2  "Prob." - -   0.45  0.4  0.35  0.3  0.25  0.2  0.15  0.1  0.05  0  swallow crow eagle bird bug bee insect  Figure 2  Word-based istribution estimated using MLE.  models, which assign (conditional) probability values to (existing) classes of words,  rather than individual words.  2.2 Class-based Models  An example of the class-based approach is Resnik\\\'s method of generalizing values of  a case frame slot using a thesaurus and the so-called selectional association measure  (Resnik 1993a, 1993b). The selectional association, denoted A(C I v, r), is defined as  follows:  P(CIv,  r) (2) A(C I v, F) = P(C I v, F) x log P(C)  where C is a class of nouns present in a given thesaurus, v is a verb and r is a slot name,  as described earlier. In generalizing a given noun n to a noun class, this method selects  the noun class C having the maximum A(C I v, r), among all super classes of n in a  given thesaurus. This method is based on an interesting intuition, but its interpretation  as a method of estimation is not clear. We propose a class-based generalization method  whose performance as a method of estimation is guaranteedto be near optimal.  We define the class-based model as a model that consists of a partition of the set  .N" of nouns, and a parameter associated with each member of the partition. Here, a  partition F of .M is any collection of mutually disjoint subsets of iV" that exhaustively  cover N.  The parameters specify the conditional probability P(C I v, r) for each class  (subset) C in that partition, such that  P(CIv,  r) = 1. (3)  CEF  Within a given class C, it is assumed that each noun is generated with equal probability,  namely  1 Vn E C: P(n l v, r) = ~ x P(C I v, F). (4)  Here, we assume that a word belongs to a single class. In practice, however,  many words have sense ambiguity and a word can belong to several different classes,  e.g., bird is a member of both BIRD and MEAT. Thorough treatment of this problem  is beyond the scope of the present paper; we simply note that one can employ an  existing word-sense disambiguation technique (e.g.,Yarowsky 1992, 1994) in prepro-  cessing, and use the disambiguated word senses as virtual words in the following  220  Li and Abe Generalizing Case Frames  ANIMAL  BIRD INSECT  swallow crow eagle bird bug bee insect  Figure 3  An example thesaurus.  case-pattern acquisition process. It is also possible to extend our model so that each  word probabilistically belongs to several different classes, which would allow us to  resolve both structural and word-sense ambiguities at the time of disambiguation. 2  Employing probabilistic membership, however, would make the estimation process  significantly more computationally demanding. We therefore leave this issue as a fu-  ture topic, and employ a simple heuristic of equally distributing each word occurrence  in the data to all of its potential word senses in our experiments. Since our learning  method based on MDL is robust against noise, this should not significantly degrade  performance.  2.3 The Tree Cut Model   Since the number of partitions for a given set of nouns is extremely large, the problem  of selecting the best model from among all possible class-based models is most likely  intractable. In this paper, we reduce the number of possible partitions to consider by  using a thesaurus as prior knowledge, following a basic idea of Resnik\\\'s (1992).  In particular, we restrict our attention to those partitions that exist within the  thesaurus in the form of a cut. By thesaurus, we mean a tree in which each leaf node  stands for a noun, while each internal node represents a noun class, and domination  stands for set inclusion (see Figure 3). A cut in a tree is any set of nodes in the tree  that defines a partition of the leaf nodes, viewing each node as representing the set  of all leaf nodes it dominates. For example,in the thesaurus of Figure 3, there are  five cuts: \\\\[ANIMAL\\\\], \\\\[BIRD, INSECT\\\\], \\\\[BIRD, bug, bee, insect\\\\], \\\\[swallow, crow, eagle,  bird, INSECT\\\\], and \\\\[swallow, crow, eagle, bird, bug, bee, insect\\\\]. The class of tree cut  models of a fixed thesaurus tree is then obtained by restricting the partition P in the  definition of a class-based model to be those partitions that are present as a cut in that  thesaurus tree.  Formally, a tree cut model M can be represented by a pair consisting of a tree cut  lP and a probability parameter vector 0 of the same length, that is:  V = (r, e) (5)  where lP and 0 are:  r = \\\\[C1, C2 . . . . .  Ck+l\\\\], e = \\\\[P(C1), P(C2) . . . . .  P(Ck+l)\\\\] (6)  k+l where C1, C2 . . . . .  Ck+l is a cut in the thesaurus tree and ~i=1 P(Ci) = 1 is satisfied.  For simplicity we sometimes write P(Ci), i = 1 . . . . .  (k + 1) for P(Ci \\\\[ v, r).  If we use MLE for the parameter estimation, we can obtain five tree cut models  from the co-occurrence data in Figure 1; Figures 4-6 show three of these. For example,  2 The model used by Pereira, Tishby, and Lee (1993) is indeed along this direction.  221  Computational Linguistics Volume 24, Number 2  0.45  0.4  0.35  0.3  0.25  0.2  0.15  0.1  0.05  0  - r~- \\\' - - - - - - -~- -~ ,Prob r-\\\'~--  swallow crow eagle bird bug bee insect  Figure 4  A tree cut model with \\\\[swallow, crow, eagle, bird, bug, bee, insect\\\\].  0.45  0.4  0.35  0.3  0.25  0.2  0.15  0.1  0.05  0  "Prob."  swa\\\'ll . . . .  ow eagle bi\\\'rd bug bee ins\\\'ect  Figure 5  A tree cut model with \\\\[BIRD, bug, bee, insect\\\\].  ~- (\\\\[BIRD, bug, bee, insect\\\\], \\\\[0.8,0,0.2,0\\\\]) shown in Figure 5 is one such tree cut  model. Recall that M defines a conditional probability distribution PM(n I v,r) as  follows: For any noun that is in the tree cut, such as bee, the probability is given as  explicitly specified by the model, i.e., PM(bee I flY, argl) = 0.2. For any class in the tree  cut, the probability is distributed uniformly to all nouns dominated by it. For example,  since there are four nouns that fall under the class BIRD, and swallow is one of them,  the probability of swallow is thus given by Pt~(swallow I flY, argl) = 0.8/4 = 0.2. Note  that the probabilities assigned to the nouns under BIRD are smoothed, even if the  nouns have different observed frequencies.  We have thus formalized the problem of generalizing values of a case frame slot as that of estimating a model from the class of tree cut models for some fixed thesaurus  tree; namely, selecting a model that best explains the data from among the class of  tree cut models.  3. Generalization Method Based On MDL  The question now becomes what strategy (criterion) we should employ to select the best  tree-cut model. We adopt the Minimum Description Length principle (Rissanen 1978,  222  Li and Abe Generalizing Case Frames  0.45  0.4  0,35  0,3  0.25  0.2  O,lS  0.1  0,05  0 swallow crow eagle bi\\\'rd  Figure 6  A tree cut model with \\\\[BIRD, INSECT\\\\].  "Prob." - -   Table 2  Number of parameters and KL distance from the empirical distribution for the five  tree cut models.  P Number of Parameters KL Distance  \\\\[ANIMAL\\\\]  \\\\[BIRD, INSECT\\\\]  \\\\[BIRD, bug, bee, insect\\\\]  \\\\[swallow, crow, eagle, bird, INSECT\\\\]  \\\\[swallow, crow, eagle, bird, bug, bee, insect\\\\]  0 0.89  1 0.72  3 0.4  4 0.32  6 0  1983, 1984, 1986, 1989), which has various desirable properties, as will be described  later. 3  MDL is a principle of data compression and statistical estimation from informa-  tion theory, which states that the best probability model for given data is that which  requires the least code length in bits for the encoding of the model itself and the given  data observed through it. 4 The former is the model description length and the latter  the data description length.  In our current problem, it tends to be the case, in general, that a model nearer the  root of the thesaurus tree, such as that in Figure 6, is simpler (in terms of the number  of parameters), but tends to have a poorer fit to the data. In contrast, a model nearer  the leaves of the thesaurus tree, such as that in Figure 4, is more complex, but tends  to have a better fit to the data. Table 2 shows the number of free parameters and  the KL distance from the empirical distribution of the data (namely, the word-based  distribution estimated by MLE) shown in Figure 2 for each of the five tree cut models. 5 In the table, one can see that there is a trade-off between the simplicity of a model  and the goodness of fit to the data.  In the MDL framework, the model description length is an indicator of model  3 Estimation strategies related to MDL have been independently proposed and studied by various  authors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992).  4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction tothe MDL principle.  5 The KL distance (alsO known as KL-divergence orrelative ntropy), which is widely used in  information theory and statistics, is a measure of distance between two distributions (e.g.,Cover and  Thomas 1991). It is always normegative and is zero if and only if the two distributions are identical,  but is asymmetric and hence not a metric (the usual notion of distance).  223  Computational Linguistics Volume 24, Number 2  complexity, while the data description length indicates goodness of fit to the data. The  MDL principle stipulates that the model that minimizes the sum total of the description  lengths hould be the best model (both for data compression and statistical estimation).  In the remainder of this section, we will describe how we apply MDL to our  current problem. We will then discuss the rationale behind using MDL in our present  context.  3.1 Calculat ing Descr ipt ion Length  We first show how the description length for a model is calculated. We use S to  denote a sample (or set of data), which is a multiset of examples, each of which is an  occurrence of a noun at a given slot r of a given verb v (i.e., duplication is allowed).  We let ISI denote the size of S as a multiset, and n E S indicate the inclusion of n  in S as a multiset. For example, the column labeled slot_value in Table 1 represents a  sample S for the subject slot offly, and in this case ISI = 10.  Given a sample S and a tree cut F, we employ MLE to estimate the parame-  ters of the corresponding tree cut model ~,I = (F, 0), where 6 denotes the estimated  parameters.  The total description length L(/~,I, S) of the tree cut model/vl and the sample S  observed through M is computed as the sum of the model description length L(P),  parameter description length L(0 I P), and data description length L(S I F, 6):  L(M,S) = L((F,6),S) = L(r) + L(6 I r)  +L(Str,6). (7)  Note that we sometimes refer to L(F) + L(0 I F) as the model description length.  The model description length L(F) is a subjective quantity, which depends on the  coding scheme mployed. Here, we choose to assign the same code length to each cut  and let:  L(F) = log IG\\\\[ (8)  where ~ denotes the set of all cuts in the thesaurus tree T. 6 This corresponds to assum-  ing that each tree cut model is equally likely a priori, in the Bayesian interpretation of MDL. (See Section 3.4.)  The parameter description length L(O I F) is calculated by:  k L(0 I r) = ~ x log IsI (9)  where ISI denotes the sample size and k denotes the number of free parameters in the  tree cut model, i.e., k equals the number of nodes in P minus one. It is known to be  best to use this number of bits to describe probability parameters in order to minimize  the expected total description length (Rissanen 1984, 1986).An intuitive explanation  of this is that the standard deviation of the maximum-likelihood estimator of each  parameter is of the order ~,  and hence describing each parameter using more than  1 1 log ISI bits would be wasteful for the estimation accuracy possible with - log x /~ - 2  the given sample size.  Finally, the data description length L(S I F, 0) is calculated by:  L(S I r, 0) = - ~ log P(n) (10)  nES  6 Here and throughout, log denotes the logarithm to the base 2. For reasons why Equation 8 holds, see,  for example, Quinlan and Rivest (1989).  224  Li and Abe Generalizing Case Frames  Table 3  Calculating the description length for the  model of Figure 5.  C B IRD bug bee insect  f(C) 8 0 2 0  ICI 4 1 1 1  P(C) 0.8 0.0 0.2 0.0  P(n) 0.2 0.0 0.2 0.0  P \\\\[BIRD, bug, bee, insect\\\\]  L(0 1 r) (47l) x log 10 = 4.98  L(S I P,~) - (2+4+2+2)  x log0.2 = 23.22  where for simplicity we write P(n) for PM(n \\\\[ v, r). Recall that P(n) is obtained by  MLE, namely, by normalizing the frequencies:  1 P(n) = ~ x P(C) (11)  for each C c P and each n E C, where for each C c P:  = d(C)  (12)  ISI  wheref(C) denotes the total frequency of nouns in class C in the sample S, and F is a  tree cut. We note that, in fact, the maximum-likelihood estimate is one that minimizes  the data description length L(S I F, 0).  With description length defined in the above manner, we wish to select a model  with the minimum description length and output it as the result of generalization.  Since we assume here that every tree cut has an equal L(P), technically we need only  calculate and compare L\\\'(/\\\\[d, S) = L(~ I F) + L(S t F, ~) as the description length. For  simplicity, we will sometimes write just L\\\'(F) for L\\\'(7\\\\[/I, S), where I ~ is the tree cut of  M, when ~,I and S are clear from context.  The description lengths for the data in Figure 1 using various tree cut models  of the thesaurus tree in Figure 3 are shown in Table 4. (Table 3 shows how the de-  scription length is calculated for the model of tree cut \\\\]BIRD, bug, bee, insect\\\\].) These  figures indicate that the model in Figure 6 is the best model, according to MDL. Thus,  given the data in Table 1 as input, the generalization result shown in Table 5 is ob-  tained.  3.2 An Efficient Algorithm  In generalizing values of a case flame slot using MDL, we could, in principle, calculate  the description length of every possible tree cut model and output a model withthe  minimum description length as the generalization result, if computation time were of  no concern. But since the number of cuts in a thesaurus tree is exponential in the size  of the tree (for example, it is easy to verify that for a complete b-ary tree of depth d it is  of the order o(2ba-1)), it is impractical to do so. Nonetheless, we were able to devise a  225  Computational Linguistics Volume 24, Number 2  Table 4  Description length of the five tree cut models.  r L(~ I r) L(S \\\\] r,~) L\\\'(P)  \\\\[ANIMAL\\\\] 0 28.07 28.07  \\\\[BIRD, INSECT\\\\] 1.66 26.39 28.05  \\\\[BIRD, bug, bee, insect\\\\] 4.98 23.22 28.20  \\\\[swallow, crow, eagle, bird, INSECT\\\\] 6.64 22.39 29.03  \\\\[swallow, crow, eagle, bird, bug, bee, insect\\\\] 9.97 19.22 29.19  Table 5  Generalization result.  verb slot~name slot_value probability  fly argl BIRD 0.8  fly argl INSECT 0.2  Here we let t denote a thesaurus (sub)tree, root(t) the root of the tree t.  Initially t is set to the entire tree.  Also input to the algorithm is a co-occurrence data.  algorithm Find-MDL(t) := cut  1. if  2. t is a leaf node  3. then  4. retum(\\\\[t\\\\])  5. else  6. For each child tree ti of t ci :=Find-MDL(ti)  7. c:= append(ci)  8. if  9. L\\\'(\\\\[root(t)\\\\]) < L\\\'(c)  10. then  11. return(\\\\[root(t)\\\\])  12. else  13. return(c)  Figure 7  The algorithm: Find-MDL.  simple and efficient algorithm based on dynamic programming, which is guaranteed  to find a model with the minimum description length.  Our algorithm, which we call Find-MDL, recursively finds the optimal MDL model  for each child subtree of a given tree and appends all the optimal models of these sub-  trees and returns the appended models, unless collapsing all the lowerqevel optimal  models into a model consisting of a single node (the root node of the given tree) re-  duces the total description length, in which case it does so. The details of the algorithm  are given in Figure 7. Note that for simplicity we describe Find-MDL as outputting a  tree cut, rather than a complete tree cut model.  Note in the above algorithm that the parameter description length is calculated as  226  Li and Abe Generalizing Case Frames  L\\\'(\\\\[ARTIFACT\\\\])=41.09  L\\\'(\\\\[VEHICLE,AIRPLANE\\\\])=40.97  ENTITY L\\\'(\\\\[AIR PLAN E\\\\])=32.27  r,airplane\\\\])=32.72  RO~iNS A a ~ 5A\\\'C"I . . . . . . . . .  .. 7o.2-.. ? 0.23 BI CT l VEHICLE AIRPLANE  swallow crow eagle bird bug ~lS~"~insect car bike jet helicopter airplane  f(swallow)=4,f(crow)=4,f(eagle)=4,f(bird)=6,f(bee)=8,f(car)=l ,f(jet)=4,f(airplane)=4  Figure 8  An example application of Find-MDL.  log ISI, where k + 1 is the number of nodes in the current cut, both when t is the 2  entire tree and when it is a proper subtree. This contrasts with the fact that thenumber  of free parameters i k for the former, while it is k + 1 for the latter. For the purpose  of finding a tree cut with the minimum description length, however, this distinction  can be ignored (see Appendix A).  Figure 8 illustrates how the algorithm works (on the co-occurrence data shown  at the bottom): In the recursive application of Find-MDL on the subtree rooted at  AIRPLANE, the if-clause on line 9 evaluates to true since L\\\'(\\\\[AIRPLANE\\\\]) = 32.27,  L\\\'(~et, helicopter, airplane\\\\]) = 32.72, and hence \\\\[AIRPLANE\\\\] is returned. Then in the  call to Find-MDL on the subtree rooted at ARTIFACT, the same if-clause evaluates  to false since L\\\'(\\\\[VEHICLE, AIRPLANE\\\\]) = 40.97, L\\\'(\\\\[ARTIFACT\\\\]) = 41.09, and hence  \\\\[VEHICLE, AIRPLANE\\\\] is returned.  Concerning the above algorithm, we show that the following proposition holds:  Proposition 1  The algorithm Find-MDL terminates in time O(N x ISI), where N denotes the number  of leaf nodes in the input thesaurus tree T and ISI denotes the input sample size, and  outputs a tree cut model of T with the minimum description length (with respect o  the encoding scheme described in Section 3.1).  Here we will give an intuitive explanation of why the proposition holds, and give  the formal proof in Appendix A. The MLE of each node (class) is obtained simply by  dividing the frequency of nouns within that class by the total sample size. Thus, the  parameter estimation for each subtree can be done independently from the estimation  of the parameters outside the subtree. The data description length for a subtree thus  depends olely on the tree cut within that subtree, and its calculation can be performed  independently for each subtree. As for the parameter description length for a subtree,  it depends only on the number of classes in the tree cut within that subtree, and hence  can be computed independently as well. The formal proof proceeds by mathematical  induction, which verifies that the optimal model in any (sub)tree is either the model  227  Computational Linguistics Volume 24, Number 2  consisting of the root of the tree or the model obtained by appending the optimal  submodels for its child subtrees. 7  3.3 Estimation, Generalization, and MDL  When a discrete model (a partition F of the set of nouns W" in our present context) is  fixed, and the estimation problem involves only the estimation of probability parame-  ters, the classic maximum-likelihood estimation (MLE) is known to be satisfactory. In particular, the estimation of a word-based model is one such problem, since the parti-  tion is fixed and the size of the partition equals \\\\[.M\\\\[. Furthermore, for a fixed discrete  model, it is known that MLE coincides with MDL: Given data S = {xi : i = 1 . . . . .  m},   MLE estimates parameter P, which maximizes the likelihood with respect to the data;  thatis:  m  = arg mpax H P(xi). (13)  i=1  It is easy to see that P also satisfies:  m  = arg nun ~ - log P(xi). (14)  i=1  This is nothing but the MDL estimate in this case, since ~i~1 -log P(xi) is the data  description length.  When the estimation problem involves model selection, i.e., the choice of a tree cut  in the present context, MDUs behavior significantly deviates from that of MLE. This  is because MDL insists on minimizing the sum total of the data description length  and the model description length, while MLE is still equivalent to minimizing the data  description length only. So, for our problem of estimating a tree cut model, MDL tends  to select a model that is reasonably simple yet fits the data quite well, whereas the  model selected by MLE will be a word-based model (or a tree cut model equivalent  to the word-based modelS), as it will always manage to fit the data.  In statistical terms, the superiority of MDL as an estimation method is related to  the fact we noted earlier that even though MLE can provide the best fit to the given  data, the estimation accuracy of the parameters is poor, when applied on a sample of  modest size, as there are too many parameters to estimate. MLE is likely to estimate  most parameters tobe zero, and thus suffers from the data sparseness problem. Note  in Table 4, that MDL avoids this problem by taking into account the model complexity  as well as the fit to the data.  MDL stipulates that the model with the minimum description length should be  selected both for data compression and estimation. This intimate connection between  estimation and data compression can also be thought of as that between estimation and  generalization, since in order to compress information, generalization is necessary. In  our current problem, this corresponds tothe generalization f individual nouns present  in case frame instances in the data as classes of nouns present in a given thesaurus. For  example, given the thesaurus in Figure 3 and frequency data in Figure 1, we would  7 The process of finding the MDL model tends to be computationally demanding and is often  intractable. When the model class under consideration is restricted to tree structures, however, dynamic  programming is often applicable and the MDL model can be efficiently found. For example, Rissanen  (1995) has devised an algorithm for learning decision trees.  8 Consider, for example, the case when the co-occurrence data is given as  f(swal low) = 2,f(crow) = 2,f(eagle) = 2,f(bird) = 2 for the problem in Section 2.  228  Li and Abe Generalizing Case Frames  like our system to judge that the class BIRD and the noun bee can be the subject slot of  the verb fly. The problem of deciding whether to stop generalizing at BIRDand bee, or  generalizing further to ANIMAL has been addressed by a number of authors (Webster  and Marcus 1989; Velardi, Pazienza, and Fasolo 1991; Nomiyama 1992). Minimization  of the total description length provides a disciplined criterion to do this.  A remarkable fact about MDL is that theoretical findings have indeed verified that  MDL, as an estimation strategy, is near optimal in terms of the rate of convergence of its estimated models to the true model as data size increases. When the true model  is included in the class of models considered, the models selected by MDL converge  to the true model at the rate of O/~C:~9~_i~!~ where k* is the number of parameters in2.1Sl J\\\'  the true model, and \\\\[S\\\\] the data size, which is near optimal (Barron and Cover 1991;  Yamanishi 1992).  Thus, in the current problem, MDL provides (a) a way of smoothing probability  parameters to solve the data sparseness problem, and at the same time, (b) a way  of generalizing nouns in the data to noun classes of an appropriate level, both as a  corollary to the near optimal estimation of the distribution of the given data.  3.4 The Bayesian Interpretation of MDL and the Choice of Encoding Scheme  There is a Bayesian interpretation of MDL: MDL is essentially equivalent to the "pos-  terior mode" in the Bayesian terminology (Rissanen 1989). Given data S and a number  of models, the Bayesian estimator (posterior mode) selects a model M that maximizes  the posterior probability:  = argn~x(P(M).  P(S I M)) (15)  where P(M) denotes the prior probability of the model M and P(S \\\\[ M) the probability  of observing the data S given M. Equivalently, M satisfies  ~\\\'I = argn~m(- logP(M) - logP(S I M)). (16)  This is equivalent to the MDL estimate, if we take - log P(M) to be the model descrip-  tion length. Interpreting - log P(M) as the model description length translates, in the  Bayesian estimation, to assigning larger prior probabilities on simpler models, since it  is equivalent to assuming that P(M) = (?)t(a), where I(M) is the description length of  M. (Note that if we assign uniform prior probability P(M) to all models M, then (15)  becomes equivalent to (13), giving the maximum-likelihood estimate.)  Recall, that in our definition of parameter description length, we assign a shorter  parameter description length to a model with a smaller number of parameters k,  which admits the above interpretation. As for the model description length (for tree  cuts) we assigned an equal code length to each tree cut, which translates to placing  no bias on any cut. We could have employed a different coding scheme assigning  shorter code lengths to cuts nearer the root. We chose not to do so partly because, for  sufficiently large sample sizes, the parameter description length starts dominating the  model description length anyway.  Another important property ofthe definition of description length is that it af-  fects not only the effective prior probabilities on the models, but also the procedure  for computing the model minimizing the measure. Indeed, our definition of model  description length was chosen to be compatible with the dynamic programming tech-  nique, namely, its calculation is performable locally for each subtree. For a different  choice of coding scheme, it is possible that a simple and efficient MDL algorithm like  229  Computational Linguistics Volume 24, Number 2  Find-MDL may not exist. We believe that our choice of model description length is  derived from a natural encoding scheme with reasonable interpretation as Bayesian  prior, and at the same time allows an efficient algorithm for finding a model with the  minimum description length.  3.5 The Uniform Distribution Assumption and the Level of Generalization  The uniform distribution assumption made in (4), namely that all nouns belonging  to a class contained in the tree cut model are assigned the same probability, seems  to be rather stringent. If one were to insist that the model be exactly accurate, then  it would seem that the true model would be the word-based model resulting from  no generalization at all. If we allow approximations, however, it is likely that some  reasonable tree cut model with the uniform probability assumption will be a good  approximation of the true distribution; in fact, a best model for a given data size. As  we remarked earlier, as MDL balances between the fit to the data and the simplicity  of the model, one can expect that the model selected by MDL will be a reasonable  compromise.  Nonetheless, it is still a shortcoming of our model that it contains an oversimplified  assumption, and the problem is especially pressing when rare words are involved. Rare  words may not be observed at a slot of interest in the data simply because they are  rare, and not because they are unfit for that particular slot. 9 To see how rare is too  rare for our method, consider the following example.  Suppose that the class BIRD contains 10 words, bird, swallow, crow, eagle, parrot,  waxwing, etc. Consider co-occurrence data having 8 occurrences of bird, 2 occurrences  of swallow, 1 occurrence of crow, 1 occurrence of eagle, and 0 occurrence of all other  words, as part of, say, 100 data obtained for the subject slot of verb fly. For this data  set, our method would select the model that generalizes bird, swallow, etc. to the class  BIRD, since the sum of the data and parameter description lengths for the BIRD subtree  is 76.57 + 3.32 = 79.89 if generalized, and 53.73 + 33.22 = 86.95 if not generalized. For  comparison, consider the data with 10 occurrences of bird, 3 occurrences of swallow  and 1 occurrence of crow, and 0 occurrence of all other words, also as part of 100 data for the subject slot of fly. In this case, our method would select the model that  stops generalizing at bird, swallow, eagle, etc., because the description length for the  same subtree now is 86.22 + 3.32 = 89.54 if generalized, and 55.04 + 33.22 = 88.26 if  not generalized. These examples eem to indicate that our MDL-based method would  choose to generalize, even when there are relatively large differences in frequencies of  words within a class, but knows enough to stop generalizing when the discrepancy  in frequencies i especially noticeable (relative to the given sample size).  4. Experimental Results  4.1 Experiment 1: A Qualitative Evaluation  We applied our generalization method to large corpora and inspected the obtained  tree cut models to see if they agreed with human intuition. In our experiments, we  extracted verbs and their case frame slots (verb, slot_name, slot_value triples) from the  tagged texts of the Wall Street Journal corpus (ACL/DCI CD-ROM1) consisting of  126,084 sentences, using existing techniques (specifically, those in Smadja \\\\[1993\\\\]), then  9 There are several possible measures that one could take to address this issue, including the  incorporation fabsolute frequencies of the words (inside and outside the particular slot in question).  This is outside the scope of the present paper, and we simply refer the interested reader to one possible  approach (Abe and Li 1996).  230  Li and Abe Generalizing Case Frames  Table 6  Example input data (for the direct object slot of eat).  eat arg2 food 3 eat arg2 lobster 1 eat arg2 seed 1  eat arg2 heart 2 eat arg2 liver 1 eat arg2 plant 1  eat arg2 sandwich 2 eat arg2 crab 1 eat arg2 elephant 1  eat arg2 meal 2 eat arg2 rope 1 eat arg2 seafood 1  eat arg2 amount 2 eat arg2 horse 1 eat arg2 mushroom 1  eat arg2 night 2 eat arg2 bug 1 eat arg2 ketchup 1  eat arg2 lunch 2 eat arg2 bowl 1 eat arg2 sawdust 1  eat arg2 snack 2 eat arg2 month 1 eat arg2 egg 1  eat arg2 jam 2 eat arg2 effect 1 eat arg2 sprout 1  eat arg2 diet 1 eat arg2 debt 1 eat arg2 nail 1  eat arg2 pizza 1 eat arg2 oyster 1  applied our method to generalize the slot_values. Table 6 shows some example triple  data for the direct object slot of the verb eat.  There were some extraction errors present in the data, but we chose not to remove  them, because in general there will always be extraction errors and realistic evaluation  should leave them in.  When generalizing, we used the noun taxonomy of WordNet (version 1.4) (Miller  1995) as our thesaurus. The noun taxonomy of WordNet has a structure of directed  acyclic graph (DAG), and its nodes stand for a word sense (a concept) and often  contain several words having thesame word sense. WordNet thus deviates from our  notion of thesaurus--a tree in which each leaf node stands for a noun, each internal  node stands for the class of nouns below it, and a noun is uniquely represented by a  leaf node- -so we took a few measures to deal with this.  First, we modified our algorithm FInd-MDL so that it can be applied to a DAG;  now, Find-MDL effectively copies each subgraph having multiple parents (and its  associated ata) so that the DAG is transformed to a tree structure. Note that with  this modification it is no longer guaranteed that the output model is optimal. Next,  we dealt heuristically with the issue of word-sense ambiguity by equally dividing the  observed frequency of a noun between all the nodes containing that noun. Finally,  when an internal node contained nouns actually occurring in the data, we assigned  the .frequencies of all the nodes below it to that internal node, and excised the whole  subtree (subgraph) below it. The last of these measures, in effect, defines the "starting  cut" of the thesaurus from which to begin generalizing. Since (word senses of) nouns  that occur in natural language tend to concentrate in the middle of a taxonomy, the  starting cut given by this method usually falls around the middle of the thesaurus. 1? Figure 9 shows the starting cut and the resulting cut in WordNet for the direct  object slot of eat with respect o the data in Table 6, where / . . . /  denotes a node in  WordNet. The starting cut consists of nodes/plant . . . / , / food/ ,etc ,  which are the high-  est nodes containing values of the direct object slot of eat. Since/ food/has significantly  higher frequencies than its ne ighbors /so l id /and/ f lu id / ,  thegeneralization stops there  according to MDL. In contrast, the nodes under / l i fe_ form. . . /have relatively small dif-  ferences in their frequencies, and thus they are generalized to the node/ l i fe_form.. . / .   The same is true of the nodes under /artifact/. Since / . . -amount . . . /  has a much  10 Cognitive scientists have observed that concepts in the middle of a taxonomy tend to be more  important with respect to learning, recognition, and memory, and their linguistic expressions occur  more frequently innatural language--a phenomenon k own as basic level primacy. See Lakoff (1987).  231  Computational Linguistics Volume 24, Number 2  TOP  t~ <abstraction>  o.i0 ~ - -  / . . . . . .  ~-  -~--..., -~  <life_form...> O. 11 <object> ( <measure,~:ljjafl~it~,amount...> <space> <time>  / \\\\  / / ",9-089 \\\' <plant...> % <animal...>| <substance> {<artifact...~  : ; ;, k \\\\ \\\' ,  \\\',  I I I ~ ~ / \\\\ ~ 0 .39  ~1 ~ t  J I I ~ _ ~ . . . . , . ~  ~ t. resulting cut  a a , ,,tsohd> <fluid> <foo~>~. ~ - - - ~t~rtinn c.==t  I I I I ~ %% / / \\\\ I % I  I I \\\\ I I  :mushroom> <lobster> <horse> <lobster> <pizza> <rope>  Figure 9  An example generalization, result (for the direct object slot of eat).  higher frequency than its neighbors /time/ and {space), the generalization does not  go up higher. All of these results eem to agree with human intuition, indicating that  our method results in an appropriate l vel of generalization.  Table 7 shows generalization results for the direct object slot of eat and some  other arbitrarily selected verbs, where classes are sorted in descending order of their  probability values. (Classes with probabilities less than 0.05 are discarded ue to space  limitations.)  Table 8 shows the computation time required (on a SPARC "Ultra 1" work station)  to obtain the results hown in Table 7. (The computation time for loading the WordNet  was excluded since it need be done only once.) Even though the noun taxonomy of  WordNet is a large thesaurus containing approximately 50,000 nodes, our method  still manages to efficiently generalize case slots using it. The table also shows the  average number of levels generalized for each slot, namely, the average number of  links between a node in the starting cut and its ancestor node in the resulting cut.  (For example, the number of levels generalized for/plant. . - /  is one in Figure 9.) One  can see that a significant amount of generalization is performed by our method--the  resulting tree cut is about 5 levels higher than the starting cut, on the average.  4.2 Experiment 2: PP-Attachment Disambiguation  Case frame patterns obtained by our method can be used in various tasks in natu-  ral language processing. In this paper, we test its effectiveness in a structural (PP-  attachment) disambiguation experiment.  Disambiguation Methods. It has been empirically verified that the use of lexical semantic  knowledge is effective in structural disambiguation, such as the PP-attachment prob-  lem (Hobbs and Bear 1990; Whittemore, Ferrara, and Brunner 1990). There have been  232  Li and Abe Generalizing Case Frames  Table 7  Examples of generalization results.  Class Probability Example Words  Direct Object of eat  (food,nutrient) 0.39 pizza, egg  (life_form,organism,being,living_thing) 0.11 lobster, horse  /measure,quantity, amount,quantum) 0.10 amount of  (artifact,article,artefact) 0.08 as if eat rope  Direct Object of buy  (object, inanimate-object,physical-object / 0.30 computer, painting  (asset) 0.10 stock, share  (group,grouping) 0.07 company, bank  (legal_document,legal_instrument,official_document .... ) 0.05 security, ticket  Direct Object of .fly  (entity) 0.35 airplane, flag, executive  (linear_measure,long_measure) 0.28 mile  /group,grouping) 0 .08  delegation  Direct Object of operate  /group,grouping/ 0 .13 company, fleet  (act,human_action,human_activity) 0.13 flight, operation  (structure,construction/ 0.12 center  (abstraction) 0.11 service, unit  (possession/ 0.06 profit, earnings  Table 8  Required computation time and number of generalized levels.  Verb CPU Time (second) Average Number of Generalized Levels  eat 1.00 5.2  buy 0.66 4.6  fly 1.11 6.0  operate 0.90 5.0  Average 0.92 5.2  many probabilistic methods proposed in the literature to address the PP-attachment  problem using lexical semantic knowledge which, in our view, can be classified into  three types.  The first approach (Hindle and Rooth 1991, 1993) takes doubles of the form  (verb, prep) and (nounl, prep), like those in Table 9, as training data to acquire semantic  knowledge and judges the attachment sites of the prepositional phrases in quadru-  ples of the form (verb, nounl, prep, noun2) e.g., (see, girl, with, telescope)--based on the acquired knowledge. Hindle and Rooth (1991) proposed the use of the lexical  association measure calculated based on such doubles. More specifically, they esti-  mate P(prep I verb) and P(prep \\\\[ noun1), and calculate the so-called t-score, which is  a measure of the statistical significance of the difference between P(prep I verb) and  P(prep \\\\[ nounl). If the t-score indicates that the former probability is significantly larger,  233  Computational Linguistics Volume 24, Number 2  Table 9  Example input data as doubles.  see in  see with  girl with  man with  Table 10  Example input data as triples.  see in park  see with telescope  girl with scarf  see with friend  man with hat  Table 11  Example input data as quadruples and  labels.  see girl in park ADV  see man with telescope ADV  see girl with scarf ADN  then the prepositional phrase is attached to verb, if the latter probability is significantly  larger, it is attached to nounl, and otherwise no decision is made.  The second approach (Sekine et al 1992; Chang, Luo, and Su 1992; Resnik 1993a;  Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2)  and (nounl, prep, noun2), like those in Table 10, as training data for acquiring semantic  knowledge and performs PP-attachment disambiguation on quadruples. For example,  Resnik (1993a) proposes the use of the selectional ssociation measure calculated based  on such triples, as described in Section 2. More specifically, his method compares  maxclassi~noun2 A(Classi \\\\[ verb, prep) and maxclassi~no,m2 A(Classi I nounl,prep) to make  disambiguation decisions.  The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994;  Collins and Brooks 1995) receives quadruples (verb, noun1, prep, noun2) and labels indi-  cating which way the PP-attachment goes, like those in Table 11, and learns a disam-  biguation rule for resolving PP-attachment ambiguities. For example, Brill and Resnik,  (1994) propose a method they call transformation-based error-driven learning (see also  Brill \\\\[1995\\\\]). Their method first learns IF-THENtype rules, where the IF parts repre-  sent conditions like (prep is with) and (verb is see), and the THEN parts represent  transformations from (attach to verb) to (attach to nounl), or vice versa. The first rule  is always a default decision, and all the other rules indicate transformations (changes  of attachment sites) subject o various IF conditions.  We note that, for the disambiguation problem, the first two approaches are basi-  cally unsupervised learning methods, in the sense that the training data are merely  positive examples for both types of attachments, which could in principle be extracted  from pure corpus data with no human intervention. (For example, one could just  use unambiguous sentences.) The third approach, on the other hand, is a supervised  learning method, which requires labeled data prepared by a human being.  234  Li and Abe Generalizing Case Frames  Table 12  Number of different types of data.  Training Data  Average number of doubles per data set 91218.1  Average number of triples per data set 91218.1  Average number of quadruples per data set 21656.6  Test Data  Average number of quadruples per data set 820.4  The generalization method we propose falls into the second category, although it  can also be used as a component in a combined scheme with many of the above meth-  ods (see Brill and Resnik \\\\[1994\\\\], Alshawi and Carter \\\\[1994\\\\]). We estimate P(noun2 I  verb, prep) and P(noun2 I nount, prep) from training data consisting of triples, and com-  pare them: If the former exceeds the latter (by a certain margin) we attach it to verb,  else if the latter exceeds the former (by the same margin) we attach it to noun1.  In our experiments, described below, we compare the performance of our proposed  method, which we refer to as MDL, against the methods proposed by Hindle and  Rooth (1991), Resnik (1993b), and Brill and Resnik (1994), referred to respectively as  LA, SA, and TEL.  Data Set. We used the bracketed corpus of the Penn Treebank (Wall Street Journal cor-  pus) (Marcus, Santorini, and Marcinkiewicz 1993) as our data. First we randomly  selected one of the 26 directories of the WSJ files as the test data and what remains as  the training data. We repeated this process 10 times and obtained 10 sets of data con-  sisting of different raining data and test data. We used these 10 data sets to conduct  cross-validation as described below.  From the test data in each data set, we extracted (verb, noun1, prep, noun2) quadru-  ples using the extraction tool provided by the Penn Treebank called "tgrep." At the  same time, we obtained the answer for the PP-attachment site for each quadruple.  We did not double-check if the answers provided in the Penn Treebank were actually  correct or not. Then from the training data of each data set, we extracted (verb,prep)  and (noun, prep) doubles, and (verb, prep, noun2) and (nounl,prep, noun2) triples using  tools we developed ourselves. We also extracted quadruples from the training data as  before. We then applied 12 heuristic rules to further preprocess the data, which include  (1) changing the inflected form of a word to its stem form, (2) replacing numerals with  the word number, (3) replacing integers between 1,900 and 2,999 with the word year, (4)  replacing co., ltd., etc. with the words company, limited, etc. 11 After preprocessing there  still remained some minor errors, which we did not remove further, due to the lack  of a good method for doing so automatically. Table 12 shows the number of different  types of data obtained by the above process.  Experimental Procedure. We first compared the accuracy and coverage for each of the  three disambiguation methods based on unsupervised learning: MDL, SA, and LA.  11 The experimental results obtained here are better than those obtained in our preliminary experiment  (Li and Abe 1995), in part because we only adopted rule (1) in the past.  235  Computational Linguistics Volume 24, Number 2  0.98  0.96  0.94  0.92  0.9  0.88  0.66  0.84  0.82  0.8  0  "\\\'-. . ,   " \\\' " - , . , . ,   " " , . ,   "E3,  "\\\'"\\\', ..,  "D,.  x* \\\' ,   f I I {  0.2 0.4 0.6 0.8  coverage  Figure 10  Accuracy-coverage curves for MDL, SA, and LA.  i  "MDL"  "SA" -4--  "LA" -~--  "LA.t" x ?  \\\'D,.  "~t  "El  For MDL, we generalized noun2 given (verb, prep, noun2) and (nounl,prep, noun2)  triples as training data for each data set, using WordNet as the thesaurus in the same  manner as in experiment 1. When disambiguating, we actually compared P(Classl \\\\[  verb, prep) and P(Class2 I noun1, prep), where Class1 and Class2 are classes in the out-  put tree cut models dominating noun2 in place of P(noun2 \\\\] verb, prep) and P(noun2 \\\\]  nounl,prep). 12 We found that doing so gives a slightly better esult. For SA, we em-  ployed a somewhat simplified version in which noun2 is generalized given (verb, prep,  noun2) and (nounl,prep, noun2) triples using WordNet, and maxcl~ss,~,o,,2 A(Classi I  verb, prep) and maxctass,~no,n2 A(Classi l nounl, prep) are compared for disambiguation:  If the former exceeds the latter then the prepositional phrase is attached to verb, and  otherwise to noun1. For LA, we estimated P(prep \\\\] verb) and P(prep \\\\] noun1) from the  training data of each data set and compared them for disambiguation. We then eval-  uated the results achieved by the three methods in terms of accuracy and coverage.  Here, coverage refers to the proportion as a percentage, of the test quadruples on which the disambiguation method could make a decision, and accuracy refers to the  proportion of correct decisions among them.  In Figure 10, we plot the accuracy-coverage curves for the three methods. In plot-  ting these curves, the attachment site is determined by simply seeing if the difference  between the appropriate measures for the two alternatives, be it probabilities or selec-  tional association values, exceeds a threshold. For each method, the threshold was set  successively to 0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, and 0.75. When the difference between  the two measures i less than a threshold, we rule that no decision can be made. These  curves were obtained by averaging over the 10 data sets.  12 Recall that a node in WordNet represents a word sense and not a word; noun2 can belong to several  classes in the thesaurus. We thus use maxciassignou,2 (P(Classi \\\\[ verb, prep)) and  maxclassi gno,m2 ( P( Classi \\\\[ nounl, prep) in place of P( Classl \\\\] verb, prep) and P( Class2 \\\\[ nounl, prep).  236  Li and Abe Generalizing Case Frames  Table 13  Results of PP-attachment disambiguation.  Coverage(%) Accuracy(%)  Default 100 56.2  MDL + Default 100 82.2  SA + Default 100 76.7  LA + Default 100 80.7  LA.t + Default 100 78.1  TEL 100 82.4  We also implemented the exact method proposed by Hindle and Rooth (1991),  which makes disambiguation judgement using the t-score. Figure 10 shows the re-  sult as LA.t, where the threshold for t-score is set to 1.28 (significance l vel of 90  percent.) From Figure 10 we see that with respect o accuracy-coverage curves, MDL  outperforms both SA and LA throughout, while SA is better than LA.  Next, we tested the method of applying a default rule after applying each method.  That is, attaching (prep, noun2) to verb for the part of the test data for which no deci-  sion was made by the method in question. 13We refer to these combined methods as  MDL+Default, SA+Default, LA+Default, and LA.t+Default. Table 13 shows the results,  again averaged over the 10 data sets.  Finally, we used the transformation-based error-driven learning (TEL) to acquire  transformation rules for each data set and applied the obtained rules to disambiguate  the test data. The average number of obtained rules for a data set was 2,752.3. Table 13  shows the disambiguation result averaged over the 10 data sets. From Table 13, we  see that TEL performs the best, edging over the second place MDL+Default by a small  margin, and then followed by LA+Default, and SA+Default. Below we discuss further  observations concerning these results.  MDL and SA. According to our experimental results, the accuracy and coverage of  MDL appear to be somewhat better than those of SA. As Resnik (1993b) pointed  ~ P(qv,r) out, the use of selectional ssociation Iu~ ~ seems to be appropriate for cognitive modeling. Our experiments show, however, that the generalization method currently  employed by Resnik has a tendency to overfit he data. Table 14 shows example gener-  alization results for MDL (with classes with probability less than 0.05 discarded) and  SA. Note that MDL tends to select a tree cut closer to the root of the thesaurus tree.  This is probably the key reason why MDL has a wider coverage than SA for the same  degree of accuracy. One may be concerned that MDL is "overgeneralizing" here, 14 but  as shown in Figure 10, its disambiguation accuracy does not seem to be degraded.  Another problem that must be dealt with concerning SA is how to remove noise  (resulting, for example, from erroneous extraction) from the generalization results.  P(Clv,r) Since SA estimates the ratio between two probability values, namely -~y- ,  the gen-  eralization result may be lead astray if one of the estimates of P(C I v, r) and P(C) is  unreliable. For instance, a high estimated value for/drop, bead, pearl / at protect against  13 Interestingly, for the entire data set it is more favorable to attach (prep, noun2) to noun1, but for what  remains after applying LA and MDL, it turns out to be more favorable to attach (prep, noun2) to verb.  14 Note that in Experiment 1, there were more data available, and thus the data were more appropriately  generalized.  237  Computational Linguistics Volume 24, Number 2  Table 14  Example generalization results for SA and MDL.  Input  Verb Preposition Noun Frequency  protect against accusation 1  protect against damage 1  protect against decline 1  protect against drop 1  protect against loss 1  protect against resistance 1  protect against squall 1  protect against vagary 1  Generalization Result of MDL  Verb Preposition Noun Class Probability  protect against (act,human_action,human_activity) 0.212  protect against (phenomenon) 0.170  protect against (psychological_feature) 0.099  protect against (event) 0.097  protect against (abstraction) 0.093  Generalization Result of SA  Verb Preposition Noun Class SA  protect against  protect against  protect against  protect against  protect against  protect against  protect against  protect against  protect against  protect against  protect against  protect against  protect against  protect against  protect against  (caprice,impulse,vagary, whim) 1.528  (phenomenon) 0.899  (happening,occurrence,natural_event) 0.339  (deterioration,worsening,decline,declination) 0.285  (act,human_action,human_activity) 0.260  (drop,bead,pearl) 0.202  (drop) 0.202  (descent,declivity, fall,decline,downslope) 0.188  (resistor, resistance) 0.130  (underground,resistance) 0.130  {immunity, resistance) O. 124  (resistance, opposition) 0.111  (loss,deprivation) 0.105  (loss) 0.096  (cost,price,terms,damage / 0.052  shown in Table 14 is rather odd, and is because the estimate of P(C) is unreliable (too  small). This problem apparently costs SA a nonnegligible drop in disambiguation ac-  curacy. In contrast, MDL does not suffer from this problem since a high estimated  probability value is only possible with highfrequency, which cannot result just from  extraction errors. Consider, for example, the occurrence of car in the data shown in  Figure 8, which has supposedly resulted from an erroneous extraction. The effect of  this datum gets washed away, as the estimated probability for VEHICLE, to which car  has been generalized, is negligible.  On the other hand, SA has a merit not shared by MDL, namely its use of the  association ratio factors out the effect of absolute frequencies of words, and focuses  238  Li and Abe Generalizing Case Frames  Table 15  Some hard examples for LA.  Attached to verb Attached to noun1  acquire interest in year  buy stock in trade  ease restriction on export  forecast sale for year  make payment on million  meet standard for resistance  reach agreement in august  show interest in session  win verdict in winter  acquire interest in firm  buy stock in index  ease restriction on type  forecast sale for venture  make payment on debt  meet standard for car  reach agreement in principle  show interest in stock  win verdict in case  on their co-occurrence r lation. Since both MDL and SA have pros and cons, it would  be desirable to develop a methodology that combines the merits of the two methods  (cf. Abe and Li \\\\[1996\\\\]).  MDL and LA. LA makes its disambiguation decision completely ignoring noun2. As  Resnik (1993b) pointed out, if we hope to improve disambiguation performance by  increasing training data, we need a richer model such as those used in MDL and SA.  We found that 8.8% of the quadruples in our entire test data were such that they shared  the same verb, prep, noun1 but had different noun2, and their PP-attachment si es go both  ways in the same data, i.e., both to verb and to noun1. Clearly, for these examples, the  PP-attachment site cannot be reliably determined without knowing noun2. Table 15  shows some of these examples. (We adopted the attachment sites given in the Penn  Tree Bank, without correcting apparently wrong judgements.)  MDL and TEL. We chose TEL as an example of the quadruple approach. This method  was designed specifically for the purpose of resolving PP-attachment ambiguities, and  seems to perform slightly better than ours.  As we remarked earlier, however, the input data required by our method (triples)  could be generated automatically from unparsed corpora making use of existing  heuristic rules (Brent 1993; Smadja 1993), although for the experiments we report  here we used a parsed corpus. Thus it would seem to be easier to obtain more data  in the future for MDL and other methods based on unsupervised learning. Also note  that our method of generalizing values of a case slot can be used for purposes other  than disambiguation. 5. Conc lus ions   We proposed a new method of generalizing case frames. Our approach of applying  MDL to estimate a tree cut model in an existing thesaurus i not limited to just the  problem of generalizing values of a case frame slot. It is potentially useful in other  natural language processing tasks, such as the problem of estimating n-gram models  (Brown et al 1992) or the problem of semantic tagging (Cucchiarelli and Velardi 1997).  We believe that our method has the following merits: (1) it is theoretically sound; (2) it  is computationally efficient; (3) it is robust against noise. Our experimental results  indicate that the performance of our method is better than, or at least comparable  to, existing methods. One of the disadvantages of our method is that its performance  239  Computational Linguistics Volume 24, Number 2  depends on the structure of the particular thesaurus used. This, however, is a prob-  lem commonly shared by any generalization method that uses a thesaurus as prior  knowledge.  Appendix A: Proof of Proposition 1  Proof  For an arbitrary subtree T\\\' of a thesaurus tree T and an arbitrary tree cut model  M = (F,0) of T, let MT, = (FT , ,0T , )  denote the submodel of M that is contained in  T\\\'. Also for any sample S and any subtree T\\\' of T, let ST, denote the subsample of S  contained in T\\\'. (Note that MT = M, ST = S.) Then define, in general for any submodel  MT, and subsample ST,, L(ST, \\\\[ FT,, ~T\\\') to be the data description length of subsample  ST, using submodel MT,, L(~T, \\\\[ FT,) to be the parameter description length for the  submodel MT,, and L\\\'(MT,,ST,) to  be L(ST, I FT \\\' ,~T \\\' )  q- L(~T, \\\\[ FT,). (Note that, when  calculating the parameter description length for a submodel, the sample size of the  entire sample \\\\]S\\\\] is used.)  First note that for any (sub)tree T, (sub)model MT = (FT, ~T) contained in T, and  (sub)sample ST contained in T, and T\\\'s child subtrees Ti : i = 1,. . . ,  k, we have:  k  L(ST I PT, g )  =  L(ST, I PT,,g,) (17)  i=1  provided that Fz is not a single node (root node of T). This follows from the mutual  disjointness of the Ti, and the independence of the parameters in the Ti.  We also have, when T is a proper subtree of the thesaurus tree:  k  L(OT I FT) = ~ L(OT, I FT,). (18)  i=1  Since the number of free parameters of a model in the entire thesaurus tree equals  the number of nodes in the model minus one due to the stochastic ondition (that the  probability parameters must sum to one), when T equals the entire thesaurus tree,  theoretically theparameter description length for a tree cut model of T should be:  L(g I rr) = L r)  k  =  L(0r, I rr,)  i=1  log Isl (19)  where ISI is the size of the entire sample. Since the second term - ~  in (19) is  constant once the input sample S is fixed, for the purpose of finding a model with the  minimum description length, it is irrelevant. We will thus use the identity (18) both  when T is the entire tree and when it is a proper subtree. (This allows us to use the  same recursive algorithm, Find-MDL, in all cases.)  It follows from (17) and (18) that the minimization of description length can be  done essentially independently for each subtree. Namely, if we let Clmin (MT, ST) denote  the minimum description length (as defined by \\\\[17\\\\] and \\\\[18\\\\]) achievable for (sub)model  Mr on (sub)sample ST contained in (sub)tree T, \\\\[)s(~) the MLE estimate for node ~\\\\]  240  Li and Abe Generalizing Case Frames  using the entire sample S, and root(T) the root node of tree T, then we have:  L~nin(MT, ST) min L~nin (MTi, ST i),  k i=1  L\\\'( (\\\\[root(T)\\\\], \\\\[Ps(root(T) )\\\\]), ST) } (20)  The rest of the proof proceeds by induction. First, when T is of a single leaf  node, the submodel consisting solely of the node and the MLE of the generation  probability for the class represented by T is returned, which is clearly a submodel  with minimum description length in the subtree T. Next, inductively assume that  Find-MDL(T ~) correctly outputs a (sub)model with the minimum description length  for any tree T\\\' of size less than n. Then, given a tree T of size n whose root node has at  least two children, say Ti : i = 1 . . . . .  k, for each Ti, Find-MDL(Ti) returns a (sub)model  with the minimum description length by the inductive hypothesis. Then, since (20)  holds, whichever way the if-clause on lines 8, 9 of Find-MDL evaluates to, what is  returned on line 11 or line 13 will still be a (sub)model with the minimum description  length, completing the inductive step.  It is easy to see that the running time of the algorithm is linear in both the number  of leaf nodes of the input thesaurus tree and the input sample size. ?  Acknowledgments  We are grateful to K. Nakamura nd  T. Fujita of NEC C&C Res. Labs. for their  constant encouragement. Wethank  K. Yaminishi and J. Takeuchi of C&C Res.  Labs. for their suggestions and comments.  We thank T. Futagami of NIS for his  programming efforts. We also express our  special appreciation to the two anonymous  reviewers who have provided many  valuable comments. We acknowledge the  ACL for providing the ACL/DCI CD-ROM,  LDC of the University ofPennsylvania for  providing the Penn Treebank corpus data,  and Princeton University for providing  WordNet, and E. Brill and P. Resnik for  providing their PP-attachment  disambiguation program. \''), ('text2', b"b'1 Introduction Verb classifications have been shown to be useful both from a theoretical and from a practical perspective. From the theoretical viewpoint, they permit capturing syntactic and/or semantic generalisations about verbs (Levin, 1993; Kipper Schuler, 2006). From a practical perspective, they support factorisation and have been shown to be effective in various NLP (Natural language Processing) tasks such as semantic role labelling (Swier and Stevenson, 2005) or word sense disambiguation (Dang, 2004). While there has been much work on automatically acquiring verb classes for English (Sun et al, 2010) and to a lesser extent for German (Brew and Schulte im Walde, 2002; Schulte im Walde, 2003; Schulte im Walde, 2006), Japanese (Oishi and Matsumoto, 1997) and Italian (Merlo et al, 2002), few studies have been conducted on the automatic classification of French verbs. Recently however, two proposals have been put forward. On the one hand, (Sun et al, 2010) applied a clustering approach developed for English to French. They exploit features extracted from a large scale subcategorisation lexicon (LexSchem (Messiant, 2008)) acquired fully automatically from Le Monde newspaper corpus and show that, as for English, syntactic frames and verb selectional preferences perform better than lexical cooccurence features. Their approach achieves a F-measure of 55.1 on 116 verbs occurring at least 150 times in Lexschem. The best performance is achieved when restricting the approach to verbs occurring at least 4000 times (43 verbs) with an F-measure of 65.4. On the other hand, Falk and Gardent (2011) present a classification approach for French verbs based on the use of Formal Concept Analysis (FCA). FCA (Barbut and Monjardet, 1970) is a symbolic classification technique which permits creating classes associating sets of objects (eg. French verbs) with sets of features (eg. syntactic frames). Falk and Gardent (2011) provide no evaluation for their results however, only a qualitative analysis. In this paper, we describe a novel approach to the clustering of French verbs which (i) gives good results on the established benchmark used in (Sun et al., 2010) and (ii) associates verbs with a feature profile describing their syntactic and semantic properties. The approach exploits a clustering method called IGNGF (Incremental Growing Neural Gas with Feature Maximisation, (Lamirel et al, 2011b)) which uses the features characterising each cluster both to guide the clustering process and to label the output clusters. We apply this method to the data contained in various verb lexicons and we evalu854 ate the resulting classification on a slightly modified version of the gold standard provided by (Sun et al, 2010). We show that the approach yields promising results (F-measure of 70%) and that the clustering produced systematically associates verbs with syntactic frames and thematic grids thereby providing an interesting basis for the creation and evaluation of a Verbnet-like classification. Section 2 describes the lexical resources used for feature extraction and Section 3 the experimental setup. Sections 4 and 5 present the data used for and the results obtained. Section 6 concludes. 2 Lexical Resources Used Our aim is to accquire a classification which covers the core verbs of French, could be usedto support semantic role labelling and is similar in spirit to the English Verbnet. In this first experiment, we therefore favoured extracting the features used for clustering, not from a large corpus parsed automatically, but from manually validated resources1. These lexical resources are (i) a syntactic lexicon produced by merging three existing lexicons for French and (ii) the English Verbnet. Among the many syntactic lexicons available for French (Nicolas et al, 2008; Messiant, 2008; Kups?c? and Abeille?, 2008; van den Eynde and Mertens, 2003; Gross, 1975), we selected and merged three lexicons built or validated manually namely, Dicovalence, TreeLex and the LADL tables. The resulting lexicon contains 5918 verbs, 20433 lexical entries (i.e., verb/frame pairs) and 345 subcategorisation frames. It also contains more detailed syntactic and semantic features such as lexical preferences (e.g., locative argument, concrete object) or thematic role information (e.g., symmetric arguments, asset role) which we make use of for clustering. We use the English Verbnet as a resource for associating French verbs with thematic grids as follows. We translate the verbs in the English Verbnet classes to French using English-French dictionaries2. To 1Of course, the same approach could be applied to corpus based data (as done e.g., in (Sun et al, 2010)) thus making the approach fully unsupervised and directly applicable to any language for which a parser is available. 2For the translation we use the following resources: SciFran-Euradic, a French-English bilingual dictionary, built and improved by linguists (http://catalog.elra.info/ deal with polysemy, we train a supervised classifier as follows. We first map French verbs with English Verbnet classes: A French verb is associated with an English Verbnet class if, according to our dictionaries, it is a translation of an English verb in this class. The task of the classifier is then to produce a probability estimate for the correctness of this association, given the training data. The training set is built by stating for 1740 ?French verb, English Verbnet class? pairs whether the verb has the thematic grid given by the pair?s Verbnet class3. This set is used to train an SVM (support vector machine) classifier4. The features we use are similar to those used in (Mouton, 2010): they are numeric and are derived for example from the number of translations an English or French verb had, the size of the Verbnet classes, the number of classes a verb is a member of etc. The resulting classifier gives for each ?French verb, English VN class? pair the estimated probability of the pair?s verb being a member of the pair?s class5. We select 6000 pairs with highest probability estimates and obtain the translated classes by assigning each verb in a selected pair to the pair?s class. This way French verbs are effectively associated with one or more English Verbnet thematic grids. 3 Clustering Methods, Evaluation Metrics and Experimental Setup 3.1 Clustering Methods The IGNGF clustering method is an incremental neural ?winner-take-most? clustering method belonging to the family of the free topology neural clustering methods. Like other neural free topology methods such as Neural Gas (NG) (Martinetz and Schulten, 1991), GrowingNeural Gas (GNG) (Fritzke, 1995), or Incremental Growing Neural Gas (IGNG) (Prudent and Ennaji, 2005), the IGNGF method makes use of Hebbian learning product_info.php?products_id=666), Google dictionary (http://www.google.com/dictionary) and Dicovalence (van den Eynde and Mertens, 2003). 3The training data consists of the verbs and Verbnet classes used in the gold standard presented in (Sun et al, 2010). 4We used the libsvm (Chang and Lin, 2011) implementation of the classifier for this step. 5The accuracy of the classifier on the held out random test set of 100 pairs was of 90%. 855 (Hebb, 1949) for dynamically structuring the learning space. However, contrary to these methods, the use of a standard distance measure for determining a winner is replaced in IGNGF by feature maximisation. Feature maximisation is a cluster quality metric which associates each cluster with maximal features i.e., features whose Feature F-measure is maximal. Feature F-measure is the harmonic mean of Feature Recall and Feature Precision which in turn are defined as: FRc(f) = ? v?c W fv ? c??C ? v?c? W fv , FPc(f) = ? v?c W fv ? f ??Fc,v?c W f ? v where W fx represents the weight of the feature f for element x and Fc designates the set of features associated with the verbs occuring in the cluster c. A feature is then said to be maximal for a given cluster iff its Feature F-measure is higher for that cluster than for any other cluster. The IGNGF method was shown to outperform other usual neural and non neural methods for clustering tasks on relatively clean data (Lamirel et al, 2011b). Since we use features extracted from manually validated sources, this clustering technique seems a good fit for our application. In addition, the feature maximisation and cluster labeling performed by the IGNGF method has proved promising both for visualising clustering results (Lamirel et al, 2008) and for validating or optimising a clustering method (Attik et al, 2006). We make use of these processes in all our experiments and systematically compute cluster labelling and feature maximisation on the output clusterings. As we shall see, this permits distinguishing between clusterings with similar F-measure but lower ?linguistic plausibility? (cf. Section 5). This facilitates clustering interpretation in that cluster labeling clearly indicates the association between clusters (verbs) and their prevalent features. And this supports the creation of a Verbnet style classification in that cluster labeling directly provides classes grouping together verbs, thematic grids and subcategorisation frames. 3.2 Evaluation metrics We use several evaluation metrics which bear on different properties of the clustering. Modified Purity and Accuracy. Following (Sun et al, 2010), we use modified purity (mPUR); weighted class accuracy (ACC) and F-measure to evaluate the clusterings produced. These are computed as follows. Each induced cluster is assigned the gold class (its prevalent class, prev(C)) to which most of its member verbs belong. A verb is then said to be correct if the gold associates it with the prevalent class of the cluster it is in. Given this, purity is the ratio between the number of correct gold verbs in the clusteringand the total number of gold verbs in the clustering6: mPUR = ? C?Clustering,|prev(C)|>1 |prev(C) ? C| VerbsGold?Clustering , where VerbsGold?Clustering is the total number of gold verbs in the clustering. Accuracy represents the proportion of gold verbs in those clusters which are associated with a gold class, compared to all the gold verbs in the clustering. To compute accuracy we associate to each gold class CGold a dominant cluster, ie. the cluster dom(CGold) which has most verbs in common with the gold class. Then accuracy is given by the following formula: ACC = ? C?Gold |dom(C) ? C| VerbsGold?Clustering Finally, F-measure is the harmonic mean of mPUR and ACC. Coverage. To assess the extent to which a clustering matches the gold classification, we additionally compute the coverage of each clustering that is, the proportion of gold classes that are prevalent classes in the clustering. Cumulative Micro Precision (CMP). As pointed out in (Lamirel et al, 2008; Attik et al, 2006), unsupervised evaluation metrics based on cluster labelling and feature maximisation can prove very useful for identifying the best clustering strategy. Following (Lamirel et al, 2011a), we use CMP to identify the best clustering. Computed on the clustering results, this metrics evaluates the quality of a clustering w.r.t. the cluster features rather than w.r.t. 6Clusters for which the prevalent class has only one element are ignored 856 to a gold standard. It was shown in (Ghribi et al, 2010) to be effective in detecting degenerated clustering results including a small number of large heterogeneous, ?garbage? clusters and a big number of small size ?chunk? clusters. First, the local Recall (Rfc ) and the local Precision (P fc ) of a feature f in a cluster c are defined as follows: Rfc = |vfc | |V f | P fc = |vfc | |Vc| where vfc is the set of verbs having feature f in c, Vc the set of verbs in c and V f , the set of verbs with feature f . Cumulative Micro-Precision (CMP) is then defined as follows: CMP = ? i=|Cinf |,|Csup| 1 |Ci+|2 ? c?Ci+,f?Fc P f c ? i=|Cinf |,|Csup| 1 Ci+ where Ci+ represents the subset of clusters of C for which the number of associated verbs is greater than i, and: Cinf = argminci?C |ci|, Csup = argmaxci?C |ci| 3.3 Cluster display, feature f-Measure and confidence score To facilitate interpretation, clusters are displayed as illustrated in Table 1. Features are displayed in decreasing order of Feature F-measure (cf. Section 3.1) and features whose Feature F-measure is under the average Feature F-measure of the overall clustering are clearly delineated from others. In addition, for each verb in a cluster, a confidence score is displayed which is the ratio between the sum of the F-measures of its cluster maximised features over the sum of the F-measures of the overall cluster maximised features. Verbs whose confidence score is 0 are considered as orphan data. 3.4 Experimental setup We applied an IDF-Norm weighting scheme (Robertson and Jones, 1976) to decrease the influence of the most frequent features(IDF component) and to compensate for discrepancies in feature number (normalisation). C6- 14(14) [197(197)] ???Prevalent Label ? = AgExp-Cause 0.341100 G-AgExp-Cause 0.274864 C-SUJ:Ssub,OBJ:NP 0.061313 C-SUJ:Ssub 0.042544 C-SUJ:NP,DEOBJ:Ssub ********** ********** 0.017787 C-SUJ:NP,DEOBJ:VPinf 0.008108 C-SUJ:VPinf,AOBJ:PP . . . [**de?primer 0.934345 4(0)] [affliger 0.879122 3(0)] [e?blouir 0.879122 3(0)] [choquer 0.879122 3(0)] [de?cevoir 0.879122 3(0)] [de?contenancer 0.879122 3(0)] [de?contracter 0.879122 3(0)] [de?sillusionner 0.879122 3(0)] [**ennuyer 0.879122 3(0)] [fasciner 0.879122 3(0)] [**heurter 0.879122 3(0)] . . . Table 1: Sample output for a cluster produced with the grid-scf-sem feature set and the IGNGF clustering method. We use K-Means as a baseline. For each clustering method (K-Means and IGNGF), we let the number of clusters vary between 1 and 30 to obtain a partition that reaches an optimum F-measure and a number of clusters that is in the same order of magnitude as the initial number of Gold classes (i.e. 11 classes). 4 Features and Data Features In the simplest case the features are the subcategorisation frames (scf) associated to the verbs by our lexicon. We also experiment with different combinations of additional, syntactic (synt) and semantic features (sem) extracted from the lexicon and with the thematic grids (grid) extracted from the English Verbnet. The thematic grid information is derived from the English Verbnet as explained in Section 2. The syntactic features extracted from the lexicon are listed in Table 1(a). They indicate whether a verb accepts symmetric arguments (e.g., John met Mary/John and Mary met); has four or more arguments; combines with a predicative phrase (e.g., John named Mary president); takes a sentential complement or an optional object; or accepts the passive in se (similar to the English middle voice Les habits se vendent bien / The clothes sell well). As shown in Table 1(a), these 857 (a) Additional syntactic features. Feature related VN class Symmetric arguments amalgamate-22.2, correspond-36.1 4 or more arguments get-13.5.1, send-11.1 Predicate characterize-29.2 Sentential argument correspond-36.1, characterize-29.2 Optional object implicit theme (Randall, 2010), p. 95 Passive built with se theme role (Randall, 2010), p. 120 (b) Additional semantic features. Feature related VN class Location role put-9.1, remove-10.1, . . . Concrete object hit-18.1 (eg. INSTRUMENT) (non human role) other cos-45.4 . . . Asset role get-13.5.1 Plural role amalgamate-22.2, correspond-36.1 Table 2: Additional syntactic (a) and semantic (b) features extracted from the LADL and Dicovalence resources and the alternations/roles they are possibly related to. features are meant to help identify specific Verbnet classes and thematic roles. Finally, we extract four semantic features from the lexicon. These indicate whether a verb takes a locative or an asset argument and whether it requires a concrete object (non human role) or a plural role. The potential correlation between these features and Verbnet classes is given in Table 1(b). French Gold Standard To evaluate our approach, we use the gold standard proposed by Sun et al (2010). This resource consists of 16 fine grained Levin classes with 12 verbs each whose predominant sense in English belong to that class. Since our goal is to build a Verbnet like classification for French, we mapped the 16 Levin classesof the Sun et al (2010)?s Gold Standard to 11 Verbnet classes thereby associating each class with a thematic grid. In addition we group Verbnet semantic roles as shown in Table 4. Table 3 shows the reference we use for evaluation. Verbs For our clustering experiments we use the 2183 French verbs occurring in the translations of the 11 classes in the gold standard (cf. Section 4). Since we ignore verbs with only one feature the number of verbs and ?verb, feature? pairs considered may vary slightly across experiments. AgExp Agent, Experiencer AgentSym Actor, Actor1, Actor2 Theme Theme, Topic, Stimulus, Proposition PredAtt Predicate, Attribute ThemeSym Theme, Theme1, Theme2 Patient Patient PatientSym Patient, Patient1, Patient2 Start Material (transformation), Source (motion, transfer) End Product (transformation), Destination (motion), Recipient (transfer) Location Instrument Cause Beneficiary Table 4: Verbnet role groups. 5 Results 5.1 Quantitative Analysis Table 4(a) includes the evaluation results for all the feature sets when using IGNGF clustering. In terms of F-measure, the results range from 0.61 to 0.70. This generally outperforms (Sun et al, 2010) whose best F-measures vary between 0.55 for verbs occurring at least 150 times in the training data and 0.65 for verbs occurring at least 4000 times in this training data. The results are not directly comparable however since the gold data is slightly different due to the grouping of Verbnet classes through their thematic grids. In terms of features, the best results are obtained using the grid-scf-sem feature set with an Fmeasure of 0.70. Moreover, for this data set, the unsupervised evaluation metrics (cf. Section 3) highlight strong cluster cohesion with a number of clusters close to the number of gold classes (13 clusters for 11 gold classes); a low number of orphan verbs (i.e., verbs whose confidence score is zero); and a high Cumulated Micro Precision (CMP = 0.3) indicating homogeneous clusters in terms of maximising features. The coverage of 0.72 indicates that approximately 8 out of the 11 gold classes could be matched to a prevalent label. That is, 8 clusters were labelled with a prevalent label corresponding to 8 distinct gold classes. In contrast, the classification obtained using the scf-synt-sem feature set has a higher CMP for the clustering with optimal mPUR (0.57); but a lower F-measure (0.61), a larger number of classes (16) 858 AgExp, PatientSym amalgamate-22.2: incorporer, associer, re?unir, me?langer, me?ler, unir, assembler, combiner, lier, fusionner Cause, AgExp amuse-31.1: abattre, accabler, briser, de?primer, consterner, ane?antir, e?puiser, exte?nuer, e?craser, ennuyer, e?reinter, inonder AgExp, PredAtt, Theme characterize-29.2: appre?hender, concevoir, conside?rer, de?crire, de?finir, de?peindre, de?signer, envisager, identifier, montrer, percevoir, repre?senter, ressentir AgentSym, Theme correspond-36.1: coope?rer, participer, collaborer, concourir, contribuer, associer AgExp, Beneficiary, Extent, Start, Theme get-13.5.1: acheter, prendre, saisir, re?server, conserver, garder, pre?server, maintenir, retenir, louer, affre?ter AgExp, Instrument, Patient hit-18.1: cogner, heurter, battre, frapper, fouetter, taper, rosser, brutaliser, e?reinter, maltraiter, corriger other cos-45.4: me?langer, fusionner, consolider, renforcer, fortifier, adoucir, polir, atte?nuer, tempe?rer, pe?trir, fac?onner, former AgExp, Location, Theme light emission-43.1 briller, e?tinceler, flamboyer, luire, resplendir, pe?tiller, rutiler, rayonner, scintiller modes of being with motion-47.3: trembler, fre?mir, osciller, vaciller, vibrer, tressaillir, frissonner, palpiter, gre?siller, trembloter, palpiterrun-51.3.2: voyager, aller, errer, circuler, courir, bouger, naviguer, passer, promener, de?placer AgExp, End, Theme manner speaking-37.3: ra?ler, gronder, crier, ronchonner, grogner, bougonner, maugre?er, rouspe?ter, grommeler, larmoyer, ge?mir, geindre, hurler, gueuler, brailler, chuchoter put-9.1: accrocher, de?poser, mettre, placer, re?partir, re?inte?grer, empiler, emporter, enfermer, inse?rer, installer say-37.7: dire, re?ve?ler, de?clarer, signaler, indiquer, montrer, annoncer, re?pondre, affirmer, certifier, re?pliquer AgExp, Theme peer-30.3: regarder, e?couter, examiner, conside?rer, voir, scruter, de?visager AgExp, Start, Theme remove-10.1: o?ter, enlever, retirer, supprimer, retrancher, de?barasser, soustraire, de?compter, e?liminer AgExp, End, Start, Theme send-11.1: envoyer, lancer, transmettre, adresser, porter, expe?dier, transporter, jeter, renvoyer, livrer Table 3: French gold classes and their member verbs presented in (Sun et al, 2010). and a higher number of orphans (156). That is, this clustering has many clusters with strong feature cohesion but a class structure that markedly differs from the gold. Since there might be differences in structure between the English Verbnet and the thematic classification for French we are building, this is not necessarily incorrect however. Further investigation on a larger data set would be required to assess which clustering is in fact better given the data used and the classification searched for. In general, data sets whose description includes semantic features (sem or grid) tend to produce better results than those that do not (scf or synt). This is in line with results from (Sun et al, 2010) which shows that semantic features help verb classification. It differs from it however in that the semantic features used by Sun et al (2010) are selectional preferences while ours are thematic grids and a restricted set of manually encoded selectional preferences. Noticeably, the synt feature degrades performance throughout: grid,scf,synt has lower Fmeasure than grid,scf; scf,synt,sem than scf,sem; and scf,synt than scf. We have no clear explanation for this. The best results are obtained with IGNGF method on most of the data sets. Table 4(b) illustrates the differences between the results obtained with IGNGF and those obtained with K-means on the grid-scf-sem data set (best data set). Although Kmeans and IGNGF optimal model reach similar Fmeasure and display a similar number of clusters, the very low CMP (0.10) of the K-means model shows that, despite a good Gold class coverage (0.81), K-means tend to produce more heterogeneous clusters in terms of features. Table 4(b) also shows the impact of IDF feature weighting and feature vector normalisation on clustering. The benefit of preprocessing the data appears clearly. When neither IDF weighting nor vector normalisation are used, F-measure decreases from 0.70 to 0.68 and cumulative micro-precision from 0.30 to 0.21. When either normalisation or IDF weighting is left out, the cumulative micro-precision drops by up to 15 points (from 0.30 to 0.15 and 0.18) and the number of orphans increases from 67 up to 180. 859 (a) The impact of the feature set. Feat. set Nbr. feat. Nbr. verbs mPUR ACC F (Gold) Nbr. classes Cov. Nbr. orphans CMP at opt (13cl.) scf 220 2085 0.93 0.48 0.64 17 0.55 129 0.28 (0.27) grid, scf 231 2085 0.94 0.54 0.68 14 0.64 183 0.12 (0.12) grid, scf, sem 2372183 0.86 0.59 0.70 13 0.72 67 0.30 (0.30) grid, scf, synt 236 2150 0.87 0.50 0.63 14 0.72 66 0.13 (0.14) grid, scf, synt, sem 242 2201 0.99 0.52 0.69 16 0.82 100 0.50 (0.22) scf, sem 226 2183 0.83 0.55 0.66 23 0.64 146 0.40 (0.26) scf, synt 225 2150 0.91 0.45 0.61 15 0.45 83 0.17 (0.22) scf, synt, sem 231 2101 0.89 0.47 0.61 16 0.64 156 0.57 (0.11) (b) Metrics for best performing clustering method (IGNGF) compared to K-means. Feature set is grid, scf, sem. Method mPUR ACC F (Gold) Nbr. classes Cov. Nbr. orphans CMP at opt (13cl.) IGNGF with IDF and norm. 0.86 0.59 0.70 13 0.72 67 0.30 (0.30) K-means with IDF and norm. 0.88 0.57 0.70 13 0.81 67 0.10 (0.10) IGNGF, no IDF 0.86 0.59 0.70 17 0.81 126 0.18 (0.14) IGNGF, no norm. 0.78 0.62 0.70 18 0.72 180 0.15 (0.11) IGNGF, no IDF, no norm. 0.87 0.55 0.68 14 0.81 103 0.21 (0.21) Table 5: Results. Cumulative micro precision (CMP) is given for the clustering at the mPUR optimum and in parantheses for 13 classes clustering. That is, clusters are less coherent in terms of features. 5.2 Qualitative Analysis We carried out a manual analysis of the clusters examining both the semantic coherence of each cluster (do the verbs in that cluster share a semantic component?) and the association between the thematic grids, the verbs and the syntactic frames provided by clustering. Semantic homogeneity: To assess semantic homogeneity, we examined each cluster and sought to identify one or more Verbnet labels characterising the verbs contained in that cluster. From the 13 clusters produced by clustering, 11 clusters could be labelled. Table 6 shows these eleven clusters, the associated labels (abbreviated Verbnet class names), some example verbs, a sample subcategorisation frame drawn from the cluster maximising features and an illustrating sentence. As can be seen, some clusters group together several subclasses and conversely, some Verbnet classes are spread over several clusters. This is not necessarily incorrect though. To start with, recall that we are aiming for a classification which groups together verbs with the same thematic grid. Given this, cluster C2 correctly groups together two Verbnet classes (other cos-45.4 and hit-18.1) which share the same thematic grid (cf. Table 3). In addition, the features associated with this cluster indicate that verbs in these two classes are transitive, select a concrete object, and can be pronominalised which again is correct for most verbs in that cluster. Similarly, cluster C11 groups together verbs from two Verbnet classes with identical theta grid (light emission-43.1 and modes of being with motion-47.3) while its associated features correctly indicate that verbs from both classes accept both the intransitive form without object (la jeune fille rayonne / the young girl glows, un cheval galope / a horse gallops) and with a prepositional object (la jeune fille rayonne de bonheur / the young girl glows with happiness, un cheval galope vers l?infini / a horse gallops to infinity). The third cluster grouping together verbs from two Verbnet classes isC7 which contains mainly judgement verbs (to applaud, bless, compliment, punish) but also some verbs from the (very large) other cos-45.4 class. In this case, a prevalent shared feature is that both types of verbs accept a de-object that is, a prepositional object introduced by ?de? (Jean applaudit Marie d?avoir danse? / Jean applaudit Marie for having danced; Jean de?gage le sable de la route / Jean clears the sand of the road). The semantic features necessary to provide a finer grained analysis of their differences are lacking. Interestingly, clustering also highlights classes which are semantically homogeneous but syntactically distinct. While clusters C6 and C10 both 860 contain mostly verbs from the amuse-31.1 class (amuser,agacer,e?nerver,de?primer), their features indicate that verbs in C10 accept the pronominal form (e.g., Jean s?amuse) while verbs in C6 do not (e.g., *Jean se de?prime). In this case, clustering highlights a syntactic distinction which is present in French but not in English. In contrast, the dispersion of verbs from the other cos-45.4 class over clusters C2 and C7 has no obvious explanation. One reason might be that this class is rather large (361 verbs) and thus might contain French verbs that do not necessarily share properties with the original Verbnet class. Syntax and Semantics. We examined whether the prevalent syntactic features labelling each cluster were compatible with the verbs and with the semantic class(es) manually assigned to the clusters. Table 6 sketches the relation between cluster, syntactic frames and Verbnet like classes. It shows for instance that the prevalent frame of the C0 class (manner speaking-37.3) correctly indicates that verbs in that cluster subcategorise for a sentential argument and an AOBJ (prepositional object in ?a`?) (e.g., Jean bafouille a` Marie qu?il est amoureux / Jean stammers to Mary that he is in love); and that verbs in the C9 class (characterize-29.2) subcategorise for an object NP and an attribute (Jean nomme Marie pre?sidente / Jean appoints Marie president). In general, we found that the prevalent frames associated with each cluster adequately characterise the syntax of that verb class.6 Conclusion We presented an approach to the automatic classification of french verbs which showed good results on an established testset and associates verb clusters with syntactic and semantic features. Whether the features associated by the IGNGF clustering with the verb clusters appropriately caracterise these clusters remains an open question. We carried out a first evaluation using these features to label the syntactic arguments of verbs in a corpus with thematic roles and found that precision is high but recall low mainly because of polysemy: the frames and grids made available by the classification for a given verb are correct for that verb but not for the verb sense occurring in the corpus. This suggests that overlapping clustering techniques need to C0 speaking: babiller, bafouiller, balbutier SUJ:NP,OBJ:Ssub,AOBJ:PP Jean bafouille a` Marie qu?il l?aime / Jean stammers to Mary that he is in love C1 put: entasser, re?pandre, essaimer SUJ:NP,POBJ:PP,DUMMY:REFL Loc, Plural Les de?chets s?entassent dans la cour / Waste piles in the yard C2 hit: broyer, de?molir, fouetter SUJ:NP,OBJ:NP T-Nhum Ces pierres broient les graines / These stones grind the seeds. other cos: agrandir, alle?ger, amincir SUJ:NP,DUMMY:REFL les ae?roports s?agrandissent sans arre?t / airports grow constantly C4 dedicate: s?engager a`, s?obliger a`, SUJ:NP,AOBJ:VPinf,DUMMY:REFL Cette promesse t?engage a` nous suivre / This promise commits you to following us C5 conjecture: penser, attester, agre?er SUJ:NP,OBJ:Ssub Le me?decin atteste que l?employe? n?est pas en e?tat de travailler / The physician certifies that the employee is not able to work C6 amuse: de?primer, de?contenancer, de?cevoir SUJ:Ssub,OBJ:NP SUJ:NP,DEOBJ:Ssub Travailler de?prime Marie / Working depresses Marie Marie de?prime de ce que Jean parte / Marie depresses because of Jean?s leaving C7 other cos: de?gager, vider, drainer, sevrer judgement SUJ:NP,OBJ:NP,DEOBJ:PP vider le re?cipient de son contenu / empty the container of its contents applaudir, be?nir, bla?mer, SUJ:NP,OBJ:NP,DEOBJ:Ssub Jean blame Marie d?avoir couru / Jean blames Mary for runnig C9 characterise: promouvoir, adouber, nommer SUJ:NP,OBJ:NP,ATB:XP Jean nomme Marie pre?sidente / Jean appoints Marie president C10 amuse: agacer, amuser, enorgueillir SUJ:NP,DEOBJ:XP,DUMMY:REFL Jean s?enorgueillit d?e?tre roi/ Jean is proud to be king C11 light: rayonner,clignoter,cliqueter SUJ:NP,POBJ:PP Jean clignote des yeux / Jean twinkles his eyes motion: aller, passer, fuir, glisser SUJ:NP,POBJ:PP glisser sur le trottoir verglace? / slip on the icy sidewalk C12 transfer msg: enseigner, permettre, interdire SUJ:NP,OBJ:NP,AOBJ:PP Jean enseigne l?anglais a` Marie / Jean teaches Marie English. Table 6: Relations between clusters, syntactic frames and Verbnet like classes. be applied. We are also investigating how the approach scales up to the full set of verbs present in the lexicon. Both Dicovalence and the LADL tables contain rich detailed information about the syntactic and semantic properties of French verbs. We intend to tap on that potential and explore how well the various semantic features that can be extracted from these resources support automatic verb classification for the full set of verbs present in our lexicon. 861'")])
INFO:tensorflow:Finished getting dataset.
I0712 12:59:25.761384 139911399675712 input_pipeline.py:91] Finished getting dataset.
I0712 12:59:25.761618 139911399675712 input_pipeline.py:94] Using char-level/byte dataset..
I0712 12:59:25.868529 139911399675712 train.py:106] Vocab Size: 257
I0712 12:59:39.531282 139911399675712 checkpoints.py:249] Found no checkpoint files in trained_models/matching/local with prefix checkpoint_
I0712 12:59:40.154183 139911399675712 train_utils.py:370] Starting training
I0712 12:59:40.154434 139911399675712 train_utils.py:371] ====================
I0712 13:00:08.201550 139911399675712 train_utils.py:377] train in step: 0
I0712 13:00:08.229159 139911399675712 train_utils.py:377] train in step: 1
I0712 13:00:08.258613 139911399675712 train_utils.py:377] train in step: 2
I0712 13:00:08.281057 139911399675712 train_utils.py:377] train in step: 3
I0712 13:00:08.302653 139911399675712 train_utils.py:377] train in step: 4
I0712 13:00:08.327414 139911399675712 train_utils.py:377] train in step: 5
I0712 13:00:08.351733 139911399675712 train_utils.py:377] train in step: 6
I0712 13:00:08.373809 139911399675712 train_utils.py:377] train in step: 7
I0712 13:00:08.396517 139911399675712 train_utils.py:377] train in step: 8
I0712 13:00:08.423364 139911399675712 train_utils.py:377] train in step: 9
I0712 13:00:08.449445 139911399675712 train_utils.py:377] train in step: 10
I0712 13:00:08.475639 139911399675712 train_utils.py:377] train in step: 11
I0712 13:00:08.506801 139911399675712 train_utils.py:377] train in step: 12
I0712 13:00:08.532567 139911399675712 train_utils.py:377] train in step: 13
I0712 13:00:08.558271 139911399675712 train_utils.py:377] train in step: 14
I0712 13:00:08.584656 139911399675712 train_utils.py:377] train in step: 15
I0712 13:00:08.614694 139911399675712 train_utils.py:377] train in step: 16
I0712 13:00:08.649945 139911399675712 train_utils.py:377] train in step: 17
I0712 13:00:08.678091 139911399675712 train_utils.py:377] train in step: 18
I0712 13:00:08.702549 139911399675712 train_utils.py:377] train in step: 19
I0712 13:00:08.727880 139911399675712 train_utils.py:377] train in step: 20
I0712 13:00:08.755368 139911399675712 train_utils.py:377] train in step: 21
I0712 13:00:08.781439 139911399675712 train_utils.py:377] train in step: 22
I0712 13:00:08.810954 139911399675712 train_utils.py:377] train in step: 23
I0712 13:00:08.833090 139911399675712 train_utils.py:377] train in step: 24
I0712 13:00:08.859411 139911399675712 train_utils.py:377] train in step: 25
I0712 13:00:08.885144 139911399675712 train_utils.py:377] train in step: 26
I0712 13:00:08.912728 139911399675712 train_utils.py:377] train in step: 27
I0712 13:00:08.938660 139911399675712 train_utils.py:377] train in step: 28
I0712 13:00:08.964468 139911399675712 train_utils.py:377] train in step: 29
I0712 13:00:08.990274 139911399675712 train_utils.py:377] train in step: 30
I0712 13:00:09.016593 139911399675712 train_utils.py:377] train in step: 31
I0712 13:00:09.042602 139911399675712 train_utils.py:377] train in step: 32
I0712 13:00:09.068604 139911399675712 train_utils.py:377] train in step: 33
I0712 13:00:09.094485 139911399675712 train_utils.py:377] train in step: 34
I0712 13:00:09.120543 139911399675712 train_utils.py:377] train in step: 35
I0712 13:00:09.150811 139911399675712 train_utils.py:377] train in step: 36
I0712 13:00:09.174479 139911399675712 train_utils.py:377] train in step: 37
I0712 13:00:09.199072 139911399675712 train_utils.py:377] train in step: 38
I0712 13:00:09.225404 139911399675712 train_utils.py:377] train in step: 39
I0712 13:00:09.254538 139911399675712 train_utils.py:377] train in step: 40
I0712 13:00:09.279193 139911399675712 train_utils.py:377] train in step: 41
I0712 13:00:09.306070 139911399675712 train_utils.py:377] train in step: 42
I0712 13:00:09.335859 139911399675712 train_utils.py:377] train in step: 43
I0712 13:00:09.360178 139911399675712 train_utils.py:377] train in step: 44
I0712 13:00:09.386046 139911399675712 train_utils.py:377] train in step: 45
I0712 13:00:09.416338 139911399675712 train_utils.py:377] train in step: 46
I0712 13:00:09.440839 139911399675712 train_utils.py:377] train in step: 47
I0712 13:00:09.470888 139911399675712 train_utils.py:377] train in step: 48
I0712 13:00:09.495454 139911399675712 train_utils.py:377] train in step: 49
I0712 13:00:09.519178 139911399675712 train_utils.py:377] train in step: 50
I0712 13:00:09.545130 139911399675712 train_utils.py:377] train in step: 51
I0712 13:00:09.571184 139911399675712 train_utils.py:377] train in step: 52
I0712 13:00:09.598338 139911399675712 train_utils.py:377] train in step: 53
I0712 13:00:09.623948 139911399675712 train_utils.py:377] train in step: 54
I0712 13:00:09.649822 139911399675712 train_utils.py:377] train in step: 55
I0712 13:00:09.679785 139911399675712 train_utils.py:377] train in step: 56
I0712 13:00:09.705934 139911399675712 train_utils.py:377] train in step: 57
I0712 13:00:09.732297 139911399675712 train_utils.py:377] train in step: 58
I0712 13:00:09.759511 139911399675712 train_utils.py:377] train in step: 59
I0712 13:00:09.785378 139911399675712 train_utils.py:377] train in step: 60
I0712 13:00:09.818968 139911399675712 train_utils.py:377] train in step: 61
I0712 13:00:09.841438 139911399675712 train_utils.py:377] train in step: 62
I0712 13:00:09.867736 139911399675712 train_utils.py:377] train in step: 63
I0712 13:00:09.894401 139911399675712 train_utils.py:377] train in step: 64
I0712 13:00:09.920703 139911399675712 train_utils.py:377] train in step: 65
I0712 13:00:09.947177 139911399675712 train_utils.py:377] train in step: 66
I0712 13:00:09.973331 139911399675712 train_utils.py:377] train in step: 67
I0712 13:00:10.003027 139911399675712 train_utils.py:377] train in step: 68
I0712 13:00:10.026802 139911399675712 train_utils.py:377] train in step: 69
I0712 13:00:10.052622 139911399675712 train_utils.py:377] train in step: 70
I0712 13:00:10.081269 139911399675712 train_utils.py:377] train in step: 71
I0712 13:00:10.107156 139911399675712 train_utils.py:377] train in step: 72
I0712 13:00:10.132852 139911399675712 train_utils.py:377] train in step: 73
I0712 13:00:10.163697 139911399675712 train_utils.py:377] train in step: 74
I0712 13:00:10.188190 139911399675712 train_utils.py:377] train in step: 75
I0712 13:00:10.215266 139911399675712 train_utils.py:377] train in step: 76
I0712 13:00:10.242142 139911399675712 train_utils.py:377] train in step: 77
I0712 13:00:10.270252 139911399675712 train_utils.py:377] train in step: 78
I0712 13:00:10.294674 139911399675712 train_utils.py:377] train in step: 79
I0712 13:00:10.321547 139911399675712 train_utils.py:377] train in step: 80
I0712 13:00:10.348814 139911399675712 train_utils.py:377] train in step: 81
I0712 13:00:10.373840 139911399675712 train_utils.py:377] train in step: 82
I0712 13:00:10.402055 139911399675712 train_utils.py:377] train in step: 83
I0712 13:00:10.427292 139911399675712 train_utils.py:377] train in step: 84
I0712 13:00:10.453664 139911399675712 train_utils.py:377] train in step: 85
I0712 13:00:10.482227 139911399675712 train_utils.py:377] train in step: 86
I0712 13:00:10.507920 139911399675712 train_utils.py:377] train in step: 87
I0712 13:00:10.535108 139911399675712 train_utils.py:377] train in step: 88
I0712 13:00:10.563095 139911399675712 train_utils.py:377] train in step: 89
I0712 13:00:10.600415 139911399675712 train_utils.py:377] train in step: 90
I0712 13:00:10.626737 139911399675712 train_utils.py:377] train in step: 91
I0712 13:00:10.659231 139911399675712 train_utils.py:377] train in step: 92
I0712 13:00:10.688855 139911399675712 train_utils.py:377] train in step: 93
I0712 13:00:10.713495 139911399675712 train_utils.py:377] train in step: 94
I0712 13:00:10.741595 139911399675712 train_utils.py:377] train in step: 95
I0712 13:00:10.768509 139911399675712 train_utils.py:377] train in step: 96
I0712 13:00:10.795021 139911399675712 train_utils.py:377] train in step: 97
I0712 13:00:10.825671 139911399675712 train_utils.py:377] train in step: 98
I0712 13:00:10.852688 139911399675712 train_utils.py:377] train in step: 99
I0712 13:00:10.880424 139911399675712 train_utils.py:377] train in step: 100
I0712 13:00:10.909299 139911399675712 train_utils.py:377] train in step: 101
I0712 13:00:10.934204 139911399675712 train_utils.py:377] train in step: 102
I0712 13:00:10.961501 139911399675712 train_utils.py:377] train in step: 103
I0712 13:00:10.988793 139911399675712 train_utils.py:377] train in step: 104
I0712 13:00:11.015388 139911399675712 train_utils.py:377] train in step: 105
I0712 13:00:11.043746 139911399675712 train_utils.py:377] train in step: 106
I0712 13:00:11.070569 139911399675712 train_utils.py:377] train in step: 107
I0712 13:00:11.097655 139911399675712 train_utils.py:377] train in step: 108
I0712 13:00:11.124337 139911399675712 train_utils.py:377] train in step: 109
I0712 13:00:11.152641 139911399675712 train_utils.py:377] train in step: 110
I0712 13:00:11.178758 139911399675712 train_utils.py:377] train in step: 111
I0712 13:00:11.213667 139911399675712 train_utils.py:377] train in step: 112
I0712 13:00:11.240507 139911399675712 train_utils.py:377] train in step: 113
I0712 13:00:11.271490 139911399675712 train_utils.py:377] train in step: 114
I0712 13:00:11.300080 139911399675712 train_utils.py:377] train in step: 115
I0712 13:00:11.331187 139911399675712 train_utils.py:377] train in step: 116
I0712 13:00:11.355573 139911399675712 train_utils.py:377] train in step: 117
I0712 13:00:11.384566 139911399675712 train_utils.py:377] train in step: 118
I0712 13:00:11.412600 139911399675712 train_utils.py:377] train in step: 119
I0712 13:00:11.439650 139911399675712 train_utils.py:377] train in step: 120
I0712 13:00:11.470095 139911399675712 train_utils.py:377] train in step: 121
I0712 13:00:11.495160 139911399675712 train_utils.py:377] train in step: 122
I0712 13:00:11.522637 139911399675712 train_utils.py:377] train in step: 123
I0712 13:00:11.551460 139911399675712 train_utils.py:377] train in step: 124
I0712 13:00:11.578665 139911399675712 train_utils.py:377] train in step: 125
I0712 13:00:11.606930 139911399675712 train_utils.py:377] train in step: 126
I0712 13:00:11.633919 139911399675712 train_utils.py:377] train in step: 127
I0712 13:00:11.660415 139911399675712 train_utils.py:377] train in step: 128
I0712 13:00:11.690355 139911399675712 train_utils.py:377] train in step: 129
I0712 13:00:11.716044 139911399675712 train_utils.py:377] train in step: 130
I0712 13:00:11.742247 139911399675712 train_utils.py:377] train in step: 131
I0712 13:00:11.769887 139911399675712 train_utils.py:377] train in step: 132
I0712 13:00:11.797821 139911399675712 train_utils.py:377] train in step: 133
I0712 13:00:11.823892 139911399675712 train_utils.py:377] train in step: 134
I0712 13:00:11.850861 139911399675712 train_utils.py:377] train in step: 135
I0712 13:00:11.878717 139911399675712 train_utils.py:377] train in step: 136
I0712 13:00:11.905881 139911399675712 train_utils.py:377] train in step: 137
I0712 13:00:11.932624 139911399675712 train_utils.py:377] train in step: 138
I0712 13:00:11.960051 139911399675712 train_utils.py:377] train in step: 139
I0712 13:00:11.988430 139911399675712 train_utils.py:377] train in step: 140
I0712 13:00:12.014442 139911399675712 train_utils.py:377] train in step: 141
I0712 13:00:12.043743 139911399675712 train_utils.py:377] train in step: 142
I0712 13:00:12.068962 139911399675712 train_utils.py:377] train in step: 143
I0712 13:00:12.096954 139911399675712 train_utils.py:377] train in step: 144
I0712 13:00:12.123267 139911399675712 train_utils.py:377] train in step: 145
I0712 13:00:12.159356 139911399675712 train_utils.py:377] train in step: 146
I0712 13:00:12.184080 139911399675712 train_utils.py:377] train in step: 147
I0712 13:00:12.213844 139911399675712 train_utils.py:377] train in step: 148
I0712 13:00:12.240028 139911399675712 train_utils.py:377] train in step: 149
I0712 13:00:12.266552 139911399675712 train_utils.py:377] train in step: 150
I0712 13:00:12.294769 139911399675712 train_utils.py:377] train in step: 151
I0712 13:00:12.321666 139911399675712 train_utils.py:377] train in step: 152
I0712 13:00:12.348027 139911399675712 train_utils.py:377] train in step: 153
I0712 13:00:12.375951 139911399675712 train_utils.py:377] train in step: 154
I0712 13:00:12.402885 139911399675712 train_utils.py:377] train in step: 155
I0712 13:00:12.429391 139911399675712 train_utils.py:377] train in step: 156
I0712 13:00:12.457514 139911399675712 train_utils.py:377] train in step: 157
I0712 13:00:12.483907 139911399675712 train_utils.py:377] train in step: 158
I0712 13:00:12.512299 139911399675712 train_utils.py:377] train in step: 159
I0712 13:00:12.538291 139911399675712 train_utils.py:377] train in step: 160
I0712 13:00:12.564911 139911399675712 train_utils.py:377] train in step: 161
I0712 13:00:12.592043 139911399675712 train_utils.py:377] train in step: 162
I0712 13:00:12.619480 139911399675712 train_utils.py:377] train in step: 163
I0712 13:00:12.646068 139911399675712 train_utils.py:377] train in step: 164
I0712 13:00:12.674857 139911399675712 train_utils.py:377] train in step: 165
I0712 13:00:12.702966 139911399675712 train_utils.py:377] train in step: 166
I0712 13:00:12.729601 139911399675712 train_utils.py:377] train in step: 167
I0712 13:00:12.758204 139911399675712 train_utils.py:377] train in step: 168
I0712 13:00:12.784216 139911399675712 train_utils.py:377] train in step: 169
I0712 13:00:12.811209 139911399675712 train_utils.py:377] train in step: 170
I0712 13:00:12.838298 139911399675712 train_utils.py:377] train in step: 171
I0712 13:00:12.865572 139911399675712 train_utils.py:377] train in step: 172
I0712 13:00:12.892249 139911399675712 train_utils.py:377] train in step: 173
I0712 13:00:12.919632 139911399675712 train_utils.py:377] train in step: 174
I0712 13:00:12.947034 139911399675712 train_utils.py:377] train in step: 175
I0712 13:00:12.975309 139911399675712 train_utils.py:377] train in step: 176
I0712 13:00:13.002400 139911399675712 train_utils.py:377] train in step: 177
I0712 13:00:13.029222 139911399675712 train_utils.py:377] train in step: 178
I0712 13:00:13.056666 139911399675712 train_utils.py:377] train in step: 179
I0712 13:00:13.083114 139911399675712 train_utils.py:377] train in step: 180
I0712 13:00:13.113312 139911399675712 train_utils.py:377] train in step: 181
I0712 13:00:13.138299 139911399675712 train_utils.py:377] train in step: 182
I0712 13:00:13.164876 139911399675712 train_utils.py:377] train in step: 183
I0712 13:00:13.192216 139911399675712 train_utils.py:377] train in step: 184
I0712 13:00:13.221232 139911399675712 train_utils.py:377] train in step: 185
I0712 13:00:13.246293 139911399675712 train_utils.py:377] train in step: 186
I0712 13:00:13.273207 139911399675712 train_utils.py:377] train in step: 187
I0712 13:00:13.301319 139911399675712 train_utils.py:377] train in step: 188
I0712 13:00:13.328555 139911399675712 train_utils.py:377] train in step: 189
I0712 13:00:13.356062 139911399675712 train_utils.py:377] train in step: 190
I0712 13:00:13.385249 139911399675712 train_utils.py:377] train in step: 191
I0712 13:00:13.412605 139911399675712 train_utils.py:377] train in step: 192
I0712 13:00:13.439654 139911399675712 train_utils.py:377] train in step: 193
I0712 13:00:13.466963 139911399675712 train_utils.py:377] train in step: 194
I0712 13:00:13.499462 139911399675712 train_utils.py:377] train in step: 195
I0712 13:00:13.525131 139911399675712 train_utils.py:377] train in step: 196
I0712 13:00:13.553489 139911399675712 train_utils.py:377] train in step: 197
I0712 13:00:13.579005 139911399675712 train_utils.py:377] train in step: 198
I0712 13:00:13.610229 139911399675712 train_utils.py:377] train in step: 199
I0712 13:00:13.636523 139911399675712 train_utils.py:377] train in step: 200
I0712 13:00:14.336020 139911399675712 train_utils.py:396] train in step: 200, loss: 0.7408999800682068, acc: 0.5223999619483948
I0712 13:00:21.281555 139911399675712 train_utils.py:411] eval in step: 200, loss: 0.7698, acc: 0.4550
I0712 13:00:21.285944 139911399675712 train_utils.py:421] Testing...
I0712 13:00:24.577470 139911399675712 train_utils.py:424] test in step: 200, loss: 0.7623, acc: 0.4850
I0712 13:00:24.612604 139911399675712 train_utils.py:377] train in step: 201
I0712 13:00:24.636655 139911399675712 train_utils.py:377] train in step: 202
I0712 13:00:24.661362 139911399675712 train_utils.py:377] train in step: 203
I0712 13:00:24.684607 139911399675712 train_utils.py:377] train in step: 204
I0712 13:00:24.706645 139911399675712 train_utils.py:377] train in step: 205
I0712 13:00:24.729703 139911399675712 train_utils.py:377] train in step: 206
I0712 13:00:24.752534 139911399675712 train_utils.py:377] train in step: 207
I0712 13:00:24.771687 139911399675712 train_utils.py:377] train in step: 208
I0712 13:00:24.798213 139911399675712 train_utils.py:377] train in step: 209
I0712 13:00:24.825382 139911399675712 train_utils.py:377] train in step: 210
I0712 13:00:24.854383 139911399675712 train_utils.py:377] train in step: 211
I0712 13:00:24.882414 139911399675712 train_utils.py:377] train in step: 212
I0712 13:00:24.908504 139911399675712 train_utils.py:377] train in step: 213
I0712 13:00:24.935728 139911399675712 train_utils.py:377] train in step: 214
I0712 13:00:24.962594 139911399675712 train_utils.py:377] train in step: 215
I0712 13:00:24.990294 139911399675712 train_utils.py:377] train in step: 216
I0712 13:00:25.019250 139911399675712 train_utils.py:377] train in step: 217
I0712 13:00:25.045210 139911399675712 train_utils.py:377] train in step: 218
I0712 13:00:25.072089 139911399675712 train_utils.py:377] train in step: 219
I0712 13:00:25.106110 139911399675712 train_utils.py:377] train in step: 220
I0712 13:00:25.132942 139911399675712 train_utils.py:377] train in step: 221
I0712 13:00:25.162029 139911399675712 train_utils.py:377] train in step: 222
I0712 13:00:25.189600 139911399675712 train_utils.py:377] train in step: 223
I0712 13:00:25.225537 139911399675712 train_utils.py:377] train in step: 224
I0712 13:00:25.251260 139911399675712 train_utils.py:377] train in step: 225
I0712 13:00:25.278182 139911399675712 train_utils.py:377] train in step: 226
I0712 13:00:25.306558 139911399675712 train_utils.py:377] train in step: 227
I0712 13:00:25.333764 139911399675712 train_utils.py:377] train in step: 228
I0712 13:00:25.363396 139911399675712 train_utils.py:377] train in step: 229
I0712 13:00:25.392229 139911399675712 train_utils.py:377] train in step: 230
I0712 13:00:25.418886 139911399675712 train_utils.py:377] train in step: 231
I0712 13:00:25.445849 139911399675712 train_utils.py:377] train in step: 232
I0712 13:00:25.472714 139911399675712 train_utils.py:377] train in step: 233
I0712 13:00:25.499873 139911399675712 train_utils.py:377] train in step: 234
I0712 13:00:25.528054 139911399675712 train_utils.py:377] train in step: 235
I0712 13:00:25.554132 139911399675712 train_utils.py:377] train in step: 236
I0712 13:00:25.581495 139911399675712 train_utils.py:377] train in step: 237
I0712 13:00:25.609704 139911399675712 train_utils.py:377] train in step: 238
I0712 13:00:25.635747 139911399675712 train_utils.py:377] train in step: 239
I0712 13:00:25.666736 139911399675712 train_utils.py:377] train in step: 240
I0712 13:00:25.692515 139911399675712 train_utils.py:377] train in step: 241
I0712 13:00:25.719106 139911399675712 train_utils.py:377] train in step: 242
I0712 13:00:25.746176 139911399675712 train_utils.py:377] train in step: 243
I0712 13:00:25.773590 139911399675712 train_utils.py:377] train in step: 244
I0712 13:00:25.801731 139911399675712 train_utils.py:377] train in step: 245
I0712 13:00:25.851804 139911399675712 train_utils.py:377] train in step: 246
I0712 13:00:25.880856 139911399675712 train_utils.py:377] train in step: 247
I0712 13:00:25.909898 139911399675712 train_utils.py:377] train in step: 248
I0712 13:00:25.937036 139911399675712 train_utils.py:377] train in step: 249
I0712 13:00:25.963575 139911399675712 train_utils.py:377] train in step: 250
I0712 13:00:25.992902 139911399675712 train_utils.py:377] train in step: 251
I0712 13:00:26.018422 139911399675712 train_utils.py:377] train in step: 252
I0712 13:00:26.049821 139911399675712 train_utils.py:377] train in step: 253
I0712 13:00:26.076329 139911399675712 train_utils.py:377] train in step: 254
I0712 13:00:26.102917 139911399675712 train_utils.py:377] train in step: 255
I0712 13:00:26.130601 139911399675712 train_utils.py:377] train in step: 256
I0712 13:00:26.157098 139911399675712 train_utils.py:377] train in step: 257
I0712 13:00:26.184147 139911399675712 train_utils.py:377] train in step: 258
I0712 13:00:26.211921 139911399675712 train_utils.py:377] train in step: 259
I0712 13:00:26.241054 139911399675712 train_utils.py:377] train in step: 260
I0712 13:00:26.271741 139911399675712 train_utils.py:377] train in step: 261
I0712 13:00:26.300657 139911399675712 train_utils.py:377] train in step: 262
I0712 13:00:26.327799 139911399675712 train_utils.py:377] train in step: 263
I0712 13:00:26.354146 139911399675712 train_utils.py:377] train in step: 264
I0712 13:00:26.380835 139911399675712 train_utils.py:377] train in step: 265
I0712 13:00:26.408141 139911399675712 train_utils.py:377] train in step: 266
I0712 13:00:26.435733 139911399675712 train_utils.py:377] train in step: 267
I0712 13:00:26.461460 139911399675712 train_utils.py:377] train in step: 268
I0712 13:00:26.488288 139911399675712 train_utils.py:377] train in step: 269
I0712 13:00:26.515177 139911399675712 train_utils.py:377] train in step: 270
I0712 13:00:26.542333 139911399675712 train_utils.py:377] train in step: 271
I0712 13:00:26.569879 139911399675712 train_utils.py:377] train in step: 272
I0712 13:00:26.596687 139911399675712 train_utils.py:377] train in step: 273
I0712 13:00:26.623614 139911399675712 train_utils.py:377] train in step: 274
I0712 13:00:26.650736 139911399675712 train_utils.py:377] train in step: 275
I0712 13:00:26.678058 139911399675712 train_utils.py:377] train in step: 276
I0712 13:00:26.704212 139911399675712 train_utils.py:377] train in step: 277
I0712 13:00:26.737716 139911399675712 train_utils.py:377] train in step: 278
I0712 13:00:26.763252 139911399675712 train_utils.py:377] train in step: 279
I0712 13:00:26.792289 139911399675712 train_utils.py:377] train in step: 280
I0712 13:00:26.817522 139911399675712 train_utils.py:377] train in step: 281
I0712 13:00:26.844087 139911399675712 train_utils.py:377] train in step: 282
I0712 13:00:26.873496 139911399675712 train_utils.py:377] train in step: 283
I0712 13:00:26.900583 139911399675712 train_utils.py:377] train in step: 284
I0712 13:00:26.927034 139911399675712 train_utils.py:377] train in step: 285
I0712 13:00:26.953716 139911399675712 train_utils.py:377] train in step: 286
I0712 13:00:26.980616 139911399675712 train_utils.py:377] train in step: 287
I0712 13:00:27.007505 139911399675712 train_utils.py:377] train in step: 288
I0712 13:00:27.035140 139911399675712 train_utils.py:377] train in step: 289
I0712 13:00:27.062106 139911399675712 train_utils.py:377] train in step: 290
I0712 13:00:27.088179 139911399675712 train_utils.py:377] train in step: 291
I0712 13:00:27.115468 139911399675712 train_utils.py:377] train in step: 292
I0712 13:00:27.142774 139911399675712 train_utils.py:377] train in step: 293
I0712 13:00:27.170075 139911399675712 train_utils.py:377] train in step: 294
I0712 13:00:27.198374 139911399675712 train_utils.py:377] train in step: 295
I0712 13:00:27.226435 139911399675712 train_utils.py:377] train in step: 296
I0712 13:00:27.254626 139911399675712 train_utils.py:377] train in step: 297
I0712 13:00:27.281952 139911399675712 train_utils.py:377] train in step: 298
I0712 13:00:27.310288 139911399675712 train_utils.py:377] train in step: 299
I0712 13:00:27.338842 139911399675712 train_utils.py:377] train in step: 300
I0712 13:00:27.365812 139911399675712 train_utils.py:377] train in step: 301
I0712 13:00:27.392786 139911399675712 train_utils.py:377] train in step: 302
I0712 13:00:27.420022 139911399675712 train_utils.py:377] train in step: 303
I0712 13:00:27.446695 139911399675712 train_utils.py:377] train in step: 304
I0712 13:00:27.474805 139911399675712 train_utils.py:377] train in step: 305
I0712 13:00:27.502884 139911399675712 train_utils.py:377] train in step: 306
I0712 13:00:27.528800 139911399675712 train_utils.py:377] train in step: 307
I0712 13:00:27.560313 139911399675712 train_utils.py:377] train in step: 308
I0712 13:00:27.585500 139911399675712 train_utils.py:377] train in step: 309
I0712 13:00:27.613874 139911399675712 train_utils.py:377] train in step: 310
I0712 13:00:27.643123 139911399675712 train_utils.py:377] train in step: 311
I0712 13:00:27.668356 139911399675712 train_utils.py:377] train in step: 312
I0712 13:00:27.697696 139911399675712 train_utils.py:377] train in step: 313
I0712 13:00:27.724305 139911399675712 train_utils.py:377] train in step: 314
I0712 13:00:27.751322 139911399675712 train_utils.py:377] train in step: 315
I0712 13:00:27.782100 139911399675712 train_utils.py:377] train in step: 316
I0712 13:00:27.809122 139911399675712 train_utils.py:377] train in step: 317
I0712 13:00:27.836847 139911399675712 train_utils.py:377] train in step: 318
I0712 13:00:27.866216 139911399675712 train_utils.py:377] train in step: 319
I0712 13:00:27.903028 139911399675712 train_utils.py:377] train in step: 320
I0712 13:00:27.929965 139911399675712 train_utils.py:377] train in step: 321
I0712 13:00:27.976632 139911399675712 train_utils.py:377] train in step: 322
I0712 13:00:28.003830 139911399675712 train_utils.py:377] train in step: 323
I0712 13:00:28.031172 139911399675712 train_utils.py:377] train in step: 324
I0712 13:00:28.061019 139911399675712 train_utils.py:377] train in step: 325
I0712 13:00:28.086168 139911399675712 train_utils.py:377] train in step: 326
I0712 13:00:28.111819 139911399675712 train_utils.py:377] train in step: 327
I0712 13:00:28.139282 139911399675712 train_utils.py:377] train in step: 328
I0712 13:00:28.167562 139911399675712 train_utils.py:377] train in step: 329
I0712 13:00:28.193004 139911399675712 train_utils.py:377] train in step: 330
I0712 13:00:28.221631 139911399675712 train_utils.py:377] train in step: 331
I0712 13:00:28.248001 139911399675712 train_utils.py:377] train in step: 332
I0712 13:00:28.274179 139911399675712 train_utils.py:377] train in step: 333
I0712 13:00:28.302761 139911399675712 train_utils.py:377] train in step: 334
I0712 13:00:28.327899 139911399675712 train_utils.py:377] train in step: 335
I0712 13:00:28.357217 139911399675712 train_utils.py:377] train in step: 336
I0712 13:00:28.385670 139911399675712 train_utils.py:377] train in step: 337
I0712 13:00:28.421907 139911399675712 train_utils.py:377] train in step: 338
I0712 13:00:28.447522 139911399675712 train_utils.py:377] train in step: 339
I0712 13:00:28.474982 139911399675712 train_utils.py:377] train in step: 340
I0712 13:00:28.503010 139911399675712 train_utils.py:377] train in step: 341
I0712 13:00:28.535517 139911399675712 train_utils.py:377] train in step: 342
I0712 13:00:28.562391 139911399675712 train_utils.py:377] train in step: 343
I0712 13:00:28.589822 139911399675712 train_utils.py:377] train in step: 344
I0712 13:00:28.619297 139911399675712 train_utils.py:377] train in step: 345
I0712 13:00:28.647707 139911399675712 train_utils.py:377] train in step: 346
I0712 13:00:28.677612 139911399675712 train_utils.py:377] train in step: 347
I0712 13:00:28.706247 139911399675712 train_utils.py:377] train in step: 348
I0712 13:00:28.733460 139911399675712 train_utils.py:377] train in step: 349
I0712 13:00:28.762259 139911399675712 train_utils.py:377] train in step: 350
I0712 13:00:28.787936 139911399675712 train_utils.py:377] train in step: 351
I0712 13:00:28.815160 139911399675712 train_utils.py:377] train in step: 352
I0712 13:00:28.843392 139911399675712 train_utils.py:377] train in step: 353
I0712 13:00:28.872184 139911399675712 train_utils.py:377] train in step: 354
I0712 13:00:28.896933 139911399675712 train_utils.py:377] train in step: 355
I0712 13:00:28.924425 139911399675712 train_utils.py:377] train in step: 356
I0712 13:00:28.951335 139911399675712 train_utils.py:377] train in step: 357
I0712 13:00:28.979913 139911399675712 train_utils.py:377] train in step: 358
I0712 13:00:29.007573 139911399675712 train_utils.py:377] train in step: 359
I0712 13:00:29.034183 139911399675712 train_utils.py:377] train in step: 360
I0712 13:00:29.061581 139911399675712 train_utils.py:377] train in step: 361
I0712 13:00:29.092264 139911399675712 train_utils.py:377] train in step: 362
I0712 13:00:29.120894 139911399675712 train_utils.py:377] train in step: 363
I0712 13:00:29.148028 139911399675712 train_utils.py:377] train in step: 364
I0712 13:00:29.174738 139911399675712 train_utils.py:377] train in step: 365
I0712 13:00:29.204091 139911399675712 train_utils.py:377] train in step: 366
I0712 13:00:29.229380 139911399675712 train_utils.py:377] train in step: 367
I0712 13:00:29.262981 139911399675712 train_utils.py:377] train in step: 368
I0712 13:00:29.292010 139911399675712 train_utils.py:377] train in step: 369
I0712 13:00:29.338453 139911399675712 train_utils.py:377] train in step: 370
I0712 13:00:29.367378 139911399675712 train_utils.py:377] train in step: 371
I0712 13:00:29.394810 139911399675712 train_utils.py:377] train in step: 372
I0712 13:00:29.422777 139911399675712 train_utils.py:377] train in step: 373
I0712 13:00:29.448368 139911399675712 train_utils.py:377] train in step: 374
I0712 13:00:29.475187 139911399675712 train_utils.py:377] train in step: 375
I0712 13:00:29.509151 139911399675712 train_utils.py:377] train in step: 376
I0712 13:00:29.535934 139911399675712 train_utils.py:377] train in step: 377
I0712 13:00:29.562729 139911399675712 train_utils.py:377] train in step: 378
I0712 13:00:29.589468 139911399675712 train_utils.py:377] train in step: 379
I0712 13:00:29.617314 139911399675712 train_utils.py:377] train in step: 380
I0712 13:00:29.643516 139911399675712 train_utils.py:377] train in step: 381
I0712 13:00:29.669819 139911399675712 train_utils.py:377] train in step: 382
I0712 13:00:29.696191 139911399675712 train_utils.py:377] train in step: 383
I0712 13:00:29.723233 139911399675712 train_utils.py:377] train in step: 384
I0712 13:00:29.749989 139911399675712 train_utils.py:377] train in step: 385
I0712 13:00:29.776215 139911399675712 train_utils.py:377] train in step: 386
I0712 13:00:29.802780 139911399675712 train_utils.py:377] train in step: 387
I0712 13:00:29.831266 139911399675712 train_utils.py:377] train in step: 388
I0712 13:00:29.855962 139911399675712 train_utils.py:377] train in step: 389
I0712 13:00:29.881815 139911399675712 train_utils.py:377] train in step: 390
I0712 13:00:29.909281 139911399675712 train_utils.py:377] train in step: 391
I0712 13:00:29.936355 139911399675712 train_utils.py:377] train in step: 392
I0712 13:00:29.962865 139911399675712 train_utils.py:377] train in step: 393
I0712 13:00:30.002064 139911399675712 train_utils.py:377] train in step: 394
I0712 13:00:30.034852 139911399675712 train_utils.py:377] train in step: 395
I0712 13:00:30.062367 139911399675712 train_utils.py:377] train in step: 396
I0712 13:00:30.090939 139911399675712 train_utils.py:377] train in step: 397
I0712 13:00:30.118760 139911399675712 train_utils.py:377] train in step: 398
I0712 13:00:30.145078 139911399675712 train_utils.py:377] train in step: 399
I0712 13:00:30.173362 139911399675712 train_utils.py:377] train in step: 400
I0712 13:00:30.411329 139911399675712 train_utils.py:396] train in step: 400, loss: 0.734499990940094, acc: 0.5374999642372131
I0712 13:00:33.556282 139911399675712 train_utils.py:411] eval in step: 400, loss: 0.6910, acc: 0.5400
I0712 13:00:33.559865 139911399675712 train_utils.py:421] Testing...
I0712 13:00:36.653856 139911399675712 train_utils.py:424] test in step: 400, loss: 0.6947, acc: 0.5150
I0712 13:00:36.681786 139911399675712 train_utils.py:377] train in step: 401
I0712 13:00:36.704614 139911399675712 train_utils.py:377] train in step: 402
I0712 13:00:36.725139 139911399675712 train_utils.py:377] train in step: 403
I0712 13:00:36.749005 139911399675712 train_utils.py:377] train in step: 404
I0712 13:00:36.769280 139911399675712 train_utils.py:377] train in step: 405
I0712 13:00:36.799113 139911399675712 train_utils.py:377] train in step: 406
I0712 13:00:36.830597 139911399675712 train_utils.py:377] train in step: 407
I0712 13:00:36.856277 139911399675712 train_utils.py:377] train in step: 408
I0712 13:00:36.883762 139911399675712 train_utils.py:377] train in step: 409
I0712 13:00:36.910317 139911399675712 train_utils.py:377] train in step: 410
I0712 13:00:36.942996 139911399675712 train_utils.py:377] train in step: 411
I0712 13:00:36.970199 139911399675712 train_utils.py:377] train in step: 412
I0712 13:00:37.005178 139911399675712 train_utils.py:377] train in step: 413
I0712 13:00:37.031090 139911399675712 train_utils.py:377] train in step: 414
I0712 13:00:37.059316 139911399675712 train_utils.py:377] train in step: 415
I0712 13:00:37.085506 139911399675712 train_utils.py:377] train in step: 416
I0712 13:00:37.111899 139911399675712 train_utils.py:377] train in step: 417
I0712 13:00:37.139202 139911399675712 train_utils.py:377] train in step: 418
I0712 13:00:37.177192 139911399675712 train_utils.py:377] train in step: 419
I0712 13:00:37.204076 139911399675712 train_utils.py:377] train in step: 420
I0712 13:00:37.230393 139911399675712 train_utils.py:377] train in step: 421
I0712 13:00:37.258706 139911399675712 train_utils.py:377] train in step: 422
I0712 13:00:37.284620 139911399675712 train_utils.py:377] train in step: 423
I0712 13:00:37.315158 139911399675712 train_utils.py:377] train in step: 424
I0712 13:00:37.341226 139911399675712 train_utils.py:377] train in step: 425
I0712 13:00:37.369649 139911399675712 train_utils.py:377] train in step: 426
I0712 13:00:37.401229 139911399675712 train_utils.py:377] train in step: 427
I0712 13:00:37.427567 139911399675712 train_utils.py:377] train in step: 428
I0712 13:00:37.458808 139911399675712 train_utils.py:377] train in step: 429
I0712 13:00:37.499884 139911399675712 train_utils.py:377] train in step: 430
I0712 13:00:37.527570 139911399675712 train_utils.py:377] train in step: 431
I0712 13:00:37.555163 139911399675712 train_utils.py:377] train in step: 432
I0712 13:00:37.584487 139911399675712 train_utils.py:377] train in step: 433
I0712 13:00:37.610052 139911399675712 train_utils.py:377] train in step: 434
I0712 13:00:37.645408 139911399675712 train_utils.py:377] train in step: 435
I0712 13:00:37.671902 139911399675712 train_utils.py:377] train in step: 436
I0712 13:00:37.698733 139911399675712 train_utils.py:377] train in step: 437
I0712 13:00:37.728367 139911399675712 train_utils.py:377] train in step: 438
I0712 13:00:37.754716 139911399675712 train_utils.py:377] train in step: 439
I0712 13:00:37.782354 139911399675712 train_utils.py:377] train in step: 440
I0712 13:00:37.809675 139911399675712 train_utils.py:377] train in step: 441
I0712 13:00:37.836672 139911399675712 train_utils.py:377] train in step: 442
I0712 13:00:37.863206 139911399675712 train_utils.py:377] train in step: 443
I0712 13:00:37.893115 139911399675712 train_utils.py:377] train in step: 444
I0712 13:00:37.920622 139911399675712 train_utils.py:377] train in step: 445
I0712 13:00:37.947708 139911399675712 train_utils.py:377] train in step: 446
I0712 13:00:37.973934 139911399675712 train_utils.py:377] train in step: 447
I0712 13:00:38.001393 139911399675712 train_utils.py:377] train in step: 448
I0712 13:00:38.030201 139911399675712 train_utils.py:377] train in step: 449
I0712 13:00:38.056149 139911399675712 train_utils.py:377] train in step: 450
I0712 13:00:38.083393 139911399675712 train_utils.py:377] train in step: 451
I0712 13:00:38.110252 139911399675712 train_utils.py:377] train in step: 452
I0712 13:00:38.137359 139911399675712 train_utils.py:377] train in step: 453
I0712 13:00:38.173696 139911399675712 train_utils.py:377] train in step: 454
I0712 13:00:38.201230 139911399675712 train_utils.py:377] train in step: 455
I0712 13:00:38.229180 139911399675712 train_utils.py:377] train in step: 456
I0712 13:00:38.257667 139911399675712 train_utils.py:377] train in step: 457
I0712 13:00:38.284953 139911399675712 train_utils.py:377] train in step: 458
I0712 13:00:38.312976 139911399675712 train_utils.py:377] train in step: 459
I0712 13:00:38.340655 139911399675712 train_utils.py:377] train in step: 460
I0712 13:00:38.367580 139911399675712 train_utils.py:377] train in step: 461
I0712 13:00:38.394493 139911399675712 train_utils.py:377] train in step: 462
I0712 13:00:38.421654 139911399675712 train_utils.py:377] train in step: 463
I0712 13:00:38.448707 139911399675712 train_utils.py:377] train in step: 464
I0712 13:00:38.476236 139911399675712 train_utils.py:377] train in step: 465
I0712 13:00:38.503050 139911399675712 train_utils.py:377] train in step: 466
I0712 13:00:38.530289 139911399675712 train_utils.py:377] train in step: 467
I0712 13:00:38.557689 139911399675712 train_utils.py:377] train in step: 468
I0712 13:00:38.584432 139911399675712 train_utils.py:377] train in step: 469
I0712 13:00:38.611896 139911399675712 train_utils.py:377] train in step: 470
I0712 13:00:38.638717 139911399675712 train_utils.py:377] train in step: 471
I0712 13:00:38.665990 139911399675712 train_utils.py:377] train in step: 472
I0712 13:00:38.696815 139911399675712 train_utils.py:377] train in step: 473
I0712 13:00:38.722965 139911399675712 train_utils.py:377] train in step: 474
I0712 13:00:38.749926 139911399675712 train_utils.py:377] train in step: 475
I0712 13:00:38.778292 139911399675712 train_utils.py:377] train in step: 476
I0712 13:00:38.805384 139911399675712 train_utils.py:377] train in step: 477
I0712 13:00:38.833787 139911399675712 train_utils.py:377] train in step: 478
I0712 13:00:38.862137 139911399675712 train_utils.py:377] train in step: 479
I0712 13:00:38.894102 139911399675712 train_utils.py:377] train in step: 480
I0712 13:00:38.920939 139911399675712 train_utils.py:377] train in step: 481
I0712 13:00:38.948842 139911399675712 train_utils.py:377] train in step: 482
I0712 13:00:38.975634 139911399675712 train_utils.py:377] train in step: 483
I0712 13:00:39.003075 139911399675712 train_utils.py:377] train in step: 484
I0712 13:00:39.029754 139911399675712 train_utils.py:377] train in step: 485
I0712 13:00:39.058611 139911399675712 train_utils.py:377] train in step: 486
I0712 13:00:39.084675 139911399675712 train_utils.py:377] train in step: 487
I0712 13:00:39.112124 139911399675712 train_utils.py:377] train in step: 488
I0712 13:00:39.139588 139911399675712 train_utils.py:377] train in step: 489
I0712 13:00:39.166804 139911399675712 train_utils.py:377] train in step: 490
I0712 13:00:39.193436 139911399675712 train_utils.py:377] train in step: 491
I0712 13:00:39.222332 139911399675712 train_utils.py:377] train in step: 492
I0712 13:00:39.248460 139911399675712 train_utils.py:377] train in step: 493
I0712 13:00:39.275587 139911399675712 train_utils.py:377] train in step: 494
I0712 13:00:39.302891 139911399675712 train_utils.py:377] train in step: 495
I0712 13:00:39.329531 139911399675712 train_utils.py:377] train in step: 496
I0712 13:00:39.356629 139911399675712 train_utils.py:377] train in step: 497
I0712 13:00:39.383982 139911399675712 train_utils.py:377] train in step: 498
I0712 13:00:39.412058 139911399675712 train_utils.py:377] train in step: 499
I0712 13:00:39.440808 139911399675712 train_utils.py:377] train in step: 500
I0712 13:00:39.467041 139911399675712 train_utils.py:377] train in step: 501
I0712 13:00:39.494213 139911399675712 train_utils.py:377] train in step: 502
I0712 13:00:39.522240 139911399675712 train_utils.py:377] train in step: 503
I0712 13:00:39.556410 139911399675712 train_utils.py:377] train in step: 504
I0712 13:00:39.587986 139911399675712 train_utils.py:377] train in step: 505
I0712 13:00:39.616165 139911399675712 train_utils.py:377] train in step: 506
I0712 13:00:39.648807 139911399675712 train_utils.py:377] train in step: 507
I0712 13:00:39.675941 139911399675712 train_utils.py:377] train in step: 508
I0712 13:00:39.703543 139911399675712 train_utils.py:377] train in step: 509
I0712 13:00:39.731127 139911399675712 train_utils.py:377] train in step: 510
I0712 13:00:39.765786 139911399675712 train_utils.py:377] train in step: 511
I0712 13:00:39.791961 139911399675712 train_utils.py:377] train in step: 512
I0712 13:00:39.819437 139911399675712 train_utils.py:377] train in step: 513
I0712 13:00:39.849326 139911399675712 train_utils.py:377] train in step: 514
I0712 13:00:39.875518 139911399675712 train_utils.py:377] train in step: 515
I0712 13:00:39.904216 139911399675712 train_utils.py:377] train in step: 516
I0712 13:00:39.931266 139911399675712 train_utils.py:377] train in step: 517
I0712 13:00:39.962400 139911399675712 train_utils.py:377] train in step: 518
I0712 13:00:39.988173 139911399675712 train_utils.py:377] train in step: 519
I0712 13:00:40.016993 139911399675712 train_utils.py:377] train in step: 520
I0712 13:00:40.051644 139911399675712 train_utils.py:377] train in step: 521
I0712 13:00:40.078323 139911399675712 train_utils.py:377] train in step: 522
I0712 13:00:40.105345 139911399675712 train_utils.py:377] train in step: 523
I0712 13:00:40.132059 139911399675712 train_utils.py:377] train in step: 524
I0712 13:00:40.160382 139911399675712 train_utils.py:377] train in step: 525
I0712 13:00:40.187094 139911399675712 train_utils.py:377] train in step: 526
I0712 13:00:40.215598 139911399675712 train_utils.py:377] train in step: 527
I0712 13:00:40.245771 139911399675712 train_utils.py:377] train in step: 528
I0712 13:00:40.272812 139911399675712 train_utils.py:377] train in step: 529
I0712 13:00:40.298510 139911399675712 train_utils.py:377] train in step: 530
I0712 13:00:40.327593 139911399675712 train_utils.py:377] train in step: 531
I0712 13:00:40.357064 139911399675712 train_utils.py:377] train in step: 532
I0712 13:00:40.394937 139911399675712 train_utils.py:377] train in step: 533
I0712 13:00:40.431324 139911399675712 train_utils.py:377] train in step: 534
I0712 13:00:40.456783 139911399675712 train_utils.py:377] train in step: 535
I0712 13:00:40.486987 139911399675712 train_utils.py:377] train in step: 536
I0712 13:00:40.518367 139911399675712 train_utils.py:377] train in step: 537
I0712 13:00:40.544840 139911399675712 train_utils.py:377] train in step: 538
I0712 13:00:40.571193 139911399675712 train_utils.py:377] train in step: 539
I0712 13:00:40.598399 139911399675712 train_utils.py:377] train in step: 540
I0712 13:00:40.626298 139911399675712 train_utils.py:377] train in step: 541
I0712 13:00:40.654819 139911399675712 train_utils.py:377] train in step: 542
I0712 13:00:40.688698 139911399675712 train_utils.py:377] train in step: 543
I0712 13:00:40.715674 139911399675712 train_utils.py:377] train in step: 544
I0712 13:00:40.747139 139911399675712 train_utils.py:377] train in step: 545
I0712 13:00:40.775236 139911399675712 train_utils.py:377] train in step: 546
I0712 13:00:40.800180 139911399675712 train_utils.py:377] train in step: 547
I0712 13:00:40.824310 139911399675712 train_utils.py:377] train in step: 548
I0712 13:00:40.854796 139911399675712 train_utils.py:377] train in step: 549
I0712 13:00:40.882755 139911399675712 train_utils.py:377] train in step: 550
I0712 13:00:40.911689 139911399675712 train_utils.py:377] train in step: 551
I0712 13:00:40.938459 139911399675712 train_utils.py:377] train in step: 552
I0712 13:00:40.966433 139911399675712 train_utils.py:377] train in step: 553
I0712 13:00:40.992851 139911399675712 train_utils.py:377] train in step: 554
I0712 13:00:41.030363 139911399675712 train_utils.py:377] train in step: 555
I0712 13:00:41.057601 139911399675712 train_utils.py:377] train in step: 556
I0712 13:00:41.087663 139911399675712 train_utils.py:377] train in step: 557
I0712 13:00:41.117514 139911399675712 train_utils.py:377] train in step: 558
I0712 13:00:41.142917 139911399675712 train_utils.py:377] train in step: 559
I0712 13:00:41.186403 139911399675712 train_utils.py:377] train in step: 560
I0712 13:00:41.218753 139911399675712 train_utils.py:377] train in step: 561
I0712 13:00:41.244818 139911399675712 train_utils.py:377] train in step: 562
I0712 13:00:41.276144 139911399675712 train_utils.py:377] train in step: 563
I0712 13:00:41.299190 139911399675712 train_utils.py:377] train in step: 564
I0712 13:00:41.326340 139911399675712 train_utils.py:377] train in step: 565
I0712 13:00:41.360719 139911399675712 train_utils.py:377] train in step: 566
I0712 13:00:41.387712 139911399675712 train_utils.py:377] train in step: 567
I0712 13:00:41.414652 139911399675712 train_utils.py:377] train in step: 568
I0712 13:00:41.441109 139911399675712 train_utils.py:377] train in step: 569
I0712 13:00:41.473144 139911399675712 train_utils.py:377] train in step: 570
I0712 13:00:41.499961 139911399675712 train_utils.py:377] train in step: 571
I0712 13:00:41.526936 139911399675712 train_utils.py:377] train in step: 572
I0712 13:00:41.553944 139911399675712 train_utils.py:377] train in step: 573
I0712 13:00:41.580791 139911399675712 train_utils.py:377] train in step: 574
I0712 13:00:41.607849 139911399675712 train_utils.py:377] train in step: 575
I0712 13:00:41.635827 139911399675712 train_utils.py:377] train in step: 576
I0712 13:00:41.661809 139911399675712 train_utils.py:377] train in step: 577
I0712 13:00:41.689214 139911399675712 train_utils.py:377] train in step: 578
I0712 13:00:41.718159 139911399675712 train_utils.py:377] train in step: 579
I0712 13:00:41.756653 139911399675712 train_utils.py:377] train in step: 580
I0712 13:00:41.784197 139911399675712 train_utils.py:377] train in step: 581
I0712 13:00:41.813002 139911399675712 train_utils.py:377] train in step: 582
I0712 13:00:41.838998 139911399675712 train_utils.py:377] train in step: 583
I0712 13:00:41.875212 139911399675712 train_utils.py:377] train in step: 584
I0712 13:00:41.901091 139911399675712 train_utils.py:377] train in step: 585
I0712 13:00:41.927064 139911399675712 train_utils.py:377] train in step: 586
I0712 13:00:41.954177 139911399675712 train_utils.py:377] train in step: 587
I0712 13:00:41.981480 139911399675712 train_utils.py:377] train in step: 588
I0712 13:00:42.008594 139911399675712 train_utils.py:377] train in step: 589
I0712 13:00:42.036019 139911399675712 train_utils.py:377] train in step: 590
I0712 13:00:42.062919 139911399675712 train_utils.py:377] train in step: 591
I0712 13:00:42.090597 139911399675712 train_utils.py:377] train in step: 592
I0712 13:00:42.117522 139911399675712 train_utils.py:377] train in step: 593
I0712 13:00:42.145232 139911399675712 train_utils.py:377] train in step: 594
I0712 13:00:42.172540 139911399675712 train_utils.py:377] train in step: 595
I0712 13:00:42.199573 139911399675712 train_utils.py:377] train in step: 596
I0712 13:00:42.226272 139911399675712 train_utils.py:377] train in step: 597
I0712 13:00:42.253464 139911399675712 train_utils.py:377] train in step: 598
I0712 13:00:42.280679 139911399675712 train_utils.py:377] train in step: 599
I0712 13:00:42.307865 139911399675712 train_utils.py:377] train in step: 600
I0712 13:00:42.368410 139911399675712 train_utils.py:396] train in step: 600, loss: 0.7119999527931213, acc: 0.5074999928474426
I0712 13:00:45.490558 139911399675712 train_utils.py:411] eval in step: 600, loss: 0.7069, acc: 0.4950
I0712 13:00:45.494969 139911399675712 train_utils.py:421] Testing...
I0712 13:00:48.692342 139911399675712 train_utils.py:424] test in step: 600, loss: 0.6946, acc: 0.5350
I0712 13:00:48.724962 139911399675712 train_utils.py:377] train in step: 601
I0712 13:00:48.748076 139911399675712 train_utils.py:377] train in step: 602
I0712 13:00:48.769123 139911399675712 train_utils.py:377] train in step: 603
I0712 13:00:48.791805 139911399675712 train_utils.py:377] train in step: 604
I0712 13:00:48.813920 139911399675712 train_utils.py:377] train in step: 605
I0712 13:00:48.836374 139911399675712 train_utils.py:377] train in step: 606
I0712 13:00:48.858285 139911399675712 train_utils.py:377] train in step: 607
I0712 13:00:48.883966 139911399675712 train_utils.py:377] train in step: 608
I0712 13:00:48.912199 139911399675712 train_utils.py:377] train in step: 609
I0712 13:00:48.938787 139911399675712 train_utils.py:377] train in step: 610
I0712 13:00:48.967287 139911399675712 train_utils.py:377] train in step: 611
I0712 13:00:48.993296 139911399675712 train_utils.py:377] train in step: 612
I0712 13:00:49.020920 139911399675712 train_utils.py:377] train in step: 613
I0712 13:00:49.047873 139911399675712 train_utils.py:377] train in step: 614
I0712 13:00:49.091372 139911399675712 train_utils.py:377] train in step: 615
I0712 13:00:49.118248 139911399675712 train_utils.py:377] train in step: 616
I0712 13:00:49.148967 139911399675712 train_utils.py:377] train in step: 617
I0712 13:00:49.176239 139911399675712 train_utils.py:377] train in step: 618
I0712 13:00:49.207368 139911399675712 train_utils.py:377] train in step: 619
I0712 13:00:49.232003 139911399675712 train_utils.py:377] train in step: 620
I0712 13:00:49.259395 139911399675712 train_utils.py:377] train in step: 621
I0712 13:00:49.286958 139911399675712 train_utils.py:377] train in step: 622
I0712 13:00:49.313314 139911399675712 train_utils.py:377] train in step: 623
I0712 13:00:49.340049 139911399675712 train_utils.py:377] train in step: 624
I0712 13:00:49.368116 139911399675712 train_utils.py:377] train in step: 625
I0712 13:00:49.395769 139911399675712 train_utils.py:377] train in step: 626
I0712 13:00:49.421821 139911399675712 train_utils.py:377] train in step: 627
I0712 13:00:49.449020 139911399675712 train_utils.py:377] train in step: 628
I0712 13:00:49.474994 139911399675712 train_utils.py:377] train in step: 629
I0712 13:00:49.506262 139911399675712 train_utils.py:377] train in step: 630
I0712 13:00:49.532816 139911399675712 train_utils.py:377] train in step: 631
I0712 13:00:49.560613 139911399675712 train_utils.py:377] train in step: 632
I0712 13:00:49.587164 139911399675712 train_utils.py:377] train in step: 633
I0712 13:00:49.613893 139911399675712 train_utils.py:377] train in step: 634
I0712 13:00:49.641277 139911399675712 train_utils.py:377] train in step: 635
I0712 13:00:49.668430 139911399675712 train_utils.py:377] train in step: 636
I0712 13:00:49.695579 139911399675712 train_utils.py:377] train in step: 637
I0712 13:00:49.722626 139911399675712 train_utils.py:377] train in step: 638
I0712 13:00:49.754201 139911399675712 train_utils.py:377] train in step: 639
I0712 13:00:49.785644 139911399675712 train_utils.py:377] train in step: 640
I0712 13:00:49.813506 139911399675712 train_utils.py:377] train in step: 641
I0712 13:00:49.840018 139911399675712 train_utils.py:377] train in step: 642
I0712 13:00:49.867473 139911399675712 train_utils.py:377] train in step: 643
I0712 13:00:49.899036 139911399675712 train_utils.py:377] train in step: 644
I0712 13:00:49.930382 139911399675712 train_utils.py:377] train in step: 645
I0712 13:00:49.956672 139911399675712 train_utils.py:377] train in step: 646
I0712 13:00:49.984269 139911399675712 train_utils.py:377] train in step: 647
I0712 13:00:50.010766 139911399675712 train_utils.py:377] train in step: 648
I0712 13:00:50.037965 139911399675712 train_utils.py:377] train in step: 649
I0712 13:00:50.071746 139911399675712 train_utils.py:377] train in step: 650
I0712 13:00:50.101144 139911399675712 train_utils.py:377] train in step: 651
I0712 13:00:50.128967 139911399675712 train_utils.py:377] train in step: 652
I0712 13:00:50.155485 139911399675712 train_utils.py:377] train in step: 653
I0712 13:00:50.185260 139911399675712 train_utils.py:377] train in step: 654
I0712 13:00:50.212151 139911399675712 train_utils.py:377] train in step: 655
I0712 13:00:50.248081 139911399675712 train_utils.py:377] train in step: 656
I0712 13:00:50.274307 139911399675712 train_utils.py:377] train in step: 657
I0712 13:00:50.302204 139911399675712 train_utils.py:377] train in step: 658
I0712 13:00:50.329271 139911399675712 train_utils.py:377] train in step: 659
I0712 13:00:50.356265 139911399675712 train_utils.py:377] train in step: 660
I0712 13:00:50.383316 139911399675712 train_utils.py:377] train in step: 661
I0712 13:00:50.410442 139911399675712 train_utils.py:377] train in step: 662
I0712 13:00:50.436192 139911399675712 train_utils.py:377] train in step: 663
I0712 13:00:50.465168 139911399675712 train_utils.py:377] train in step: 664
I0712 13:00:50.494648 139911399675712 train_utils.py:377] train in step: 665
I0712 13:00:50.522289 139911399675712 train_utils.py:377] train in step: 666
I0712 13:00:50.552075 139911399675712 train_utils.py:377] train in step: 667
I0712 13:00:50.576999 139911399675712 train_utils.py:377] train in step: 668
I0712 13:00:50.604389 139911399675712 train_utils.py:377] train in step: 669
I0712 13:00:50.629996 139911399675712 train_utils.py:377] train in step: 670
I0712 13:00:50.657503 139911399675712 train_utils.py:377] train in step: 671
I0712 13:00:50.685819 139911399675712 train_utils.py:377] train in step: 672
I0712 13:00:50.716401 139911399675712 train_utils.py:377] train in step: 673
I0712 13:00:50.744056 139911399675712 train_utils.py:377] train in step: 674
I0712 13:00:50.771164 139911399675712 train_utils.py:377] train in step: 675
I0712 13:00:50.797622 139911399675712 train_utils.py:377] train in step: 676
I0712 13:00:50.827404 139911399675712 train_utils.py:377] train in step: 677
I0712 13:00:50.854359 139911399675712 train_utils.py:377] train in step: 678
I0712 13:00:50.881322 139911399675712 train_utils.py:377] train in step: 679
I0712 13:00:50.908471 139911399675712 train_utils.py:377] train in step: 680
I0712 13:00:50.945476 139911399675712 train_utils.py:377] train in step: 681
I0712 13:00:50.972704 139911399675712 train_utils.py:377] train in step: 682
I0712 13:00:50.998926 139911399675712 train_utils.py:377] train in step: 683
I0712 13:00:51.026987 139911399675712 train_utils.py:377] train in step: 684
I0712 13:00:51.053872 139911399675712 train_utils.py:377] train in step: 685
I0712 13:00:51.080562 139911399675712 train_utils.py:377] train in step: 686
I0712 13:00:51.108660 139911399675712 train_utils.py:377] train in step: 687
I0712 13:00:51.136815 139911399675712 train_utils.py:377] train in step: 688
I0712 13:00:51.163277 139911399675712 train_utils.py:377] train in step: 689
I0712 13:00:51.190833 139911399675712 train_utils.py:377] train in step: 690
I0712 13:00:51.219109 139911399675712 train_utils.py:377] train in step: 691
I0712 13:00:51.246041 139911399675712 train_utils.py:377] train in step: 692
I0712 13:00:51.272986 139911399675712 train_utils.py:377] train in step: 693
I0712 13:00:51.300227 139911399675712 train_utils.py:377] train in step: 694
I0712 13:00:51.327817 139911399675712 train_utils.py:377] train in step: 695
I0712 13:00:51.354021 139911399675712 train_utils.py:377] train in step: 696
I0712 13:00:51.381309 139911399675712 train_utils.py:377] train in step: 697
I0712 13:00:51.408690 139911399675712 train_utils.py:377] train in step: 698
I0712 13:00:51.435631 139911399675712 train_utils.py:377] train in step: 699
I0712 13:00:51.465651 139911399675712 train_utils.py:377] train in step: 700
I0712 13:00:51.490204 139911399675712 train_utils.py:377] train in step: 701
I0712 13:00:51.518309 139911399675712 train_utils.py:377] train in step: 702
I0712 13:00:51.547672 139911399675712 train_utils.py:377] train in step: 703
I0712 13:00:51.580993 139911399675712 train_utils.py:377] train in step: 704
I0712 13:00:51.610333 139911399675712 train_utils.py:377] train in step: 705
I0712 13:00:51.639687 139911399675712 train_utils.py:377] train in step: 706
I0712 13:00:51.665635 139911399675712 train_utils.py:377] train in step: 707
I0712 13:00:51.693270 139911399675712 train_utils.py:377] train in step: 708
I0712 13:00:51.720174 139911399675712 train_utils.py:377] train in step: 709
I0712 13:00:51.748202 139911399675712 train_utils.py:377] train in step: 710
I0712 13:00:51.776662 139911399675712 train_utils.py:377] train in step: 711
I0712 13:00:51.813267 139911399675712 train_utils.py:377] train in step: 712
I0712 13:00:51.849596 139911399675712 train_utils.py:377] train in step: 713
I0712 13:00:51.876805 139911399675712 train_utils.py:377] train in step: 714
I0712 13:00:51.905114 139911399675712 train_utils.py:377] train in step: 715
I0712 13:00:51.935617 139911399675712 train_utils.py:377] train in step: 716
I0712 13:00:51.963069 139911399675712 train_utils.py:377] train in step: 717
I0712 13:00:51.992743 139911399675712 train_utils.py:377] train in step: 718
I0712 13:00:52.019278 139911399675712 train_utils.py:377] train in step: 719
I0712 13:00:52.049424 139911399675712 train_utils.py:377] train in step: 720
I0712 13:00:52.073814 139911399675712 train_utils.py:377] train in step: 721
I0712 13:00:52.099926 139911399675712 train_utils.py:377] train in step: 722
I0712 13:00:52.126872 139911399675712 train_utils.py:377] train in step: 723
I0712 13:00:52.153483 139911399675712 train_utils.py:377] train in step: 724
I0712 13:00:52.180268 139911399675712 train_utils.py:377] train in step: 725
I0712 13:00:52.207314 139911399675712 train_utils.py:377] train in step: 726
I0712 13:00:52.234807 139911399675712 train_utils.py:377] train in step: 727
I0712 13:00:52.262027 139911399675712 train_utils.py:377] train in step: 728
I0712 13:00:52.288293 139911399675712 train_utils.py:377] train in step: 729
I0712 13:00:52.317542 139911399675712 train_utils.py:377] train in step: 730
I0712 13:00:52.344175 139911399675712 train_utils.py:377] train in step: 731
I0712 13:00:52.371203 139911399675712 train_utils.py:377] train in step: 732
I0712 13:00:52.405400 139911399675712 train_utils.py:377] train in step: 733
I0712 13:00:52.427474 139911399675712 train_utils.py:377] train in step: 734
I0712 13:00:52.455059 139911399675712 train_utils.py:377] train in step: 735
I0712 13:00:52.482891 139911399675712 train_utils.py:377] train in step: 736
I0712 13:00:52.510261 139911399675712 train_utils.py:377] train in step: 737
I0712 13:00:52.537039 139911399675712 train_utils.py:377] train in step: 738
I0712 13:00:52.564855 139911399675712 train_utils.py:377] train in step: 739
I0712 13:00:52.593580 139911399675712 train_utils.py:377] train in step: 740
I0712 13:00:52.624448 139911399675712 train_utils.py:377] train in step: 741
I0712 13:00:52.652730 139911399675712 train_utils.py:377] train in step: 742
I0712 13:00:52.680644 139911399675712 train_utils.py:377] train in step: 743
I0712 13:00:52.707120 139911399675712 train_utils.py:377] train in step: 744
I0712 13:00:52.733894 139911399675712 train_utils.py:377] train in step: 745
I0712 13:00:52.761424 139911399675712 train_utils.py:377] train in step: 746
I0712 13:00:52.788792 139911399675712 train_utils.py:377] train in step: 747
I0712 13:00:52.815268 139911399675712 train_utils.py:377] train in step: 748
I0712 13:00:52.843544 139911399675712 train_utils.py:377] train in step: 749
I0712 13:00:52.869895 139911399675712 train_utils.py:377] train in step: 750
I0712 13:00:52.896734 139911399675712 train_utils.py:377] train in step: 751
I0712 13:00:52.923917 139911399675712 train_utils.py:377] train in step: 752
I0712 13:00:52.950796 139911399675712 train_utils.py:377] train in step: 753
I0712 13:00:52.978250 139911399675712 train_utils.py:377] train in step: 754
I0712 13:00:53.004665 139911399675712 train_utils.py:377] train in step: 755
I0712 13:00:53.031955 139911399675712 train_utils.py:377] train in step: 756
I0712 13:00:53.058802 139911399675712 train_utils.py:377] train in step: 757
I0712 13:00:53.086372 139911399675712 train_utils.py:377] train in step: 758
I0712 13:00:53.113374 139911399675712 train_utils.py:377] train in step: 759
I0712 13:00:53.140904 139911399675712 train_utils.py:377] train in step: 760
I0712 13:00:53.167248 139911399675712 train_utils.py:377] train in step: 761
I0712 13:00:53.194030 139911399675712 train_utils.py:377] train in step: 762
I0712 13:00:53.223278 139911399675712 train_utils.py:377] train in step: 763
I0712 13:00:53.250918 139911399675712 train_utils.py:377] train in step: 764
I0712 13:00:53.283257 139911399675712 train_utils.py:377] train in step: 765
I0712 13:00:53.311965 139911399675712 train_utils.py:377] train in step: 766
I0712 13:00:53.340139 139911399675712 train_utils.py:377] train in step: 767
I0712 13:00:53.368093 139911399675712 train_utils.py:377] train in step: 768
I0712 13:00:53.394553 139911399675712 train_utils.py:377] train in step: 769
I0712 13:00:53.421476 139911399675712 train_utils.py:377] train in step: 770
I0712 13:00:53.449454 139911399675712 train_utils.py:377] train in step: 771
I0712 13:00:53.475946 139911399675712 train_utils.py:377] train in step: 772
I0712 13:00:53.503273 139911399675712 train_utils.py:377] train in step: 773
I0712 13:00:53.530342 139911399675712 train_utils.py:377] train in step: 774
I0712 13:00:53.557613 139911399675712 train_utils.py:377] train in step: 775
I0712 13:00:53.584959 139911399675712 train_utils.py:377] train in step: 776
I0712 13:00:53.611655 139911399675712 train_utils.py:377] train in step: 777
I0712 13:00:53.638368 139911399675712 train_utils.py:377] train in step: 778
I0712 13:00:53.665848 139911399675712 train_utils.py:377] train in step: 779
I0712 13:00:53.692914 139911399675712 train_utils.py:377] train in step: 780
I0712 13:00:53.719814 139911399675712 train_utils.py:377] train in step: 781
I0712 13:00:53.746896 139911399675712 train_utils.py:377] train in step: 782
I0712 13:00:53.774152 139911399675712 train_utils.py:377] train in step: 783
I0712 13:00:53.800764 139911399675712 train_utils.py:377] train in step: 784
I0712 13:00:53.827903 139911399675712 train_utils.py:377] train in step: 785
I0712 13:00:53.856162 139911399675712 train_utils.py:377] train in step: 786
I0712 13:00:53.882287 139911399675712 train_utils.py:377] train in step: 787
I0712 13:00:53.909311 139911399675712 train_utils.py:377] train in step: 788
I0712 13:00:53.936723 139911399675712 train_utils.py:377] train in step: 789
I0712 13:00:53.972426 139911399675712 train_utils.py:377] train in step: 790
I0712 13:00:54.000650 139911399675712 train_utils.py:377] train in step: 791
I0712 13:00:54.045561 139911399675712 train_utils.py:377] train in step: 792
I0712 13:00:54.074097 139911399675712 train_utils.py:377] train in step: 793
I0712 13:00:54.100480 139911399675712 train_utils.py:377] train in step: 794
I0712 13:00:54.127758 139911399675712 train_utils.py:377] train in step: 795
I0712 13:00:54.154494 139911399675712 train_utils.py:377] train in step: 796
I0712 13:00:54.181920 139911399675712 train_utils.py:377] train in step: 797
I0712 13:00:54.208588 139911399675712 train_utils.py:377] train in step: 798
I0712 13:00:54.236027 139911399675712 train_utils.py:377] train in step: 799
I0712 13:00:54.263354 139911399675712 train_utils.py:377] train in step: 800
I0712 13:00:54.320770 139911399675712 train_utils.py:396] train in step: 800, loss: 0.7073999643325806, acc: 0.4861999750137329
I0712 13:00:57.422658 139911399675712 train_utils.py:411] eval in step: 800, loss: 0.6981, acc: 0.5200
I0712 13:00:57.426380 139911399675712 train_utils.py:421] Testing...
I0712 13:01:00.626126 139911399675712 train_utils.py:424] test in step: 800, loss: 0.7118, acc: 0.4800
I0712 13:01:00.657081 139911399675712 train_utils.py:377] train in step: 801
I0712 13:01:00.676958 139911399675712 train_utils.py:377] train in step: 802
I0712 13:01:00.698199 139911399675712 train_utils.py:377] train in step: 803
I0712 13:01:00.722191 139911399675712 train_utils.py:377] train in step: 804
I0712 13:01:00.742408 139911399675712 train_utils.py:377] train in step: 805
I0712 13:01:00.763697 139911399675712 train_utils.py:377] train in step: 806
I0712 13:01:00.790953 139911399675712 train_utils.py:377] train in step: 807
I0712 13:01:00.817901 139911399675712 train_utils.py:377] train in step: 808
I0712 13:01:00.844890 139911399675712 train_utils.py:377] train in step: 809
I0712 13:01:00.871921 139911399675712 train_utils.py:377] train in step: 810
I0712 13:01:00.899506 139911399675712 train_utils.py:377] train in step: 811
I0712 13:01:00.928575 139911399675712 train_utils.py:377] train in step: 812
I0712 13:01:00.953542 139911399675712 train_utils.py:377] train in step: 813
I0712 13:01:00.980951 139911399675712 train_utils.py:377] train in step: 814
I0712 13:01:01.007934 139911399675712 train_utils.py:377] train in step: 815
I0712 13:01:01.036733 139911399675712 train_utils.py:377] train in step: 816
I0712 13:01:01.061991 139911399675712 train_utils.py:377] train in step: 817
I0712 13:01:01.089351 139911399675712 train_utils.py:377] train in step: 818
I0712 13:01:01.117275 139911399675712 train_utils.py:377] train in step: 819
I0712 13:01:01.144334 139911399675712 train_utils.py:377] train in step: 820
I0712 13:01:01.171832 139911399675712 train_utils.py:377] train in step: 821
I0712 13:01:01.198811 139911399675712 train_utils.py:377] train in step: 822
I0712 13:01:01.227040 139911399675712 train_utils.py:377] train in step: 823
I0712 13:01:01.257382 139911399675712 train_utils.py:377] train in step: 824
I0712 13:01:01.285328 139911399675712 train_utils.py:377] train in step: 825
I0712 13:01:01.312523 139911399675712 train_utils.py:377] train in step: 826
I0712 13:01:01.339814 139911399675712 train_utils.py:377] train in step: 827
I0712 13:01:01.366707 139911399675712 train_utils.py:377] train in step: 828
I0712 13:01:01.394580 139911399675712 train_utils.py:377] train in step: 829
I0712 13:01:01.420546 139911399675712 train_utils.py:377] train in step: 830
I0712 13:01:01.448858 139911399675712 train_utils.py:377] train in step: 831
I0712 13:01:01.475654 139911399675712 train_utils.py:377] train in step: 832
I0712 13:01:01.503993 139911399675712 train_utils.py:377] train in step: 833
I0712 13:01:01.531036 139911399675712 train_utils.py:377] train in step: 834
I0712 13:01:01.557872 139911399675712 train_utils.py:377] train in step: 835
I0712 13:01:01.584836 139911399675712 train_utils.py:377] train in step: 836
I0712 13:01:01.611962 139911399675712 train_utils.py:377] train in step: 837
I0712 13:01:01.641010 139911399675712 train_utils.py:377] train in step: 838
I0712 13:01:01.667496 139911399675712 train_utils.py:377] train in step: 839
I0712 13:01:01.694344 139911399675712 train_utils.py:377] train in step: 840
I0712 13:01:01.721386 139911399675712 train_utils.py:377] train in step: 841
I0712 13:01:01.748650 139911399675712 train_utils.py:377] train in step: 842
I0712 13:01:01.775757 139911399675712 train_utils.py:377] train in step: 843
I0712 13:01:01.803685 139911399675712 train_utils.py:377] train in step: 844
I0712 13:01:01.831419 139911399675712 train_utils.py:377] train in step: 845
I0712 13:01:01.857265 139911399675712 train_utils.py:377] train in step: 846
I0712 13:01:01.884558 139911399675712 train_utils.py:377] train in step: 847
I0712 13:01:01.911927 139911399675712 train_utils.py:377] train in step: 848
I0712 13:01:01.940247 139911399675712 train_utils.py:377] train in step: 849
I0712 13:01:01.969497 139911399675712 train_utils.py:377] train in step: 850
I0712 13:01:01.996542 139911399675712 train_utils.py:377] train in step: 851
I0712 13:01:02.026947 139911399675712 train_utils.py:377] train in step: 852
I0712 13:01:02.054270 139911399675712 train_utils.py:377] train in step: 853
I0712 13:01:02.082955 139911399675712 train_utils.py:377] train in step: 854
I0712 13:01:02.110098 139911399675712 train_utils.py:377] train in step: 855
I0712 13:01:02.137214 139911399675712 train_utils.py:377] train in step: 856
I0712 13:01:02.163986 139911399675712 train_utils.py:377] train in step: 857
I0712 13:01:02.190659 139911399675712 train_utils.py:377] train in step: 858
I0712 13:01:02.221017 139911399675712 train_utils.py:377] train in step: 859
I0712 13:01:02.245356 139911399675712 train_utils.py:377] train in step: 860
I0712 13:01:02.272742 139911399675712 train_utils.py:377] train in step: 861
I0712 13:01:02.299838 139911399675712 train_utils.py:377] train in step: 862
I0712 13:01:02.327124 139911399675712 train_utils.py:377] train in step: 863
I0712 13:01:02.353671 139911399675712 train_utils.py:377] train in step: 864
I0712 13:01:02.380993 139911399675712 train_utils.py:377] train in step: 865
I0712 13:01:02.408314 139911399675712 train_utils.py:377] train in step: 866
I0712 13:01:02.435155 139911399675712 train_utils.py:377] train in step: 867
I0712 13:01:02.461894 139911399675712 train_utils.py:377] train in step: 868
I0712 13:01:02.489270 139911399675712 train_utils.py:377] train in step: 869
I0712 13:01:02.516446 139911399675712 train_utils.py:377] train in step: 870
I0712 13:01:02.544113 139911399675712 train_utils.py:377] train in step: 871
I0712 13:01:02.570503 139911399675712 train_utils.py:377] train in step: 872
I0712 13:01:02.598096 139911399675712 train_utils.py:377] train in step: 873
I0712 13:01:02.627422 139911399675712 train_utils.py:377] train in step: 874
I0712 13:01:02.654284 139911399675712 train_utils.py:377] train in step: 875
I0712 13:01:02.681261 139911399675712 train_utils.py:377] train in step: 876
I0712 13:01:02.708302 139911399675712 train_utils.py:377] train in step: 877
I0712 13:01:02.736032 139911399675712 train_utils.py:377] train in step: 878
I0712 13:01:02.763009 139911399675712 train_utils.py:377] train in step: 879
I0712 13:01:02.789554 139911399675712 train_utils.py:377] train in step: 880
I0712 13:01:02.816920 139911399675712 train_utils.py:377] train in step: 881
I0712 13:01:02.843964 139911399675712 train_utils.py:377] train in step: 882
I0712 13:01:02.871216 139911399675712 train_utils.py:377] train in step: 883
I0712 13:01:02.898265 139911399675712 train_utils.py:377] train in step: 884
I0712 13:01:02.925364 139911399675712 train_utils.py:377] train in step: 885
I0712 13:01:02.952954 139911399675712 train_utils.py:377] train in step: 886
I0712 13:01:02.979753 139911399675712 train_utils.py:377] train in step: 887
I0712 13:01:03.006462 139911399675712 train_utils.py:377] train in step: 888
I0712 13:01:03.033736 139911399675712 train_utils.py:377] train in step: 889
I0712 13:01:03.061933 139911399675712 train_utils.py:377] train in step: 890
I0712 13:01:03.088015 139911399675712 train_utils.py:377] train in step: 891
I0712 13:01:03.115412 139911399675712 train_utils.py:377] train in step: 892
I0712 13:01:03.142346 139911399675712 train_utils.py:377] train in step: 893
I0712 13:01:03.170650 139911399675712 train_utils.py:377] train in step: 894
I0712 13:01:03.197345 139911399675712 train_utils.py:377] train in step: 895
I0712 13:01:03.225188 139911399675712 train_utils.py:377] train in step: 896
I0712 13:01:03.251306 139911399675712 train_utils.py:377] train in step: 897
I0712 13:01:03.278818 139911399675712 train_utils.py:377] train in step: 898
I0712 13:01:03.306137 139911399675712 train_utils.py:377] train in step: 899
I0712 13:01:03.333040 139911399675712 train_utils.py:377] train in step: 900
I0712 13:01:03.360957 139911399675712 train_utils.py:377] train in step: 901
I0712 13:01:03.388136 139911399675712 train_utils.py:377] train in step: 902
I0712 13:01:03.415155 139911399675712 train_utils.py:377] train in step: 903
I0712 13:01:03.442186 139911399675712 train_utils.py:377] train in step: 904
I0712 13:01:03.470399 139911399675712 train_utils.py:377] train in step: 905
I0712 13:01:03.496803 139911399675712 train_utils.py:377] train in step: 906
I0712 13:01:03.524432 139911399675712 train_utils.py:377] train in step: 907
I0712 13:01:03.551626 139911399675712 train_utils.py:377] train in step: 908
I0712 13:01:03.583627 139911399675712 train_utils.py:377] train in step: 909
I0712 13:01:03.608494 139911399675712 train_utils.py:377] train in step: 910
I0712 13:01:03.637123 139911399675712 train_utils.py:377] train in step: 911
I0712 13:01:03.664564 139911399675712 train_utils.py:377] train in step: 912
I0712 13:01:03.692133 139911399675712 train_utils.py:377] train in step: 913
I0712 13:01:03.718321 139911399675712 train_utils.py:377] train in step: 914
I0712 13:01:03.745614 139911399675712 train_utils.py:377] train in step: 915
I0712 13:01:03.772382 139911399675712 train_utils.py:377] train in step: 916
I0712 13:01:03.800860 139911399675712 train_utils.py:377] train in step: 917
I0712 13:01:03.827149 139911399675712 train_utils.py:377] train in step: 918
I0712 13:01:03.854218 139911399675712 train_utils.py:377] train in step: 919
I0712 13:01:03.881724 139911399675712 train_utils.py:377] train in step: 920
I0712 13:01:03.910153 139911399675712 train_utils.py:377] train in step: 921
I0712 13:01:03.936643 139911399675712 train_utils.py:377] train in step: 922
I0712 13:01:03.968464 139911399675712 train_utils.py:377] train in step: 923
I0712 13:01:03.995563 139911399675712 train_utils.py:377] train in step: 924
I0712 13:01:04.023106 139911399675712 train_utils.py:377] train in step: 925
I0712 13:01:04.051357 139911399675712 train_utils.py:377] train in step: 926
I0712 13:01:04.101222 139911399675712 train_utils.py:377] train in step: 927
I0712 13:01:04.128707 139911399675712 train_utils.py:377] train in step: 928
I0712 13:01:04.159995 139911399675712 train_utils.py:377] train in step: 929
I0712 13:01:04.191665 139911399675712 train_utils.py:377] train in step: 930
I0712 13:01:04.234038 139911399675712 train_utils.py:377] train in step: 931
I0712 13:01:04.266745 139911399675712 train_utils.py:377] train in step: 932
I0712 13:01:04.293179 139911399675712 train_utils.py:377] train in step: 933
I0712 13:01:04.319588 139911399675712 train_utils.py:377] train in step: 934
I0712 13:01:04.346404 139911399675712 train_utils.py:377] train in step: 935
I0712 13:01:04.376724 139911399675712 train_utils.py:377] train in step: 936
I0712 13:01:04.409553 139911399675712 train_utils.py:377] train in step: 937
I0712 13:01:04.437188 139911399675712 train_utils.py:377] train in step: 938
I0712 13:01:04.463999 139911399675712 train_utils.py:377] train in step: 939
I0712 13:01:04.490955 139911399675712 train_utils.py:377] train in step: 940
I0712 13:01:04.521495 139911399675712 train_utils.py:377] train in step: 941
I0712 13:01:04.547607 139911399675712 train_utils.py:377] train in step: 942
I0712 13:01:04.576139 139911399675712 train_utils.py:377] train in step: 943
I0712 13:01:04.618206 139911399675712 train_utils.py:377] train in step: 944
I0712 13:01:04.646325 139911399675712 train_utils.py:377] train in step: 945
I0712 13:01:04.692270 139911399675712 train_utils.py:377] train in step: 946
I0712 13:01:04.734333 139911399675712 train_utils.py:377] train in step: 947
I0712 13:01:04.763782 139911399675712 train_utils.py:377] train in step: 948
I0712 13:01:04.794201 139911399675712 train_utils.py:377] train in step: 949
I0712 13:01:04.826994 139911399675712 train_utils.py:377] train in step: 950
I0712 13:01:04.855982 139911399675712 train_utils.py:377] train in step: 951
I0712 13:01:04.881608 139911399675712 train_utils.py:377] train in step: 952
I0712 13:01:04.910590 139911399675712 train_utils.py:377] train in step: 953
I0712 13:01:04.937674 139911399675712 train_utils.py:377] train in step: 954
I0712 13:01:04.965032 139911399675712 train_utils.py:377] train in step: 955
I0712 13:01:04.992180 139911399675712 train_utils.py:377] train in step: 956
I0712 13:01:05.019239 139911399675712 train_utils.py:377] train in step: 957
I0712 13:01:05.046370 139911399675712 train_utils.py:377] train in step: 958
I0712 13:01:05.074720 139911399675712 train_utils.py:377] train in step: 959
I0712 13:01:05.100793 139911399675712 train_utils.py:377] train in step: 960
I0712 13:01:05.127973 139911399675712 train_utils.py:377] train in step: 961
I0712 13:01:05.155039 139911399675712 train_utils.py:377] train in step: 962
I0712 13:01:05.182377 139911399675712 train_utils.py:377] train in step: 963
I0712 13:01:05.209338 139911399675712 train_utils.py:377] train in step: 964
I0712 13:01:05.237341 139911399675712 train_utils.py:377] train in step: 965
I0712 13:01:05.264243 139911399675712 train_utils.py:377] train in step: 966
I0712 13:01:05.291027 139911399675712 train_utils.py:377] train in step: 967
I0712 13:01:05.319603 139911399675712 train_utils.py:377] train in step: 968
I0712 13:01:05.346053 139911399675712 train_utils.py:377] train in step: 969
I0712 13:01:05.373192 139911399675712 train_utils.py:377] train in step: 970
I0712 13:01:05.400349 139911399675712 train_utils.py:377] train in step: 971
I0712 13:01:05.427956 139911399675712 train_utils.py:377] train in step: 972
I0712 13:01:05.458679 139911399675712 train_utils.py:377] train in step: 973
I0712 13:01:05.485453 139911399675712 train_utils.py:377] train in step: 974
I0712 13:01:05.513779 139911399675712 train_utils.py:377] train in step: 975
I0712 13:01:05.541006 139911399675712 train_utils.py:377] train in step: 976
I0712 13:01:05.567857 139911399675712 train_utils.py:377] train in step: 977
I0712 13:01:05.595163 139911399675712 train_utils.py:377] train in step: 978
I0712 13:01:05.622321 139911399675712 train_utils.py:377] train in step: 979
I0712 13:01:05.649647 139911399675712 train_utils.py:377] train in step: 980
I0712 13:01:05.677022 139911399675712 train_utils.py:377] train in step: 981
I0712 13:01:05.703821 139911399675712 train_utils.py:377] train in step: 982
I0712 13:01:05.731436 139911399675712 train_utils.py:377] train in step: 983
I0712 13:01:05.758531 139911399675712 train_utils.py:377] train in step: 984
I0712 13:01:05.785655 139911399675712 train_utils.py:377] train in step: 985
I0712 13:01:05.813097 139911399675712 train_utils.py:377] train in step: 986
I0712 13:01:05.841650 139911399675712 train_utils.py:377] train in step: 987
I0712 13:01:05.868067 139911399675712 train_utils.py:377] train in step: 988
I0712 13:01:05.895117 139911399675712 train_utils.py:377] train in step: 989
I0712 13:01:05.922683 139911399675712 train_utils.py:377] train in step: 990
I0712 13:01:05.949769 139911399675712 train_utils.py:377] train in step: 991
I0712 13:01:05.976847 139911399675712 train_utils.py:377] train in step: 992
I0712 13:01:06.005219 139911399675712 train_utils.py:377] train in step: 993
I0712 13:01:06.031589 139911399675712 train_utils.py:377] train in step: 994
I0712 13:01:06.058842 139911399675712 train_utils.py:377] train in step: 995
I0712 13:01:06.085614 139911399675712 train_utils.py:377] train in step: 996
I0712 13:01:06.112621 139911399675712 train_utils.py:377] train in step: 997
I0712 13:01:06.149433 139911399675712 train_utils.py:377] train in step: 998
I0712 13:01:06.187046 139911399675712 train_utils.py:377] train in step: 999
I0712 13:01:06.216245 139911399675712 train_utils.py:377] train in step: 1000
I0712 13:01:06.220413 139911399675712 checkpoints.py:120] Saving checkpoint at step: 1000
I0712 13:01:06.327229 139911399675712 checkpoints.py:149] Saved checkpoint at trained_models/matching/local/checkpoint_1000
I0712 13:01:06.372794 139911399675712 train_utils.py:396] train in step: 1000, loss: 0.7102999687194824, acc: 0.5238000154495239
I0712 13:01:09.576856 139911399675712 train_utils.py:411] eval in step: 1000, loss: 0.7081, acc: 0.4750
I0712 13:01:09.580082 139911399675712 train_utils.py:421] Testing...
I0712 13:01:12.768991 139911399675712 train_utils.py:424] test in step: 1000, loss: 0.6859, acc: 0.5450
