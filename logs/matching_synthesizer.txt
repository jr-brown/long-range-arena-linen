2022-07-12 13:29:25.647013: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:29:25.647223: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:29:25.647345: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2022-07-12 13:29:25.647473: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2022-07-12 13:29:25.647588: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:29:25.647716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:29:25.647830: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2022-07-12 13:29:25.647872: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0712 13:29:25.648356 140217736775488 train.py:67] ===========Config Dict============
I0712 13:29:25.648595 140217736775488 train.py:68] available_devices:
- 0
- 1
- 2
- 3
base_type: dual_encoder
batch_size: 4
checkpoint_freq: 10000
data_dir: google_datasets/doc_retrieval/tsv_data/
emb_dim: 128
eval_frequency: 200
factors: constant * linear_warmup * rsqrt_decay
learning_rate: 0.05
max_eval_target_length: 200
max_length: 4000
max_predict_token_length: 50
max_target_length: 200
mlp_dim: 512
model_type: synthesizer
num_eval_steps: 50
num_heads: 4
num_layers: 4
num_train_steps: 1001
pooling_mode: CLS
prompt: ''
qkv_dim: 128
random_seed: 0
restore_checkpoints: true
sampling_temperature: 0.6
sampling_top_k: 20
save_checkpoints: true
task_name: aan_pairs
tokenizer: char
trial: 0
vocab_file_path: google_datasets/doc_retrieval/aan/
warmup: 1000
weight_decay: 0.1

I0712 13:29:25.659317 140217736775488 xla_bridge.py:340] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0712 13:29:26.789777 140217736775488 xla_bridge.py:340] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0712 13:29:26.790522 140217736775488 xla_bridge.py:340] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0712 13:29:26.790759 140217736775488 train.py:80] GPU devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)]
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
I0712 13:29:26.790917 140217736775488 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
I0712 13:29:26.858535 140217736775488 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 13:29:26.890326 140217736775488 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 13:29:29.269406 140217736775488 input_pipeline.py:60] Data sample: OrderedDict([('label', 1.0), ('id1', b'C10-1084'), ('id2', b'E06-1006'), ('text1', b'b"1 Introduction Modern statistical machine translation (SMT) systems, regardless of whether they are word-, phrase- or syntax-based, typically use the word as the atomic unit of translation. While this approach works when translating between languages with limited morphology such as English and French, it has been found inadequate for morphologicallyrich languages like Arabic, Czech and Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). As a result, a line of SMT research has worked to incorporate morphological analysis to gain access to information encoded within individual words. In a typical MT process, word aligned data is fed as training data to create a translation model. In cases where a highly inflected language is involved, the current word-based alignment approaches produce low-quality alignment, as the statistical correspondences between source and ?This work was supported by a National Research Foundation grant ?Interactive Media Search? (grant # R-252-000325-279) target words are diffused over many morphological forms. This problem has a direct impact on end translation quality. Our work addresses this shortcoming by proposing a morphologically sensitive approach to word alignment for language pairs involving a highly inflected language. In particular, our method focuses on a set of closed-class morphemes (CCMs), modeling their influence on nearby words. With the model, we correct erroneous alignments in the initial IBM Model 4 runs and add new alignments, which results in improved translation quality. After reviewing related work, we give a case study for morpheme alignment in Section 3. Section 4 presents our four-step approach to construct and incorporate our CCM alignment model into the grow-diag process. Section 5 describes experiments, while Section 6 analyzes the system merits. We conclude with suggestions for future work. 2 Related Work MT alignment has been an active research area. One can categorize previous approaches into those that use language-specific syntactic information and those that do not. Syntactic parse trees have been used to enhance alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; DeNero and Klein, 2007; Zhang et al, 2008; Haghighi et al., 2009). With syntactic knowledge, modeling long distance reordering is possible as the search space is confined to plausible syntactic variants. However, they generally require language-specific tools and annotated data, making such approaches infeasible for many languages. Works that follow non-syntactic approaches, such as (Matusov et al, 743 i1 declare2 resumed3 the4 session5 of6 the7 european8 parliament9 adjourned10 on11 1312 december13 199614 -1 julistan2 euroopan3 parlamentin4 perjantaina5 136 joulukuuta7 19968 keskeytyneen9 istuntokauden10 uudelleen11 avatuksi12 Direct: 1-2 2-2 3-9 4-3 5-10 6-10 7-3 8-12 9-12 10-12 11-5 12-6 13-7 14-8Inverse: 1-1 2-2 8-3 9-4 10-5 12-6 13-7 14-8 10-9 10-10 10-11 10-12 (a) Gloss: -1 declare2 european3 parliament 4 on-friday5 136 december7 19968 adjourned9 session10 resumed11,12 i1 declare2 resume+3 d4 the5 session6 of7 the8 european9 parliament10 adjourn+11 ed12 on13 1314 december15 199616 - julist+ a+ n euroopa+ n parlament+ in perjantai+ n+ a 13 joulukuu+ ta 1996 keskeyty+ neen istunto+ kauden uude+ lle+ en avatuksi1   2      3   4        5         6          7           8        9          10 11 12      13        14   15         16           17        18            19        20       21  22       23Direct: 1-23 2-23 3-23 4-23 5-22 6-23 7-22 8-6 9-5 10-7 11-16 12-16 13-9 14-12 15-13 16-15Inverse: 1-1 2-2 2-3 5-4 9-5 8-6 10-7 10-8 11-9 0-10 7-11 14-12 15-13 15-14 16-15 11-16 11-17 11-18 11-19 11-20 11-21 0-22 11-23 (b) Figure 1: Sample English-Finnish IBM Model 4 alignments: (a) word-level and (b) morpheme-level. Solid lines indicate intersection alignments, while the exhaustive asymmetric alignments are listed below. In (a), translation glosses for Finnish are given; the dash-dot line is the incorrect alignment. In (b), bolded texts are closed-class morphemes (CCM), while bolded indices indicate alignments involving CCMs. The dotted lines are correct CCM alignments not found by IBM Model 4. 2004; Liang et al, 2006; Ganchev et al, 2008), which aim to achieve symmetric word alignment during training, though good in many cases, are not designed to tackle highly inflected languages. Our work differs from these by taking a middle road. Instead of modifying the alignment algorithm directly, we preprocess asymmetric alignments to improve the input to the symmetrizing process later. Also, our approach does not make use of specific language resources, relying only on unsupervised morphological analysis. 3 A Case for Morpheme Alignment The notion that morpheme based alignment would be useful in highly inflected languages is intuitive. Morphological inflections might indicate tense, gender or number that manifest as separate words in largely uninflected languages. Capturing these subword alignments can yield better word alignments that otherwise would be missed. Let us make this idea concrete with a case study of the benefits of morpheme based alignment. We show the intersecting alignments of an actual English (source) ? Finnish (target) sentence pair in Figure 1, where (a) word-level and (b) morphemelevel alignments are shown. The morphemelevel alignment is produced by automatically segmenting words into morphemes and running IBM Model 4 on the resulting token stream. Intersection links (i.e., common to both direct and inverse alignments) play an important role in creating the final alignment (Och and Ney, 2004). While there are several heuristics used in the symmetrizing process, the grow-diag(onal) process is common and prevalent in many SMT systems, such as Moses (Koehn et al, 2007). In the growdiag process, intersection links are used as seeds to find other new alignments within their neighborhood. The process continues iteratively, until no further links can beadded. In our example, the morpheme-level intersection alignment is better as it has no misalignments and adds new alignments. However it misses some key links. In particular, the alignments of closed-class morphemes (CCMs; later formally defined) as indicated by the dotted lines in (b) are overlooked in the IBM Model 4 alignment. This difficulty in aligning CCMs is due to: 1. Occurrences of garbage-collector words (Moore, 2004) that attract CCMs to align to them. Examples of such links in (b) are 1?23 or 11?21 with the occurrences of rare words adjourn+11 and avatuksi23. We further characterize such errors in Section 6.1. 2. Ambiguity among CCMs of the same surface that causes incorrect matchings. In (b), we observe multiple occurrence of the and n on the source and target sides respectively. While the link 8?6 is correct, 5?4 is not as i1 should be aligned to n4 instead. To resolve such ambiguity, context information should be considered as detailed in Section 4.3. The fact that rare words and multiple affixes often occur in highly inflected languages exacerbates this problem, motivating our focus on improving CCM alignment. Furthermore, having access to the correct CCM alignments as illustrated 744 in Figure 1 guides the grow-diag process in finding the remaining correct alignments. For example, the addition of CCM links i1?n4 and d4? lle21 helps to identify declare2?julist2 and resume3?avatuksi23 as admissible alignments, which would otherwise be missed. 4 Methodology Our idea is to enrich the standard IBM Model 4 alignment by modeling closed-class morphemes (CCMs) more carefully using global statistics and context. We realize our idea by proposing a fourstep method. First, we take the input parallel corpus and convert it into morphemes before training the IBM Model 4 morpheme alignment. Second, from the morpheme alignment, we induce automatically bilingual CCM pairs. The core of our approach is in the third and fourth steps. In Step 3, we construct a CCM alignment model, and apply it on the segmented input corpus to obtain an automatic CCM alignment. Finally, in Step 4, we incorporate the CCM alignment into the symmetrizing process via our modified grow-diag process. 4.1 Step 1: Morphological Analysis The first step presupposes morphologically segmented input to compute the IBM Model 4 morpheme alignment. Following Virpioja et al (2007), we use Morfessor, an unsupervised analyzer which learns morphological segmentation from raw tokenized text (Creutz and Lagus, 2007). The tool segments input words into labeled morphemes: PRE (prefix), STM (stem), and SUF (suffix). Multiple affixes can be proposed for each word; word compounding is allowed as well, e.g., uncarefully is analyzed as un/PRE+ care/STM+ ful/SUF+ ly/SUF. We append a ?+? sign to each nonfinal tag to distinguish wordinternal morphemes from word-final ones, e.g., ?x/STM? and ?x/STM+? are considered different tokens. The ?+? annotation enables the restoration of the original words, a key point to enforce word boundary constraints in our work later. 4.2 Step 2: Bilingual CCM Pairs We observe that low and highly inflected languages, while intrinsically different, share more en fi en fi en fi the1 -n?1 in6 -ssa?15 me166 -ni?60-s2 -t?9 is7on?2 me166 minun?282to3 -a?6 that8 etta??7 why168 siksi?187to3 maan91 that8 ettei?283 view172 mielta??162of4 -a4 we10 -mme?10 still181 viela??108of4 -en?5 we10 meida?n?52 where183 jossa?209of4 -sta?19 we10 me?113 same186 samaa?334and5 ja?3 we10 emme123 he187 ha?n?184and5 seka??122 we10 meilla??231 good189 hyva??321and5 eika?203 . . . . . . over-408 yli-?391 Table 1: English(en)-Finnish(fi) Bilingual CCM pairs (N=128). Shown are the top 19 and last 10 of 168 bilingual CCM pairs extracted. Subscript i indicates the ith most frequent morpheme in each language. ? marks exact correspondence linguistically, whereas ? suggests rough correspondence w.r.t http://en.wiktionary.org/wiki/. in common at the morpheme level. The manyto-one relationships among words on both sides is often captured better by one-to-one correspondences among morphemes. We wish to model such bilingual correspondence in terms of closedclass morphemes (CCM), similar to Nguyen and Vogel (2008)?s work that removes nonaligned affixes during the alignment process. Let us now formally define CCM and an associative measure to gauge such correspondence. Definition 1. Closed-class Morphemes (CCM) are a fixed set of stems and affixes that exhibit grammatical functions just like closed-class words. In highly inflected languages, we observe that grammatical meanings may be encoded in morphological stems and affixes, rather than separate words. While we cannot formally identify valid CCMs in a language-independent way (as by definition they manifest language-dependent grammatical functions), we can devise a good approximation. Following Setiawan et al (2007), we induce the set of CCMs for a language as the top N frequent stems together with all affixes1. Definition 2. Bilingual Normalized PMI (biPMI) is the averaged normalized PMI computed on the asymmetric morpheme alignments. Here, normalized PMI (Bouma, 2009), known to be less biased towards low-frequency data, is defined as: nPMI(x, y) = ln p(x,y)p(x)p(y))/- ln p(x, y), where p(x), p(y), and p(x, y) follow definitions in the standard PMI formula. In our case, we only 1Note that we employ length and vowel sequence heuristics to filter out corpus-specific morphemes. 745 compute the scores for x, y being morphemes frequently aligned in both asymmetric alignments. Given these definitions, we now consider a pair of source and target CCMs related and termed a bilingual CCM pair (CCM pair, for short) if they exhibit positive correlation in their occurrences (i.e., positive nPMI2 and frequent cooccurrences). We should note that relying on a hard threshold of N as in (Setiawan et al, 2007) is brittle as the CCM set varies in sizes across languages. Our method is superior in the use of N as a starting point only; the bilingual correspondence of the two languages will ascertain the final CCM sets. Take for example the en and fi CCM sets with 154 and 214 morphemes initially (each consisting of N=128 stems). As morphemes not having their counterparts in the other language are spurious, we remove them by retaining only those in the CCM pairs. This effectively reduces the respective sizes to 91 and 114. At the same time, these final CCMs cover a much larger range of top frequent morphemes than N , up to 408 en and 391 fi morphemes, as evidenced in Table1. 4.3 Step 3: The CCM Alignment Model The goal of this model is to predict when appearances of a CCM pair should be deemed as linking. With an identified set of CCM pairs, we know when source and target morphemes correspond. However, in a sentence pair there can be many instances of both the source and target morphemes. In our example, the the?n pair corresponds to definite nouns; there are two the and three -n instances, yielding 2? 3=6 possible links. Deciding which instances are aligned is a decision problem. To solve this, we inspect the IBM Model 4 morpheme alignment to construct a CCM alignment model. The CCM model labels whether an instance of a CCM pair is deemed semantically related (linked). We cast the modeling problem as supervised learning, where we choose a maximum entropy (ME) formulation (Berger et al, 1996). We first discuss sample selection from the IBM Model 4 morpheme alignment, and then give details on the features extracted. The processes described below are done per sentence pair with fm1 , 2nPMI has a bounded range of [?1, 1] with values 1 and 0 indicating perfect positive and no correlation, respectively. en1 and U denoting the source, target sentences and the union alignments, respectively. Class labels. We base this on the initial IBM Model 4 alignment to label each CCM pair instance as a positive or negative example: Positive examples are simply CCM pairs in U. To be precise, links j?i in U are positive examples if fj?ei is a CCM pair. To find negative examples, we inventory other potential links that share the same lexical items with a positive one. That is, a link j??i? not in U is a negative example, if a positive link j?i such that fj = f ?j and ei = e?i exists. We stress that our collection of positive examples contains high-precision but low-recall IBM Model 4 links, which connect the reliable CCM pairs identified before. The model then generalizes from these samples to detect incorrect CCM links and to recover the correct ones, enhancing recall. We later detail this process in ?4.4. Feature Set. Given a CCM pair instance, we construct three feature types: lexical, monolingual, and bilingual (See Table 2). These features capture the global statistics and contexts of CCM pairs to decide if they are true alignment links. ? Lexical features reflect the tendency of the CCM pair being aligned to themselves. We use biPMI, which aggregates the global alignment statistics, to determine how likely source and target CCMs are associated with each other. ? Monolingual context features measure the association among tokens of the same language, capturing what other stems and affixes co-occur with the source/target CCM: 1. within the same word (intra). The aim is to disambiguate affixes as necessary in highly inflected languages where same stems could generate different roles or meanings. 2. outside the CCM?s word boundary (inter). This potentially capture cues such as tense, or number agreement. For example, in English, the 3sg agreement marker on verbs -s often co-occurs with nearbypronouns e.g., he, she, it; whereas the same marker on nouns (-s), often appears with plural determiners e.g., these, those, many. To accomplish this, we compute two monolingual nPMI scores in the same spirit as biPMI, but using the morphologically segmented input from 746 Feature Description Examples Lexical ? biPMI: None [?1, 0], Low (0, 1/3], Medium (1/3, 2/3], High (2/3, 1] pmid?lle=LowMonolingual Context ? Capture morpheme cooccurrence with the src/tgt CCMIntra ? Within the same word srcWd?lle=resume, tgtWd?lle=en, tgtWd?lle=uudeInter ? To the Left & Right, in different words srcLd?lle=i, srcRd?lle=the, tgtRd?lle=avatuksiBilingual context ? Capture neighbor links? cooccurrence with the CCM pair linkbi0 ? Most descriptive, capturing in terms of surface forms only ? maybe sparse bi0d?lle=resume?avatuksibi1 ? Generalizes morphemes into relative locations (Left, Within, Right) bi1d?lle=W?avatuksi, bi1d?lle=resume?Rbi2 ? Most general, coupling token types (Close, Open) /w relative positions bi2d?lle=O?WR Table 2: Maximum entropy feature set. Shown are feature types, descriptions and examples. Most examples are given for the alignment d4?lle+21 of the same running example in ?3. Note that we only partially list the bilingual context features. each language separately. Two morphemes are ?linked? if within a context window of wc words. ? Bilingual context features model crosslingual reordering, capturing the relationships between the CCM pair link and its neighbor3 links. Consider a simple translation between an English phrase of the form we ?verb? and the Finnish one ?verb? -mme, where -mme is the 1pl verb marker. We aim to capture movements such as ?the open-class morphemes on the right of we and on the left of -mme are often aligned?. These will function as evidence for the ME learner to align the CCM pair (we, -mme). We encode the bilingual context at three different granularities, from most specific to most general ones (cf Table 2). 4.4 Step 4: Incorporate CCM Alignment At test time, we apply the trained CCM alignment model to all CCM pairs occurring in each sentence pair to find CCM links. On our running example in Figure 1, the CCM classifier tests 17 CCM pairs, identifying 6 positive CCM links of which 4 are true positives (dotted lines in (b)). Though mostly correct, we note that some of the predicted links conflict: (d4?lle21, ed12? neen17 and ed12?lle21) share alignment endpoints. Such sharing in CCM alignments is rare and we believe should be disallowed. This motivates us to resolve all CCM link conflicts before incorporating them into the symmetrizing process. Resolving link conflicts. As CCM pairs are classified independently, they possess classification probabilities which we use as evidence to resolve the conflicts. In our example, the classification probabilities for (d4?lle21, ed12?neen17, ed12?lle21) are (0.99, 0.93, 0.79) respectively. We use a simple, ?best-first? greedy approach 3Within a context window of wc words as in monolingual. to determine which links are kept and which are dropped to satisfy our assumption. In our case, we pick the most confident link, d4?lle21 with probability 0.99. This precludes the incorrect link, ed12?lle21, but admits the other correct one ed12?neen17, probability 0.93. As a result, this resolution successfully removes the incorrect link. Modifying grow-diag.We incorporate the set of conflict-resolved CCM links into the grow-diag process. This step modifies the input alignments as well as the growing process. U and I denote the IBM Model 4 union and intersection alignments. In our view, the resolved CCM links can serve as a quality mark to ?upgrade? links before input into the grow-diag process. We upgrade resolved CCM links: (a) those ? U ? part of I , treating them as alignment seeds; (b) those /? U ? part of U , using them for exploration and growing. To reduce spurious alignments, we discarded links in U that conflict with the resolved CCM links. In the usual grow-diag, links immediately adjacent to a seed link l are considered candidates to be appended into the alignment seeds. While suitable for word-based alignment, we believe it is too small a context when the input are morphemes. For morpheme alignment, the candidate context makes more sense in terms of word units. We thus enforce word boundaries in our modified growdiag. We derive word boundaries for end points in l using the morphological tags and the ?+? wordend marker mentioned in ?4.1. Using such boundaries, we can then extend the grow-diag to consider candidate links within a neighborhood of wg words; hence, enhancing the candidate coverage. 5 Experiments We use English-Finnish and English-Hungarian data from past shared tasks (WPT05 and WMT09) 747 to validate our approach. Both Finnish and Hungarian are highly inflected languages, with numerous verbal and nominal cases, exhibiting agreement. Dataset statistics are given in Table 3. en-fi # en-hu # Train Europarl-v1 714K Europarl-v4 1,510K LM Europarl-v1-fi 714K News-hu 4,209K Dev wpt05-dev 2000 news-dev2009 2051 Test wpt05-test 2000 news-test2009 3027 Table 3: Dataset Statistics: the numbers of parallel sentences for training, LM training, development and test sets. We use the Moses SMT framework for our work, creating both our CCM-based systems and the baselines. In all systems built, we obtain the IBM Model 4 alignment via GIZA++ (Och and Ney, 2003). Results are reported using caseinsensitive BLEU (Papineni et al, 2001). Baselines. We build two SMT baselines: w-system: This is a standard phrase-based SMT, which operates at the word level. The system extracts phrases of maximum length 7 words, and uses a 4-gram word-based LM. wm-system: This baseline works at the word level just like the w-system, but differs at the alignment stage. Specifically, input to the IBM Model 4 training is the morpheme-level corpus, segmented by Morfessor and augmented with ?+? to provide word-boundary information (?4.1). Using such information, we constrain the alignment symmetrization to extract phrase pairs of 7 words or less in length. The morpheme-based phrase table is then mapped back to word forms. The process continues identically as in the w-system. CCM-based systems. Our CCM-based systems are similar in spirit to the wm system: train at the morpheme, but decode at the word level. We further enhance the wm-system at the alignment stage. First, we train our CCM model based on the initial IBM Model 4 morpheme alignment, and apply it to the morpheme corpusto obtain CCM alignment, which are input to our modified growdiag process. The CCM approach defines the setting of three parameters: ?N , wc, wg? (Section 4). Due to our resource constraints, we set N=128, similar to (Setiawan et al, 2007), and wc=1 experimentally. We only focus on the choice of wg, testing wg={1, 2} to explore the effect of enforcing word boundaries in the grow-diag process. 5.1 English-Finnish results We test the translation quality of both directions (en-fi) and (fi-en). We present results in Table 4 for 7 systems, including: our baselines, three CCMbased systems with word-boundary knowledge wg={0, 1, 2} and two wm-systems wg={1, 2}. Results in Table 4 show that our CCM approach effectively improves the performance. Compared to the wm-system, it chalks up a gain of 0.46 BLEU points for en-fi, and a larger improvement of 0.93 points for the easier, reverse direction. Further using word boundary knowledge in our modified grow-diag process demonstrates that the additional flexibility consistently enhances BLEU for wg = 1, 2. We achieve the best performance at wg = 2 with improvements of 0.67 and 1.22 BLEU points for en-fi and fi-en, respectively. en-fi fi-en w-system 14.58 23.56 wm-system 14.47 22.89 wm-system + CCM 14.93+0.46 23.82+0.93 wm-system + CCM + wg = 1 15.01 23.95 wm-system + CCM + wg = 2 15.14+0.67 24.11+1.22 wm-system + wg = 1 14.44 22.92 wm-system + wg = 2 14.28 23.01 (Ganchev, 2008) - Base 14.72 22.78 (Ganchev, 2008) - Postcat 14.74 23.43+0.65 (Yang, 2006) - Base N/A 22.0 (Yang, 2006) - Backoff N/A 22.3+0.3 Table 4: English/Finnish results. Shown are BLEU scores (in %) with subscripts indicating absolute improvements with respect to the wm-system baseline. Interestingly, employing the word boundary heuristic alone in the original grow-diag does not yield any improvement for en-fi, and even worsens as wg is enlarged (as seen in Rows 6?7). There are only slight improvements for fi-en with larger wg.This attests to the importance of combining the CCM model and the modified grow-diag process. Our best system outperforms the w-system baseline by 0.56 BLEU points for en-fi, and yields an improvement of 0.55 points for fi-en. Compared to works experimenting en/fi translation, we note the two prominent ones by Yang and Kirchhoff (2006) and recently by Ganchev et al (2008). The former uses a simple back-off method experimenting only fi-en, yielding an improvement of 0.3 BLEU points. Work in the op748 posite direction (en-fi) is rare, with the latter paper extending the EM algorithm using posterior constraints, but showing no improvement; for fien, they demonstrate a gain of 0.65 points. Our CCM method compares favorably against both approaches, which use the same datasets as ours. 5.2 English-Hungarian results To validate our CCM method as languageindependent, we also perform preliminary experiments on en-hu. Table 5 shows the results using the same CCM setting and experimental schemes as in en/fi. An improvement of 0.35 BLEU points is shown using the CCM model. We further improve by 0.44 points with word boundary wg=1, but performance degrades for the larger window. Due to timeconstraints, we leave experiments for the reverse, easier direction as future work. Though modest, the best improvement for en-hu is statistical significant at p<0.01 according to Collins? sign test (Collins et al, 2005). System BLEU w-system 9.63 wm-system 9.47 wm-system + CCM 9.82 +0.35 wm-system + CCM + wg = 1 9.91 +0.44 wm-system + CCM + wg = 2 9.87 Table 5: English/Hungarian results. Subscripts indicate absolute improvements with respect to the wm-system. We note that MT experiments for en/hu 4 are very limited, especially for the en to hu direction. Nova?k (2009) obtained an improvement of 0.22 BLEU with no distortion penalty; whereas Koehn and Haddow (2009) enhanced by 0.5 points using monotone-at-punctuation reordering, minimum Bayes risk and larger beam size decoding. While not directly comparable in the exact settings, these systems share the same data source and splits similar to ours. In view of these community results, we conclude that our CCM model does perform competitively in the en-hu task, and indeed seems to be language independent. 6 Detailed Analysis The macroscopic evaluation validates our approach as improving BLEU over both baselines, 4Hungarian was used in the ACL shared task 2008, 2009. but how do the various components contribute? We first analyze the effects of Step 4 in producing the CCM alignment, and then step backward to examine the contribution of the different feature classes in Step 3 towards the ME model. 6.1 Quality of CCM alignment To evaluate the quality of the predicted CCM alignment, we address the following questions: Q1: What is the portion of CCM pairs being misaligned in the IBM Model 4 alignment? Q2: How does the CCM alignment differ from the IBM Model 4 alignment? Q3: To what extent do the new links introduced by our CCM model address Q1? Given that we do not have linguistic expertise in Finnish or Hungarian, it is not possible to exhaustively list all misaligned CCM pairs in the IBM Model 4 alignment. As such, we need to find other form of approximation in order to address Q1. We observe that correct links that do not exist in the original alignment could be entirely missing, or mistakenly aligned to neighboring words. With morpheme input, we can also classify mistakes with respect to intra- or inter-word errors. Figure 2 characterizes errors T1, T2 and T3, each being a more severe error class than the previous. Focusing on ei in the figure, links connecting ei to fj? or fj?? are deemed T1 errors (misalignments happen on one side). A T2 error aligns f ??j within the same word, while a T3 error aligns it outside the current word but still within its neighborhood. This characterization is automatic, cheap and has the advantage of being language-independent. fj fj\' fj?? 1 word T1T2T3 1 word ei ei\' ei?? Figure 2: Categorization of CCM missing links. Given that a CCM pair link (fj?ei) is not present in the IBM Model 4, occurrences of any nearby link of the types T[1?3] can be construed as evidence of a potential misalignment. Statistics in Table 6(ii)answers Q1, suggesting a fairly large number of missing CCM links: 3, 418K for en/fi and 6, 216K for en/hu, about 12.35% and 12.06% of the IBM Model 4 union alignment respectively. We see that T1 errors con749 stitute the majority, a reasonable reflection of the garbage- collector5 effect discussed in Section 3. General (i) Missing CCM links (ii)en/fi en/hu en/fi en/hu Direct 17,632K 34,312K T1 2,215K 3,487KInverse 18,681K 34,676K T2 358K 690K D ? I 8,643K 17,441K T3 845K 2,039K D ? I 27,670K 51,547K Total 3,418K 6,216K Table 6: IBM Model 4 alignment statistics. (i) General statistics. (ii) Potentially missing CCM links. Q2 is addressed by the last column in Table 7. Our CCM model produces about 11.98% (1,035K/8,643K) new CCM links as compared to the size of the IBM Model 4 intersection alignment for en/fi, and similarly, 9.52% for en/hu. Orig. Resolved I U\\\\I New en/fi 5,299K 3,433K 1065K 1,332K 1,035K en/hu 9,425K 6,558K 2,752K 2,146K 1,660K Table 7: CCM vs IBM Model 4 alignments. Orig. and Resolved give # CCM links predicted in Step 4 before and after resolving conflicts. Also shown are the number of resolved links present in the Intersection, Union excluding I (U\\\\I) of the IBM Model 4 alignment and New CCM links. Lastly, figures in Table 8 answer Q3, revealing that for en/fi, 91.11% (943K/1,035K) of the new CCM links effectively cover the missing CCM alignments, recovering 27.59% (943K/3,418K) of all missing CCM links. Our modified grow-diag realizes a majority 76.56% (722K/943K) of these links in the final alignment. We obtain similar results in the en/hu pair for link recovery, but a smaller percentage 22.59% (330K/1,461K) are realized through the modified symmetrization. This partially explains why improvements are modest for en/hu. New CCM Links (i) Modified grow-diag (ii) en/fi en/hu en/fi en/hu T1 707K 1,002K 547K 228K T2 108K 146K 79K 22K T3 128K 313K 96K 80KTotal 943K 1,461K 722K 330K Table 8: Quality of the newly introduced CCM links. Shown are # new CCM links addressing the three error types before (i) and after (ii) the modified grow-diag process. 6.2 Contributions of ME Feature Classes We also evaluate the effectiveness the ME features individually through ablation tests. For brevity, 5E.g., ei prefers f?j or f??j (garbage collectors) over fj . we only examine the more difficult translation direction, en to fi. Results in Table 9 suggest that all our features are effective, and that removing any feature class degrades performance. Balancing specificity and generality, bi1 is the most influential feature in the bilingual context group. For monolingual context, inter, which captures larger monolingual context, outperforms intra. The most important feature overall is pmi, which captures global alignment preferences. As feature groups, bilingual and monolingual context features are important sources of information, as removing them drastically decreases system performance by 0.23 and 0.16 BLEU, respectively. System BLEU all (wm-system+CCM) 14.93 ?bi2 14.90 ?intra 14.89 ?bi1 14.84??0.09 ?pmi 14.81??0.12 ?bi0 14.89 ?bi{2/1/0} 14.70??0.23 ?inter 14.85 ?in{ter/tra} 14.77??0.16 Table 9: ME feature ablation tests for English-Finnish experiments. ? mark results statistically significant at p < 0.05,differences are subscripted. 7 Conclusion and Future Work In this work, we have proposed a languageindependent model that addresses morpheme alignment problems involving highly inflected languages. Our method is unsupervised, requiring no language specific information or resources, yet its improvement on BLEU is comparable to much semantically richer, language-specific work. As our approach deals only with input word alignment, any downstream modifications of the translation model also benefit. As alignment is a central focus in this work, we plan to extend our work over different and multiple input alignments. We also feel that better methods for the incorporation of CCM alignments is an area for improvement. In the en/hu pair, a large proportion of discovered CCM links are discarded, in favor of spurious links from the union alignment. Automatic estimation of the correctness of our CCM alignments may improve end translation quality over our heuristic method. 750"'), ('text2', b"b'1 Introduction Current statistical machine translation (SMT) usually works well in cases where the domain is fixed, the training and test data match, and a large amount of training data is available. Nevertheless, standard SMT models tend to perform much better on languages that are morphologically simple, whereas highly inflected languages with a large number of potential word forms are more problematic, particularly when training data is sparse. SMT attempts to find a sentence e? in the desired output language given the corresponding sentence f in the source language, according to e? = argmaxeP (f |e)P (e) (1) Most state-of-the-art SMT adopt a phrase-based approach such that e is chunked into I phrases e?1, ..., e?I and the translation model is defined over mappings between phrases in e and in f . i.e. P (f? |e?). Typically, phrases are extracted from a word-aligned training corpus. Different inflected forms of the same lemma are treated as different words, and there is no provision for unseen forms, i.e. unknown words encountered in the test data are not translated at all but appear verbatim in the output. Although the percentage of such unseen word forms may be negligible when the training set is large and matches the test set well, it may rise drastically when training data is limited or from a different domain. Many current and future applications of machine translation require the rapid porting of existing systems to new languages and domains without being able to collect appropriate training data; this problem can therefore be expected to become increasingly more important. Furthermore, untranslated words can be one of the main factors contributing to low user satisfaction in practical applications. Several previous studies (see Section 2 below) have addressed issues of morphology in SMT, but most of these have focused on the problem of word alignment and vocabulary size reduction. Principled ways of incorporating different levels of morphological abstraction into phrase-based models have mostly been ignored so far. In this paper we propose a hierarchical backoff model for phrasebased translation that integrates several layers of morphological operations, such that more specific models are preferred over more general models. We experimentally evaluate the model on translation from two highly-inflected languages, German and Finnish, into English and present improvements over a state-of-the-art system. The rest of the paper is structured as follows: The following section discusses related background work. Section 4 describes the proposed model; Sections 5 and 6 provide details about the data and baseline system used in this study. Section 7 provides experimental results and discussion. Section 8 concludes. 41 2 Morphology in SMT Systems Previous approaches have used morpho-syntactic knowledge mainly at the low-level stages of a machine translation system, i.e. for preprocessing. (Niessen and Ney, 2001a) use morpho-syntactic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon,2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languages such as Romanian (Fraser and Marcu, 2005) in order to decrease word alignment error rate. In (Niessen and Ney, 2001b), a hierarchical lexicon model is used that represents words as combinations of full forms, base forms, and part-of-speech tags, and that allows the word alignment training procedure to interpolate counts based on the different levels of representation. (Goldwater and McCloskey, 2005) investigate various morphological modifications for Czech-English translations: a subset of the vocabulary was converted to stems, pseudowords consisting of morphological tags were introduced, and combinations of stems and morphological tags were used as new word forms. Small improvements were found in combination with a word-to-word translation model. Most of these techniques have focused on improving word alignment or reducing vocabulary size; however, it is often the case that better word alignment does not improve the overall translation performance of a standard phrase-based SMT system. Phrase-based models themselves have not benefited much from additional morpho-syntactic knowledge; e.g. (Lioma and Ounis, 2005) do not report any improvement from integrating part-ofspeech information at the phrase level. One successful application of morphological knowledge is (de Gispert et al, 2005), where knowledge-based morphological techniques are used to identify unseen verb forms in the test text and to generate inflected forms in the target language based on annotated POS tags and lemmas. Phrase prediction in the target language is conditioned on the phrase in the source language as well the corresponding tuple of lemmatized phrases. This technique worked well for translating from a morphologically poor language (English) to a more highly inflected language (Spanish) when applied to unseen verb forms. Treating both known and unknown verbs in this way, however, did not result in additional improvements. Here we extend the notion of treating known and unknown words differently and propose a backoff model for phrasebased translation. 3 Backoff Models Generally speaking, backoff models exploit relationships between more general and more specific probability distributions. They specify under which conditions the more specific model is used and when the model ?backs off? to the more general distribution. Backoff models have been used in a variety of ways in natural language processing, most notably in statistical language modeling. In language modeling, a higher-order n-gram distribution is used when it is deemed reliable (determined by the number of occurrences in the training data); otherwise, the model backs off to the next lower-order n-gram distribution. For the case of trigrams, this can be expressed as: pBO(wt|wt?1, wt?2) (2) = { dcpML(wt|wt?1, wt?2) if c > ? ?(wt?1, wt?2)pBO(wt|wt?1) otherwise where pML denotes the maximum-likelihood estimate, c denotes the count of the triple (wi, wi?1, wi?2) in the training data, ? is the count threshold above which the maximum-likelihood estimate is retained, and dN(wi,wi?1,wi?2) is a discounting factor (generally between 0 and 1) that is applied to the higher-order distribution. The normalization factor ?(wi?1, wi?2) ensures that the distribution sums to one. In (Bilmes and Kirchhoff, 2003) thismethod was generalized to a backoff model with multiple paths, allowing the combination of different backed-off probability estimates. Hierarchical backoff schemes have also been used by (Zitouni et al, 2003) for language modeling and by (Gildea, 2001) for semantic role labeling. (Resnik et al, 2001) used backoff translation lexicons for cross-language information retrieval. More recently, (Xi and Hwa, 2005) have used backoff models for combining in-domain and 42 out-of-domain data for the purpose of bootstrapping a part-of-speech tagger for Chinese, outperforming standard methods such as EM. 4 Backoff Models in MT In order to handle unseen words in the test data we propose a hierarchical backoff model that uses morphological information. Several morphological operations, in particular stemming and compound splitting, are interleaved such that a more specific form (i.e. a form closer to the full word form) is chosen before a more general form (i.e. a form that has undergone morphological processing). The procedure is shown in Figure 1 and can be described as follows: First, a standard phrase table based on full word forms is trained. If an unknown word fi is encountered in the test data with context cfi = fi?n, ..., fi?1, fi+1, ..., fi+m, the word is first stemmed, i.e. f ?i = stem(fi). The phrase table entries for words sharing the same stem are then modified by replacing the respective words with their stems. If an entry can be found among these such that the source language side of the phrase pair consists of fi?n, ..., fi?1, stem(fi), fi+1, ..., fi+m, the corresponding translation is used (or, if several possible translations occur, the one with the highest probability is chosen). Note that the context may be empty, in which case a single-word phrase is used. If this step fails, the model backs off to the next level and applies compound splitting to the unknown word (further described below), i.e.(f ??i1, f ??i2) = split(fi). The match with the original word-based phrase table is then performed again. If this step fails for either of the two parts of f ??, stemming is applied again: f ???i1 = stem(f ??i1) and f ???i2 = stem(f ??i2), and a match with the stemmed phrase table entries is carried out. Only if the attempted match fails at this level is the input passed on verbatim in the translation output. The backoff procedure could in principle be performed on demand by a specialized decoder; however, since we use an off-the-shelf decoder (Pharaoh (Koehn, 2004)), backoff is implicitly enforced by providing a phrase-table that includes all required backoff levels and by preprocessing the test data accordingly. The phrase table will thus include entries for phrases based on full word forms as well as for their stemmed and/or split counterparts. For each entry with decomposed morphological i i i i1 i2 i i1 i1 i2 i2 i1 i2 Figure 1: Backoff procedure. forms, four probabilities need to be provided: two phrasal translation scores for both translation directions, p(e?|f?) and p(f? |e?), and two corresponding lexical scores, which are computed as a product of theword-by-word translation probabilities under the given alignment a: plex(e?|f?) = J ? j=1 1 |j|a(i) = j| I ? a(i)=j p(fj |ei) (3) where j ranges of words in phrase f? and i ranges of words in phrase e?. In the case of unknown words in the foreign language, we need the probabilities p(e?|stem(f?)), p(stem(f?)|e?) (where the stemming operation stem(f?) applies to the unknown words in the phrase), and their lexical equivalents. These are computed by relative frequency estimation, e.g. p(e?|stem(f?)) = count(e?, stem(f?))count(stem(f?)) (4) The other translation probabilities are computed analogously. Since normalization is performed over the entire phrase table, this procedure has the effect of discounting the original probability porig(e?|f?) since e? may now have been generated by either f? or by stem(f?). In the standard formulation of backoff models shown in Equation 3, this amounts to: pBO(e?|f?) (5) = { de?,f?porig(e?|f?) if c(e?, f?) > 0 p(e?|stem(f?)) otherwise 43 where de?,f? = 1 ? p(e?, stem(f?)) p(e?, f?) (6) is the amount by which the word-based phrase translation probability is discounted. Equivalent probability computations are carried out for the lexical translation probabilities. Similar to the backoff level that uses stemming, the translation probabilities need to be recomputed for the levels that use splitting and combined splitting/stemming. In order to derive the morphological decomposition we use existing tools. For stemming we use the TreeTagger (Schmid, 1994) for German and the Snowball stemmer1 for Finnish. A variety of ways for compound splitting have been investigated in machine translation (Koehn, 2003). Here we use a simple technique that considers all possible ways of segmenting a word into two subparts (with a minimum-length constraint of three characters on each subpart). A segmentation is accepted if the subparts appear as individual items in the training data vocabulary. The only linguistic knowledge used in the segmentation process is the removal of final <s> from the first part of the compound before trying to match it to an existing word. This character (Fugen-s) is often inserted as ?glue? when forming German compounds. Other glue characters were not considered for simplicity (but could be added in the future). The segmentation method is clearly not linguistically adequate: first, words may be split into more than two parts. Second, the method may generate multiple possible segmentations without a principled way of choosing among them; third, it may generate invalid splits. However, a manual analysis of 300 unknown compounds in the German development set (see next section) showed that 95.3% of them were decomposed correctly: for the domain at hand, most compounds need not be split into more than two parts; if one part is itself a compound it is usually frequent enough in the training data to have a translation. Furthermore, lexicalized compounds, whose decomposition would lead to wrong translations, are also typically frequent words and have an appropriate translation in the training data. 1http://snowball.tartarus.org 5 Data Our data consists of the Europarl training, development and test definitions for German-English and Finnish-English of the 2005 ACL shared data task (Koehn and Monz, 2005). Both German and Finnish are morphologicallyrich languages: German has four cases and three genders and shows number, gender and case distinctions not only on verbs, nouns, and adjectives, but also on determiners. In addition, it has notoriously many compounds. Finnish is a highly agglutinative language with a large number of inflectional paradigms (e.g. one for each of its 15 cases). Noun compounds are also frequent. On the 2005 ACL shared MT data task, Finnish to English translation showed the lowest average performance (17.9% BLEU) and German had the second lowest (21.9%), while the average BLEU scores for French-to-English and Spanish-to-English were much higher (27.1% and 27.8%, respectively). The data was preprocessed by lowercasing and filtering out sentence pairs whose length ratio (number of words in the source language divided by the number of words in the target language, or vice versa) was > 9. The development and test sets consist of 2000 sentences each. In order to study the effect of varying amounts of training data we created several training partitions consisting of random selections of a subset of the full training set. The sizes of the partitions are shown in Table 1, together with the resulting percentage of out-of-vocabulary (OOV) words in the development and test sets (?type? refers to a unique word in the vocabulary, ?token? to an instance in the actual text). 6 System We use a two-pass phrase-based statistical MT system using GIZA++ (Och and Ney, 2000) for word alignment and Pharaoh (Koehn, 2004) for phrase extraction and decoding. Word alignment is performed in both directions using the IBM4 model. Phrases are then extracted from the word alignments using the method described in (Och and Ney, 2003). For first-pass decoding we use Pharaoh in n-best mode. The decoder uses a weighted combination of seven scores: 4 translation model scores (phrase-based and lexical scores for both directions), a trigram language model score, a distortion score, and a word penalty. Nonmonotonic decoding is used, with no limit on the 44 German-English Set # sent # words oov dev oov test train1 5K 101K 7.9/42.6 7.9/42.7 train2 25K 505K 3.8/22.1 3.7/21.9 train3 50K 1013K 2.7/16.1 2.7/16.1 train4 250K 5082K 1.3/8.1 1.2/7.5 train5 751K 15258K 0.8/4.9 0.7/4.4 Finnish-English Set # sent # words oov dev oov test train1 5K 78K 16.6/50.6 16.4/50.6 train2 25K 395K 8.6/28.2 8.4/27.8 train3 50K 790K 6.3/21.0 6.2/20.8 train4 250K 3945K 3.1/10.4 3.0/10.2 train5 717K 11319K 1.8/6.2 1.8/6.1 Table 1: Training set sizes and percentages of OOV words (types/tokens) on the development and test sets. dev test Finnish-English 22.2 22.0 German-English 24.6 24.8 Table 2: Baseline system BLEU scores (%) on dev and test sets. number of moves. The score combination weights are trained by a minimum error rate training procedure similar to (Och and Ney, 2003). The trigram language model uses modified Kneser-Ney smoothing and interpolation of trigram and bigram estimates and was trained on the English side of the bitext. In the first pass, 2000 hypotheses are generated per sentence. In the second pass, the seven scores described above are combined with 4-gram language model scores. The performance of the baselinesystem on the development and test sets is shown in Table 2. The BLEU scores obtained are state-of-the-art for this task. 7 Experiments and Results We first investigated to what extent the OOV rate on the development data could be reduced by our backoff procedure. Table 3 shows the percentage of words that are still untranslatable after backoff. A comparison with Table 1 shows that the backoff model reduces the OOV rate, with a larger reduction effect observed when the training set is smaller. We next performed translation with backoff systems trained on each data partition. In each case, the combination weights for the indiGerman-English dev set test set train1 5.2/27.7 5.1/27.3 train2 2.0/11.7 2.0/11.6 train3 1.4/8.1 1.3/7.6 train4 0.5/3.1 0.5/2.9 train5 0.3/1.7 0.2/1.3 Finnish-English dev set test set train1 9.1/28.5 9.2/28.9 train2 3.8/12.4 3.7/12.3 train3 2.5/8.2 2.4/8.0 train4 0.9/3.2 0.9/3.0 train5 0.4/1.4 0.4/1.5 Table 3: OOV rates (%) on the development and test sets under the backoff model (word types/tokens). vidual model scores were re-optimized. Table 4 shows the evaluation results on the dev set. Since the BLEU score alone is often not a good indicator of successful translations of unknown words (the unigram or bigram precision may be increased but may not have a strong effect on the overall BLEU score), position-independent word error rate (PER) rate was measured as well. We see improvements in BLEU score and PERs in almost all cases. Statistical significance was measured on PER using a difference of proportions significance test and on BLEU using a segment-level paired t-test. PER improvements are significant almost all training conditions for both languages; BLEU improvements are significant in all conditions for Finnish and for the two smallest training sets for German. The effect on the overall development set (consisting of both sentences with known words only and sentences with unknown words) is shown in Table 5. As expected, the impact on overall performance is smaller, especially for larger training data sets, due to the relatively small percentage of OOV tokens (see Table 1). The evaluation results for the test set are shown in Tables 6 (for the subset of sentences with OOVs) and 7 (for the entire test set), with similar conclusions. The examples A and B in Figure 2 demonstrate higher-scoring translations produced by the backoff system as opposed to the baseline system. An analysis of the backoff system output showed that in some cases (e.g. examples C and 45 German-English baseline backoff Set BLEU PER BLEU PER train1 14.2 56.9 15.4 55.5 train2 16.3 55.2 17.3 51.8 train3 17.8 51.1 18.4 49.7 train4 19.6 51.1 19.9 47.6 train5 21.9 46.6 22.6 46.0 Finnish-English baseline backoff Set BLEU PER BLEU PER Set BLEU PER BLEU PER train1 12.4 59.9 13.6 57.8 train2 13.0 61.2 13.9 59.1 train3 14.0 58.0 14.7 57.8 train4 17.4 52.7 18.4 50.8 train5 16.8 52.7 18.7 50.2 Table 4: BLEU (%) and position-independent word error rate (PER) on the subset of the development data containing unknown words (secondpass output). Here and in the following tables, statistically significant differences to the baseline model areshown in boldface (p < 0.05). German-English baseline backoff Set BLEU PER BLEU PER train1 15.3 56.4 16.3 55.1 train2 19.0 53.0 19.5 51.6 train3 20.0 49.9 20.5 49.3 train4 22.2 49.0 22.4 48.1 train5 24.6 46.5 24.7 45.6 Finnish-English baseline backoff Set BLEU PER BLEU PER train1 13.1 59.3 14.4 57.4 train2 14.5 59.7 15.4 58.3 train3 16.0 56.5 16.5 56.5 train4 21.0 50.0 21.4 49.2 train5 22.2 50.5 22.5 49.7 Table 5: BLEU (%) and position-independent word error rate (PER) for the entire development set. German-English baseline backoff Set BLEU PER BLEU PER train1 14.3 56.2 15.5 55.1 train2 17.1 54.3 17.6 50.7 train3 17.4 50.8 18.1 49.7 train4 18.9 49.8 18.8 48.2 train5 19.1 46.3 19.4 46.2 Finnish-English baseline backoff Set BLEU PER BLEU PER train1 12.4 59.5 13.5 57.5 train2 13.3 60.7 14.2 59.0 train3 14.1 58.2 15.1 57.3 train4 17.2 54.0 18.4 50.2 train5 16.6 51.8 19.0 49.4 Table 6: BLEU (%) and position-independent word error rate (PER) for the test set (subset with OOV words). D in Figure 2), the backoff model produced a good translation, but the translation was a paraphrase rather than an identical match to the reference translation. Since only a single reference translation is available for the Europarl data (preventing the computation of a BLEU score based on multiple hand-annotated references), good but non-matching translations are not taken into account by our evaluation method. In other cases the unknown word was translated correctly, but since it was translated as single-word phrase the segmentation of the entire sentence was affected. This may cause greater distortion effects since the sentence is segmented into a larger number of smaller phrases, each of which can be reordered. We therefore added the possibility of translating an unknown word in its phrasal context by stemming up to m words to the left and right in the original sentence and finding translations for the entire stemmed phrase (i.e. the function stem() is now applied to the entire phrase). This step is inserted before the stemming of a single word f in the backoff model described above. However, since translations for entire stemmed phrases were found only in about 1% of all cases, there was no significant effect on the BLEU score. Another possibility of limiting reordering effects resulting from single-word translations of OOVs is to restrict the distortion limit of the decoder. Our 46 German-English baseline backoff Set BLEU PER BLEU PER train1 15.3 55.8 16.3 54.8 train2 19.4 52.3 19.6 50.9 train3 20.3 49.6 20.7 49.2 train4 22.5 48.1 22.5 47.9 train5 24.8 46.3 25.1 45.5 Finnish-English baseline backoff Set BLEU PER BLEU PER train1 12.9 58.7 14.0 57.0 train2 14.5 59.5 15.3 58.4 train3 15.6 56.6 16.4 56.2 train4 20.6 50.3 21.0 49.6 train5 22.0 50.0 22.3 49.5 Table 7: BLEU (%) and position-independent word error rate (PER) for the test set (entire test set). experiments showed that this improves the BLEU score slightly for both the baseline and the backoff system; the relative difference, however, remained the same.8 Conclusions We have presented a backoff model for phrasebased SMT that uses morphological abstractions to translate unseen word forms in the foreign language input. When a match for an unknown word in the test set cannot be found in the trained phrase table, the model relies instead on translation probabilities derived from stemmed or split versions of the word in its phrasal context. An evaluation of the model on German-English and FinnishEnglish translations of parliamentary proceedings showed statistically significant improvements in PER for almost all training conditions and significant improvements in BLEU when the training set is small (100K words), with larger improvements for Finnish than for German. This demonstrates that our method is mainly relevant for highly inflected languages and sparse training data conditions. It is also designed to improve human acceptance of machine translation output, which is particularly adversely affected by untranslated words. Acknowledgments This work was funded by NSF grant no. IIS0308297. We thank Ilona Pitka?nen for help with Example A: (German-English): SRC: wir sind berzeugt davon, dass ein europa des friedens nicht durch milita?rbu?ndnisse geschaffen wird. BASE: we are convinced that a europe of peace, not by milita?rbu?ndnisse is created. BACKOFF: we are convinced that a europe of peace, not by military alliance is created. REF: we are convinced that a europe of peace will not be created through military alliances. Example B. (Finnish-English): SRC: arvoisa puhemies, puhuimme ta?a?lla? eilisiltana serviasta ja siella? tapahtuvista vallankumouksellisista muutoksista. BASE: mr president, we talked about here last night, on the subject of serbia and there, of vallankumouksellisista changes. BACKOFF: mr president, we talked about here last night, on the subject of serbia and there, of revolutionary changes. REF: mr. president, last night we discussed the topic of serbia and the revolutionary changes that are taking place there. Example C. (Finnish-English): SRC: toivon ta?lta? osin, etta? yhdistyneiden kansakuntien alaisuudessa ka?yta?vissa? neuvotteluissa pa?a?sta?isiin sellaiseen lopputulokseen, etta? kyproksen kreikkalainen ja turkkilainen va?esto?nosa voisivat yhdessa? nauttia liittymisen mukanaan tuomista eduista yhdistetyssa? tasavallassa. BASE: i hope that the united nations in the negotiations to reach a conclusion that the greek and turkish accession to the benefi t of the benefi ts of the republic of ydistetyssa? brings together va?esto?nosa could, in this respect, under the auspices. BACKOFF: i hope that the united nations in the negotiations to reach a conclusion that the greek and turkish communities can work together to bring the benefi ts of the accession of the republic of ydistetyssa?. in this respect, under the REF: in this connection, i would hope that the talks conducted under the auspices of the united nations will be able to come to a successful conclusion enabling the greek and turkish cypriot populations to enjoy the advantages of membership of the european union in the context of a reunifi ed republic. Example D. (German-English): SRC:so sind wir beim durcharbeiten des textes verfahren, wobei wir bei einer reihe von punkten versucht haben, noch einige straffungen vorzunehmen. BASE: we are in the durcharbeiten procedures of the text, although we have tried to make a few straffungen to carry outon a number of issues. BACKOFF: we are in the durcharbeiten procedures, and we have tried to make a few streamlining of the text in a number of points. REF: this is how we came to go through the text, and attempted to cut down on certain items in the process. Figure 2: Translation examples (SRC = source, BASE = baseline system, BACKOFF = backoff system, REF = reference). OOVs and their translation are marked in boldface. 47 the Finnish language.'")])
INFO:tensorflow:Finished getting dataset.
I0712 13:29:29.326468 140217736775488 input_pipeline.py:91] Finished getting dataset.
I0712 13:29:29.326651 140217736775488 input_pipeline.py:94] Using char-level/byte dataset..
I0712 13:29:29.435143 140217736775488 train.py:106] Vocab Size: 257
I0712 13:29:30.950219 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:31.157124 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:31.360680 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:31.563800 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:31.795046 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:31.935929 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:32.075603 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:32.215148 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:47.761669 140217736775488 checkpoints.py:242] Found no checkpoint directory at trained_models/matching/synthesizer
I0712 13:29:50.397312 140217736775488 train_utils.py:370] Starting training
I0712 13:29:50.397524 140217736775488 train_utils.py:371] ====================
I0712 13:29:58.809694 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:59.117619 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:59.371156 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:59.626296 140217736775488 synthesizer_attention.py:273] random
I0712 13:29:59.949559 140217736775488 synthesizer_attention.py:273] random
I0712 13:30:00.410761 140217736775488 synthesizer_attention.py:273] random
I0712 13:30:00.663650 140217736775488 synthesizer_attention.py:273] random
I0712 13:30:00.917440 140217736775488 synthesizer_attention.py:273] random
I0712 13:30:18.037774 140217736775488 train_utils.py:377] train in step: 0
I0712 13:30:18.063692 140217736775488 train_utils.py:377] train in step: 1
I0712 13:30:18.409682 140217736775488 train_utils.py:377] train in step: 2
I0712 13:30:19.007282 140217736775488 train_utils.py:377] train in step: 3
I0712 13:30:19.608148 140217736775488 train_utils.py:377] train in step: 4
I0712 13:30:20.200102 140217736775488 train_utils.py:377] train in step: 5
I0712 13:30:20.797420 140217736775488 train_utils.py:377] train in step: 6
I0712 13:30:21.391985 140217736775488 train_utils.py:377] train in step: 7
I0712 13:30:21.985390 140217736775488 train_utils.py:377] train in step: 8
I0712 13:30:22.577234 140217736775488 train_utils.py:377] train in step: 9
I0712 13:30:23.175771 140217736775488 train_utils.py:377] train in step: 10
I0712 13:30:23.771661 140217736775488 train_utils.py:377] train in step: 11
I0712 13:30:24.369353 140217736775488 train_utils.py:377] train in step: 12
I0712 13:30:24.962946 140217736775488 train_utils.py:377] train in step: 13
I0712 13:30:25.560909 140217736775488 train_utils.py:377] train in step: 14
I0712 13:30:26.151012 140217736775488 train_utils.py:377] train in step: 15
I0712 13:30:26.748043 140217736775488 train_utils.py:377] train in step: 16
I0712 13:30:27.344791 140217736775488 train_utils.py:377] train in step: 17
I0712 13:30:27.942512 140217736775488 train_utils.py:377] train in step: 18
I0712 13:30:28.530319 140217736775488 train_utils.py:377] train in step: 19
I0712 13:30:29.126224 140217736775488 train_utils.py:377] train in step: 20
I0712 13:30:29.730422 140217736775488 train_utils.py:377] train in step: 21
I0712 13:30:30.323107 140217736775488 train_utils.py:377] train in step: 22
I0712 13:30:30.913599 140217736775488 train_utils.py:377] train in step: 23
I0712 13:30:31.510249 140217736775488 train_utils.py:377] train in step: 24
I0712 13:30:32.108667 140217736775488 train_utils.py:377] train in step: 25
I0712 13:30:32.708421 140217736775488 train_utils.py:377] train in step: 26
I0712 13:30:33.299911 140217736775488 train_utils.py:377] train in step: 27
I0712 13:30:33.895823 140217736775488 train_utils.py:377] train in step: 28
I0712 13:30:34.481950 140217736775488 train_utils.py:377] train in step: 29
I0712 13:30:35.075274 140217736775488 train_utils.py:377] train in step: 30
I0712 13:30:35.673762 140217736775488 train_utils.py:377] train in step: 31
I0712 13:30:36.276487 140217736775488 train_utils.py:377] train in step: 32
I0712 13:30:36.873753 140217736775488 train_utils.py:377] train in step: 33
I0712 13:30:37.472943 140217736775488 train_utils.py:377] train in step: 34
I0712 13:30:38.073450 140217736775488 train_utils.py:377] train in step: 35
I0712 13:30:38.662330 140217736775488 train_utils.py:377] train in step: 36
I0712 13:30:39.254008 140217736775488 train_utils.py:377] train in step: 37
I0712 13:30:39.858547 140217736775488 train_utils.py:377] train in step: 38
I0712 13:30:40.449926 140217736775488 train_utils.py:377] train in step: 39
I0712 13:30:41.041823 140217736775488 train_utils.py:377] train in step: 40
I0712 13:30:41.640474 140217736775488 train_utils.py:377] train in step: 41
I0712 13:30:42.238150 140217736775488 train_utils.py:377] train in step: 42
I0712 13:30:42.834857 140217736775488 train_utils.py:377] train in step: 43
I0712 13:30:43.427452 140217736775488 train_utils.py:377] train in step: 44
I0712 13:30:44.018579 140217736775488 train_utils.py:377] train in step: 45
I0712 13:30:44.615845 140217736775488 train_utils.py:377] train in step: 46
I0712 13:30:45.215347 140217736775488 train_utils.py:377] train in step: 47
I0712 13:30:45.807848 140217736775488 train_utils.py:377] train in step: 48
I0712 13:30:46.400708 140217736775488 train_utils.py:377] train in step: 49
I0712 13:30:47.000530 140217736775488 train_utils.py:377] train in step: 50
I0712 13:30:47.591450 140217736775488 train_utils.py:377] train in step: 51
I0712 13:30:48.189834 140217736775488 train_utils.py:377] train in step: 52
I0712 13:30:48.782778 140217736775488 train_utils.py:377] train in step: 53
I0712 13:30:49.377002 140217736775488 train_utils.py:377] train in step: 54
I0712 13:30:49.993865 140217736775488 train_utils.py:377] train in step: 55
I0712 13:30:50.593133 140217736775488 train_utils.py:377] train in step: 56
I0712 13:30:51.188040 140217736775488 train_utils.py:377] train in step: 57
I0712 13:30:51.790913 140217736775488 train_utils.py:377] train in step: 58
I0712 13:30:52.384590 140217736775488 train_utils.py:377] train in step: 59
I0712 13:30:52.984921 140217736775488 train_utils.py:377] train in step: 60
I0712 13:30:53.573857 140217736775488 train_utils.py:377] train in step: 61
I0712 13:30:54.167614 140217736775488 train_utils.py:377] train in step: 62
I0712 13:30:54.760885 140217736775488 train_utils.py:377] train in step: 63
I0712 13:30:55.357601 140217736775488 train_utils.py:377] train in step: 64
I0712 13:30:55.946876 140217736775488 train_utils.py:377] train in step: 65
I0712 13:30:56.554010 140217736775488 train_utils.py:377] train in step: 66
I0712 13:30:57.142332 140217736775488 train_utils.py:377] train in step: 67
I0712 13:30:57.739886 140217736775488 train_utils.py:377] train in step: 68
I0712 13:30:58.331551 140217736775488 train_utils.py:377] train in step: 69
I0712 13:30:58.930365 140217736775488 train_utils.py:377] train in step: 70
I0712 13:30:59.525650 140217736775488 train_utils.py:377] train in step: 71
I0712 13:31:00.121564 140217736775488 train_utils.py:377] train in step: 72
I0712 13:31:00.709567 140217736775488 train_utils.py:377] train in step: 73
I0712 13:31:01.306521 140217736775488 train_utils.py:377] train in step: 74
I0712 13:31:01.899498 140217736775488 train_utils.py:377] train in step: 75
I0712 13:31:02.507179 140217736775488 train_utils.py:377] train in step: 76
I0712 13:31:03.097891 140217736775488 train_utils.py:377] train in step: 77
I0712 13:31:03.693632 140217736775488 train_utils.py:377] train in step: 78
I0712 13:31:04.290695 140217736775488 train_utils.py:377] train in step: 79
I0712 13:31:04.898010 140217736775488 train_utils.py:377] train in step: 80
I0712 13:31:05.502502 140217736775488 train_utils.py:377] train in step: 81
I0712 13:31:06.102203 140217736775488 train_utils.py:377] train in step: 82
I0712 13:31:06.699437 140217736775488 train_utils.py:377] train in step: 83
I0712 13:31:07.300972 140217736775488 train_utils.py:377] train in step: 84
I0712 13:31:07.894700 140217736775488 train_utils.py:377] train in step: 85
I0712 13:31:08.495556 140217736775488 train_utils.py:377] train in step: 86
I0712 13:31:09.085227 140217736775488 train_utils.py:377] train in step: 87
I0712 13:31:09.682071 140217736775488 train_utils.py:377] train in step: 88
I0712 13:31:10.272713 140217736775488 train_utils.py:377] train in step: 89
I0712 13:31:10.869694 140217736775488 train_utils.py:377] train in step: 90
I0712 13:31:11.459376 140217736775488 train_utils.py:377] train in step: 91
I0712 13:31:12.060981 140217736775488 train_utils.py:377] train in step: 92
I0712 13:31:12.659584 140217736775488 train_utils.py:377] train in step: 93
I0712 13:31:13.265241 140217736775488 train_utils.py:377] train in step: 94
I0712 13:31:13.862650 140217736775488 train_utils.py:377] train in step: 95
I0712 13:31:14.457024 140217736775488 train_utils.py:377] train in step: 96
I0712 13:31:15.049849 140217736775488 train_utils.py:377] train in step: 97
I0712 13:31:15.643576 140217736775488 train_utils.py:377] train in step: 98
I0712 13:31:16.237732 140217736775488 train_utils.py:377] train in step: 99
I0712 13:31:16.836932 140217736775488 train_utils.py:377] train in step: 100
I0712 13:31:17.430623 140217736775488 train_utils.py:377] train in step: 101
I0712 13:31:18.024960 140217736775488 train_utils.py:377] train in step: 102
I0712 13:31:18.612338 140217736775488 train_utils.py:377] train in step: 103
I0712 13:31:19.211917 140217736775488 train_utils.py:377] train in step: 104
I0712 13:31:19.805612 140217736775488 train_utils.py:377] train in step: 105
I0712 13:31:20.398922 140217736775488 train_utils.py:377] train in step: 106
I0712 13:31:20.983711 140217736775488 train_utils.py:377] train in step: 107
I0712 13:31:21.583830 140217736775488 train_utils.py:377] train in step: 108
I0712 13:31:22.173921 140217736775488 train_utils.py:377] train in step: 109
I0712 13:31:22.784779 140217736775488 train_utils.py:377] train in step: 110
I0712 13:31:23.374534 140217736775488 train_utils.py:377] train in step: 111
I0712 13:31:23.964397 140217736775488 train_utils.py:377] train in step: 112
I0712 13:31:24.556684 140217736775488 train_utils.py:377] train in step: 113
I0712 13:31:25.155271 140217736775488 train_utils.py:377] train in step: 114
I0712 13:31:25.759965 140217736775488 train_utils.py:377] train in step: 115
I0712 13:31:26.355413 140217736775488 train_utils.py:377] train in step: 116
I0712 13:31:26.945669 140217736775488 train_utils.py:377] train in step: 117
I0712 13:31:27.539899 140217736775488 train_utils.py:377] train in step: 118
I0712 13:31:28.135447 140217736775488 train_utils.py:377] train in step: 119
I0712 13:31:28.729099 140217736775488 train_utils.py:377] train in step: 120
I0712 13:31:29.333203 140217736775488 train_utils.py:377] train in step: 121
I0712 13:31:29.928971 140217736775488 train_utils.py:377] train in step: 122
I0712 13:31:30.529663 140217736775488 train_utils.py:377] train in step: 123
I0712 13:31:31.113941 140217736775488 train_utils.py:377] train in step: 124
I0712 13:31:31.705191 140217736775488 train_utils.py:377] train in step: 125
I0712 13:31:32.312605 140217736775488 train_utils.py:377] train in step: 126
I0712 13:31:32.920305 140217736775488 train_utils.py:377] train in step: 127
I0712 13:31:33.513410 140217736775488 train_utils.py:377] train in step: 128
I0712 13:31:34.101626 140217736775488 train_utils.py:377] train in step: 129
I0712 13:31:34.704634 140217736775488 train_utils.py:377] train in step: 130
I0712 13:31:35.302068 140217736775488 train_utils.py:377] train in step: 131
I0712 13:31:35.904778 140217736775488 train_utils.py:377] train in step: 132
I0712 13:31:36.501531 140217736775488 train_utils.py:377] train in step: 133
I0712 13:31:37.089412 140217736775488 train_utils.py:377] train in step: 134
I0712 13:31:37.688180 140217736775488 train_utils.py:377] train in step: 135
I0712 13:31:38.284790 140217736775488 train_utils.py:377] train in step: 136
I0712 13:31:38.888590 140217736775488 train_utils.py:377] train in step: 137
I0712 13:31:39.486471 140217736775488 train_utils.py:377] train in step: 138
I0712 13:31:40.084228 140217736775488 train_utils.py:377] train in step: 139
I0712 13:31:40.680209 140217736775488 train_utils.py:377] train in step: 140
I0712 13:31:41.269232 140217736775488 train_utils.py:377] train in step: 141
I0712 13:31:41.871105 140217736775488 train_utils.py:377] train in step: 142
I0712 13:31:42.471554 140217736775488 train_utils.py:377] train in step: 143
I0712 13:31:43.069914 140217736775488 train_utils.py:377] train in step: 144
I0712 13:31:43.668481 140217736775488 train_utils.py:377] train in step: 145
I0712 13:31:44.265052 140217736775488 train_utils.py:377] train in step: 146
I0712 13:31:44.859179 140217736775488 train_utils.py:377] train in step: 147
I0712 13:31:45.463504 140217736775488 train_utils.py:377] train in step: 148
I0712 13:31:46.066093 140217736775488 train_utils.py:377] train in step: 149
I0712 13:31:46.660217 140217736775488 train_utils.py:377] train in step: 150
I0712 13:31:47.257009 140217736775488 train_utils.py:377] train in step: 151
I0712 13:31:47.844912 140217736775488 train_utils.py:377] train in step: 152
I0712 13:31:48.441543 140217736775488 train_utils.py:377] train in step: 153
I0712 13:31:49.036985 140217736775488 train_utils.py:377] train in step: 154
I0712 13:31:49.622953 140217736775488 train_utils.py:377] train in step: 155
I0712 13:31:50.220870 140217736775488 train_utils.py:377] train in step: 156
I0712 13:31:50.819648 140217736775488 train_utils.py:377] train in step: 157
I0712 13:31:51.420296 140217736775488 train_utils.py:377] train in step: 158
I0712 13:31:52.016428 140217736775488 train_utils.py:377] train in step: 159
I0712 13:31:52.611128 140217736775488 train_utils.py:377] train in step: 160
I0712 13:31:53.210154 140217736775488 train_utils.py:377] train in step: 161
I0712 13:31:53.806406 140217736775488 train_utils.py:377] train in step: 162
I0712 13:31:54.403539 140217736775488 train_utils.py:377] train in step: 163
I0712 13:31:54.994390 140217736775488 train_utils.py:377] train in step: 164
I0712 13:31:55.583695 140217736775488 train_utils.py:377] train in step: 165
I0712 13:31:56.183285 140217736775488 train_utils.py:377] train in step: 166
I0712 13:31:56.776743 140217736775488 train_utils.py:377] train in step: 167
I0712 13:31:57.365869 140217736775488 train_utils.py:377] train in step: 168
I0712 13:31:57.953959 140217736775488 train_utils.py:377] train in step: 169
I0712 13:31:58.564303 140217736775488 train_utils.py:377] train in step: 170
I0712 13:31:59.154598 140217736775488 train_utils.py:377] train in step: 171
I0712 13:31:59.755820 140217736775488 train_utils.py:377] train in step: 172
I0712 13:32:00.347124 140217736775488 train_utils.py:377] train in step: 173
I0712 13:32:00.940868 140217736775488 train_utils.py:377] train in step: 174
I0712 13:32:01.530480 140217736775488 train_utils.py:377] train in step: 175
I0712 13:32:02.130700 140217736775488 train_utils.py:377] train in step: 176
I0712 13:32:02.722215 140217736775488 train_utils.py:377] train in step: 177
I0712 13:32:03.330607 140217736775488 train_utils.py:377] train in step: 178
I0712 13:32:03.919797 140217736775488 train_utils.py:377] train in step: 179
I0712 13:32:04.511877 140217736775488 train_utils.py:377] train in step: 180
I0712 13:32:05.099786 140217736775488 train_utils.py:377] train in step: 181
I0712 13:32:05.692666 140217736775488 train_utils.py:377] train in step: 182
I0712 13:32:06.288737 140217736775488 train_utils.py:377] train in step: 183
I0712 13:32:06.892689 140217736775488 train_utils.py:377] train in step: 184
I0712 13:32:07.486594 140217736775488 train_utils.py:377] train in step: 185
I0712 13:32:08.084610 140217736775488 train_utils.py:377] train in step: 186
I0712 13:32:08.675937 140217736775488 train_utils.py:377] train in step: 187
I0712 13:32:09.276181 140217736775488 train_utils.py:377] train in step: 188
I0712 13:32:09.870450 140217736775488 train_utils.py:377] train in step: 189
I0712 13:32:10.470708 140217736775488 train_utils.py:377] train in step: 190
I0712 13:32:11.067152 140217736775488 train_utils.py:377] train in step: 191
I0712 13:32:11.658933 140217736775488 train_utils.py:377] train in step: 192
I0712 13:32:12.255923 140217736775488 train_utils.py:377] train in step: 193
I0712 13:32:12.862431 140217736775488 train_utils.py:377] train in step: 194
I0712 13:32:13.470268 140217736775488 train_utils.py:377] train in step: 195
I0712 13:32:14.063585 140217736775488 train_utils.py:377] train in step: 196
I0712 13:32:14.652062 140217736775488 train_utils.py:377] train in step: 197
I0712 13:32:15.246315 140217736775488 train_utils.py:377] train in step: 198
I0712 13:32:15.845548 140217736775488 train_utils.py:377] train in step: 199
I0712 13:32:16.449721 140217736775488 train_utils.py:377] train in step: 200
I0712 13:32:18.137689 140217736775488 train_utils.py:396] train in step: 200, loss: 0.736299991607666, acc: 0.5310999751091003
I0712 13:32:20.573582 140217736775488 synthesizer_attention.py:273] random
I0712 13:32:20.722832 140217736775488 synthesizer_attention.py:273] random
I0712 13:32:20.864418 140217736775488 synthesizer_attention.py:273] random
I0712 13:32:21.004694 140217736775488 synthesizer_attention.py:273] random
I0712 13:32:21.185634 140217736775488 synthesizer_attention.py:273] random
I0712 13:32:21.328325 140217736775488 synthesizer_attention.py:273] random
I0712 13:32:21.472483 140217736775488 synthesizer_attention.py:273] random
I0712 13:32:21.620113 140217736775488 synthesizer_attention.py:273] random
I0712 13:32:28.578990 140217736775488 train_utils.py:411] eval in step: 200, loss: 0.7339, acc: 0.5000
I0712 13:32:28.581888 140217736775488 train_utils.py:421] Testing...
I0712 13:32:34.546367 140217736775488 train_utils.py:424] test in step: 200, loss: 0.7566, acc: 0.4800
I0712 13:32:34.574864 140217736775488 train_utils.py:377] train in step: 201
I0712 13:32:34.598189 140217736775488 train_utils.py:377] train in step: 202
I0712 13:32:35.162340 140217736775488 train_utils.py:377] train in step: 203
I0712 13:32:35.756375 140217736775488 train_utils.py:377] train in step: 204
I0712 13:32:36.352573 140217736775488 train_utils.py:377] train in step: 205
I0712 13:32:36.945194 140217736775488 train_utils.py:377] train in step: 206
I0712 13:32:37.544541 140217736775488 train_utils.py:377] train in step: 207
I0712 13:32:38.135938 140217736775488 train_utils.py:377] train in step: 208
I0712 13:32:38.726089 140217736775488 train_utils.py:377] train in step: 209
I0712 13:32:39.324449 140217736775488 train_utils.py:377] train in step: 210
I0712 13:32:39.919084 140217736775488 train_utils.py:377] train in step: 211
I0712 13:32:40.513925 140217736775488 train_utils.py:377] train in step: 212
I0712 13:32:41.101918 140217736775488 train_utils.py:377] train in step: 213
I0712 13:32:41.693046 140217736775488 train_utils.py:377] train in step: 214
I0712 13:32:42.291152 140217736775488 train_utils.py:377] train in step: 215
I0712 13:32:42.891103 140217736775488 train_utils.py:377] train in step: 216
I0712 13:32:43.482523 140217736775488 train_utils.py:377] train in step: 217
I0712 13:32:44.081238 140217736775488 train_utils.py:377] train in step: 218
I0712 13:32:44.673601 140217736775488 train_utils.py:377] train in step: 219
I0712 13:32:45.273689 140217736775488 train_utils.py:377] train in step: 220
I0712 13:32:45.872606 140217736775488 train_utils.py:377] train in step: 221
I0712 13:32:46.476963 140217736775488 train_utils.py:377] train in step: 222
I0712 13:32:47.074659 140217736775488 train_utils.py:377] train in step: 223
I0712 13:32:47.663175 140217736775488 train_utils.py:377] train in step: 224
I0712 13:32:48.259632 140217736775488 train_utils.py:377] train in step: 225
I0712 13:32:48.856186 140217736775488 train_utils.py:377] train in step: 226
I0712 13:32:49.465682 140217736775488 train_utils.py:377] train in step: 227
I0712 13:32:50.060943 140217736775488 train_utils.py:377] train in step: 228
I0712 13:32:50.655861 140217736775488 train_utils.py:377] train in step: 229
I0712 13:32:51.255740 140217736775488 train_utils.py:377] train in step: 230
I0712 13:32:51.856304 140217736775488 train_utils.py:377] train in step: 231
I0712 13:32:52.456310 140217736775488 train_utils.py:377] train in step: 232
I0712 13:32:53.048934 140217736775488 train_utils.py:377] train in step: 233
I0712 13:32:53.642038 140217736775488 train_utils.py:377] train in step: 234
I0712 13:32:54.239575 140217736775488 train_utils.py:377] train in step: 235
I0712 13:32:54.839909 140217736775488 train_utils.py:377] train in step: 236
I0712 13:32:55.448030 140217736775488 train_utils.py:377] train in step: 237
I0712 13:32:56.039165 140217736775488 train_utils.py:377] train in step: 238
I0712 13:32:56.634779 140217736775488 train_utils.py:377] train in step: 239
I0712 13:32:57.232734 140217736775488 train_utils.py:377] train in step: 240
I0712 13:32:57.828155 140217736775488 train_utils.py:377] train in step: 241
I0712 13:32:58.429930 140217736775488 train_utils.py:377] train in step: 242
I0712 13:32:59.020379 140217736775488 train_utils.py:377] train in step: 243
I0712 13:32:59.621647 140217736775488 train_utils.py:377] train in step: 244
I0712 13:33:00.208798 140217736775488 train_utils.py:377] train in step: 245
I0712 13:33:00.805717 140217736775488 train_utils.py:377] train in step: 246
I0712 13:33:01.406172 140217736775488 train_utils.py:377] train in step: 247
I0712 13:33:01.993468 140217736775488 train_utils.py:377] train in step: 248
I0712 13:33:02.587835 140217736775488 train_utils.py:377] train in step: 249
I0712 13:33:03.179948 140217736775488 train_utils.py:377] train in step: 250
I0712 13:33:03.769449 140217736775488 train_utils.py:377] train in step: 251
I0712 13:33:04.363926 140217736775488 train_utils.py:377] train in step: 252
I0712 13:33:04.952755 140217736775488 train_utils.py:377] train in step: 253
I0712 13:33:05.547277 140217736775488 train_utils.py:377] train in step: 254
I0712 13:33:06.140079 140217736775488 train_utils.py:377] train in step: 255
I0712 13:33:06.727757 140217736775488 train_utils.py:377] train in step: 256
I0712 13:33:07.318963 140217736775488 train_utils.py:377] train in step: 257
I0712 13:33:07.909459 140217736775488 train_utils.py:377] train in step: 258
I0712 13:33:08.508326 140217736775488 train_utils.py:377] train in step: 259
I0712 13:33:09.099133 140217736775488 train_utils.py:377] train in step: 260
I0712 13:33:09.690679 140217736775488 train_utils.py:377] train in step: 261
I0712 13:33:10.278924 140217736775488 train_utils.py:377] train in step: 262
I0712 13:33:10.872231 140217736775488 train_utils.py:377] train in step: 263
I0712 13:33:11.469159 140217736775488 train_utils.py:377] train in step: 264
I0712 13:33:12.061664 140217736775488 train_utils.py:377] train in step: 265
I0712 13:33:12.653206 140217736775488 train_utils.py:377] train in step: 266
I0712 13:33:13.241236 140217736775488 train_utils.py:377] train in step: 267
I0712 13:33:13.842322 140217736775488 train_utils.py:377] train in step: 268
I0712 13:33:14.428308 140217736775488 train_utils.py:377] train in step: 269
I0712 13:33:15.018826 140217736775488 train_utils.py:377] train in step: 270
I0712 13:33:15.612694 140217736775488 train_utils.py:377] train in step: 271
I0712 13:33:16.200505 140217736775488 train_utils.py:377] train in step: 272
I0712 13:33:16.796278 140217736775488 train_utils.py:377] train in step: 273
I0712 13:33:17.387234 140217736775488 train_utils.py:377] train in step: 274
I0712 13:33:17.982735 140217736775488 train_utils.py:377] train in step: 275
I0712 13:33:18.581167 140217736775488 train_utils.py:377] train in step: 276
I0712 13:33:19.183055 140217736775488 train_utils.py:377] train in step: 277
I0712 13:33:19.775882 140217736775488 train_utils.py:377] train in step: 278
I0712 13:33:20.366382 140217736775488 train_utils.py:377] train in step: 279
I0712 13:33:20.954724 140217736775488 train_utils.py:377] train in step: 280
I0712 13:33:21.549915 140217736775488 train_utils.py:377] train in step: 281
I0712 13:33:22.158236 140217736775488 train_utils.py:377] train in step: 282
I0712 13:33:22.745238 140217736775488 train_utils.py:377] train in step: 283
I0712 13:33:23.348142 140217736775488 train_utils.py:377] train in step: 284
I0712 13:33:23.941274 140217736775488 train_utils.py:377] train in step: 285
I0712 13:33:24.536583 140217736775488 train_utils.py:377] train in step: 286
I0712 13:33:25.130731 140217736775488 train_utils.py:377] train in step: 287
I0712 13:33:25.728519 140217736775488 train_utils.py:377] train in step: 288
I0712 13:33:26.320331 140217736775488 train_utils.py:377] train in step: 289
I0712 13:33:26.918515 140217736775488 train_utils.py:377] train in step: 290
I0712 13:33:27.523196 140217736775488 train_utils.py:377] train in step: 291
I0712 13:33:28.114110 140217736775488 train_utils.py:377] train in step: 292
I0712 13:33:28.715371 140217736775488 train_utils.py:377] train in step: 293
I0712 13:33:29.309451 140217736775488 train_utils.py:377] train in step: 294
I0712 13:33:29.905121 140217736775488 train_utils.py:377] train in step: 295
I0712 13:33:30.503507 140217736775488 train_utils.py:377] train in step: 296
I0712 13:33:31.116789 140217736775488 train_utils.py:377] train in step: 297
I0712 13:33:31.723088 140217736775488 train_utils.py:377] train in step: 298
I0712 13:33:32.313254 140217736775488 train_utils.py:377] train in step: 299
I0712 13:33:32.908206 140217736775488 train_utils.py:377] train in step: 300
I0712 13:33:33.503200 140217736775488 train_utils.py:377] train in step: 301
I0712 13:33:34.106788 140217736775488 train_utils.py:377] train in step: 302
I0712 13:33:34.738211 140217736775488 train_utils.py:377] train in step: 303
I0712 13:33:35.328722 140217736775488 train_utils.py:377] train in step: 304
I0712 13:33:35.922425 140217736775488 train_utils.py:377] train in step: 305
I0712 13:33:36.518542 140217736775488 train_utils.py:377] train in step: 306
I0712 13:33:37.117868 140217736775488 train_utils.py:377] train in step: 307
I0712 13:33:37.722714 140217736775488 train_utils.py:377] train in step: 308
I0712 13:33:38.317367 140217736775488 train_utils.py:377] train in step: 309
I0712 13:33:38.913808 140217736775488 train_utils.py:377] train in step: 310
I0712 13:33:39.516922 140217736775488 train_utils.py:377] train in step: 311
I0712 13:33:40.113397 140217736775488 train_utils.py:377] train in step: 312
I0712 13:33:40.712734 140217736775488 train_utils.py:377] train in step: 313
I0712 13:33:41.306445 140217736775488 train_utils.py:377] train in step: 314
I0712 13:33:41.892350 140217736775488 train_utils.py:377] train in step: 315
I0712 13:33:42.489330 140217736775488 train_utils.py:377] train in step: 316
I0712 13:33:43.083306 140217736775488 train_utils.py:377] train in step: 317
I0712 13:33:43.678716 140217736775488 train_utils.py:377] train in step: 318
I0712 13:33:44.270873 140217736775488 train_utils.py:377] train in step: 319
I0712 13:33:44.860168 140217736775488 train_utils.py:377] train in step: 320
I0712 13:33:45.451165 140217736775488 train_utils.py:377] train in step: 321
I0712 13:33:46.047700 140217736775488 train_utils.py:377] train in step: 322
I0712 13:33:46.640509 140217736775488 train_utils.py:377] train in step: 323
I0712 13:33:47.239295 140217736775488 train_utils.py:377] train in step: 324
I0712 13:33:47.827637 140217736775488 train_utils.py:377] train in step: 325
I0712 13:33:48.421249 140217736775488 train_utils.py:377] train in step: 326
I0712 13:33:49.017724 140217736775488 train_utils.py:377] train in step: 327
I0712 13:33:49.615520 140217736775488 train_utils.py:377] train in step: 328
I0712 13:33:50.222256 140217736775488 train_utils.py:377] train in step: 329
I0712 13:33:50.810697 140217736775488 train_utils.py:377] train in step: 330
I0712 13:33:51.403515 140217736775488 train_utils.py:377] train in step: 331
I0712 13:33:51.996869 140217736775488 train_utils.py:377] train in step: 332
I0712 13:33:52.591145 140217736775488 train_utils.py:377] train in step: 333
I0712 13:33:53.188613 140217736775488 train_utils.py:377] train in step: 334
I0712 13:33:53.777770 140217736775488 train_utils.py:377] train in step: 335
I0712 13:33:54.373411 140217736775488 train_utils.py:377] train in step: 336
I0712 13:33:54.965440 140217736775488 train_utils.py:377] train in step: 337
I0712 13:33:55.563244 140217736775488 train_utils.py:377] train in step: 338
I0712 13:33:56.157403 140217736775488 train_utils.py:377] train in step: 339
I0712 13:33:56.751707 140217736775488 train_utils.py:377] train in step: 340
I0712 13:33:57.338807 140217736775488 train_utils.py:377] train in step: 341
I0712 13:33:57.930073 140217736775488 train_utils.py:377] train in step: 342
I0712 13:33:58.524970 140217736775488 train_utils.py:377] train in step: 343
I0712 13:33:59.132151 140217736775488 train_utils.py:377] train in step: 344
I0712 13:33:59.718692 140217736775488 train_utils.py:377] train in step: 345
I0712 13:34:00.311896 140217736775488 train_utils.py:377] train in step: 346
I0712 13:34:00.902858 140217736775488 train_utils.py:377] train in step: 347
I0712 13:34:01.505424 140217736775488 train_utils.py:377] train in step: 348
I0712 13:34:02.101390 140217736775488 train_utils.py:377] train in step: 349
I0712 13:34:02.699857 140217736775488 train_utils.py:377] train in step: 350
I0712 13:34:03.294802 140217736775488 train_utils.py:377] train in step: 351
I0712 13:34:03.890018 140217736775488 train_utils.py:377] train in step: 352
I0712 13:34:04.487132 140217736775488 train_utils.py:377] train in step: 353
I0712 13:34:05.084963 140217736775488 train_utils.py:377] train in step: 354
I0712 13:34:05.679415 140217736775488 train_utils.py:377] train in step: 355
I0712 13:34:06.274323 140217736775488 train_utils.py:377] train in step: 356
I0712 13:34:06.860963 140217736775488 train_utils.py:377] train in step: 357
I0712 13:34:07.458570 140217736775488 train_utils.py:377] train in step: 358
I0712 13:34:08.062364 140217736775488 train_utils.py:377] train in step: 359
I0712 13:34:08.664837 140217736775488 train_utils.py:377] train in step: 360
I0712 13:34:09.256109 140217736775488 train_utils.py:377] train in step: 361
I0712 13:34:09.847533 140217736775488 train_utils.py:377] train in step: 362
I0712 13:34:10.440452 140217736775488 train_utils.py:377] train in step: 363
I0712 13:34:11.034694 140217736775488 train_utils.py:377] train in step: 364
I0712 13:34:11.629211 140217736775488 train_utils.py:377] train in step: 365
I0712 13:34:12.222860 140217736775488 train_utils.py:377] train in step: 366
I0712 13:34:12.813558 140217736775488 train_utils.py:377] train in step: 367
I0712 13:34:13.406225 140217736775488 train_utils.py:377] train in step: 368
I0712 13:34:14.004307 140217736775488 train_utils.py:377] train in step: 369
I0712 13:34:14.605921 140217736775488 train_utils.py:377] train in step: 370
I0712 13:34:15.199589 140217736775488 train_utils.py:377] train in step: 371
I0712 13:34:15.797315 140217736775488 train_utils.py:377] train in step: 372
I0712 13:34:16.397015 140217736775488 train_utils.py:377] train in step: 373
I0712 13:34:16.992185 140217736775488 train_utils.py:377] train in step: 374
I0712 13:34:17.587947 140217736775488 train_utils.py:377] train in step: 375
I0712 13:34:18.174698 140217736775488 train_utils.py:377] train in step: 376
I0712 13:34:18.763061 140217736775488 train_utils.py:377] train in step: 377
I0712 13:34:19.366417 140217736775488 train_utils.py:377] train in step: 378
I0712 13:34:19.955597 140217736775488 train_utils.py:377] train in step: 379
I0712 13:34:20.548759 140217736775488 train_utils.py:377] train in step: 380
I0712 13:34:21.145393 140217736775488 train_utils.py:377] train in step: 381
I0712 13:34:21.747937 140217736775488 train_utils.py:377] train in step: 382
I0712 13:34:22.351197 140217736775488 train_utils.py:377] train in step: 383
I0712 13:34:22.949209 140217736775488 train_utils.py:377] train in step: 384
I0712 13:34:23.539279 140217736775488 train_utils.py:377] train in step: 385
I0712 13:34:24.138479 140217736775488 train_utils.py:377] train in step: 386
I0712 13:34:24.734417 140217736775488 train_utils.py:377] train in step: 387
I0712 13:34:25.335609 140217736775488 train_utils.py:377] train in step: 388
I0712 13:34:25.925098 140217736775488 train_utils.py:377] train in step: 389
I0712 13:34:26.521930 140217736775488 train_utils.py:377] train in step: 390
I0712 13:34:27.116891 140217736775488 train_utils.py:377] train in step: 391
I0712 13:34:27.711612 140217736775488 train_utils.py:377] train in step: 392
I0712 13:34:28.311860 140217736775488 train_utils.py:377] train in step: 393
I0712 13:34:28.910364 140217736775488 train_utils.py:377] train in step: 394
I0712 13:34:29.505169 140217736775488 train_utils.py:377] train in step: 395
I0712 13:34:30.104332 140217736775488 train_utils.py:377] train in step: 396
I0712 13:34:30.699382 140217736775488 train_utils.py:377] train in step: 397
I0712 13:34:31.307934 140217736775488 train_utils.py:377] train in step: 398
I0712 13:34:31.918742 140217736775488 train_utils.py:377] train in step: 399
I0712 13:34:32.505397 140217736775488 train_utils.py:377] train in step: 400
I0712 13:34:33.808308 140217736775488 train_utils.py:396] train in step: 400, loss: 0.7268999814987183, acc: 0.5374999642372131
I0712 13:34:39.571668 140217736775488 train_utils.py:411] eval in step: 400, loss: 0.6919, acc: 0.5300
I0712 13:34:39.574716 140217736775488 train_utils.py:421] Testing...
I0712 13:34:45.465739 140217736775488 train_utils.py:424] test in step: 400, loss: 0.7019, acc: 0.4850
I0712 13:34:45.494190 140217736775488 train_utils.py:377] train in step: 401
I0712 13:34:45.517741 140217736775488 train_utils.py:377] train in step: 402
I0712 13:34:46.082794 140217736775488 train_utils.py:377] train in step: 403
I0712 13:34:46.671682 140217736775488 train_utils.py:377] train in step: 404
I0712 13:34:47.264637 140217736775488 train_utils.py:377] train in step: 405
I0712 13:34:47.853517 140217736775488 train_utils.py:377] train in step: 406
I0712 13:34:48.445082 140217736775488 train_utils.py:377] train in step: 407
I0712 13:34:49.031815 140217736775488 train_utils.py:377] train in step: 408
I0712 13:34:49.621817 140217736775488 train_utils.py:377] train in step: 409
I0712 13:34:50.209421 140217736775488 train_utils.py:377] train in step: 410
I0712 13:34:50.804696 140217736775488 train_utils.py:377] train in step: 411
I0712 13:34:51.405788 140217736775488 train_utils.py:377] train in step: 412
I0712 13:34:51.995912 140217736775488 train_utils.py:377] train in step: 413
I0712 13:34:52.600215 140217736775488 train_utils.py:377] train in step: 414
I0712 13:34:53.190250 140217736775488 train_utils.py:377] train in step: 415
I0712 13:34:53.780307 140217736775488 train_utils.py:377] train in step: 416
I0712 13:34:54.374405 140217736775488 train_utils.py:377] train in step: 417
I0712 13:34:54.963643 140217736775488 train_utils.py:377] train in step: 418
I0712 13:34:55.556421 140217736775488 train_utils.py:377] train in step: 419
I0712 13:34:56.144198 140217736775488 train_utils.py:377] train in step: 420
I0712 13:34:56.735725 140217736775488 train_utils.py:377] train in step: 421
I0712 13:34:57.326370 140217736775488 train_utils.py:377] train in step: 422
I0712 13:34:57.916499 140217736775488 train_utils.py:377] train in step: 423
I0712 13:34:58.506308 140217736775488 train_utils.py:377] train in step: 424
I0712 13:34:59.094089 140217736775488 train_utils.py:377] train in step: 425
I0712 13:34:59.683527 140217736775488 train_utils.py:377] train in step: 426
I0712 13:35:00.279631 140217736775488 train_utils.py:377] train in step: 427
I0712 13:35:00.864418 140217736775488 train_utils.py:377] train in step: 428
I0712 13:35:01.468225 140217736775488 train_utils.py:377] train in step: 429
I0712 13:35:02.056929 140217736775488 train_utils.py:377] train in step: 430
I0712 13:35:02.650667 140217736775488 train_utils.py:377] train in step: 431
I0712 13:35:03.236554 140217736775488 train_utils.py:377] train in step: 432
I0712 13:35:03.826091 140217736775488 train_utils.py:377] train in step: 433
I0712 13:35:04.418835 140217736775488 train_utils.py:377] train in step: 434
I0712 13:35:05.020510 140217736775488 train_utils.py:377] train in step: 435
I0712 13:35:05.616079 140217736775488 train_utils.py:377] train in step: 436
I0712 13:35:06.210833 140217736775488 train_utils.py:377] train in step: 437
I0712 13:35:06.809358 140217736775488 train_utils.py:377] train in step: 438
I0712 13:35:07.407233 140217736775488 train_utils.py:377] train in step: 439
I0712 13:35:08.004430 140217736775488 train_utils.py:377] train in step: 440
I0712 13:35:08.597146 140217736775488 train_utils.py:377] train in step: 441
I0712 13:35:09.188651 140217736775488 train_utils.py:377] train in step: 442
I0712 13:35:09.791847 140217736775488 train_utils.py:377] train in step: 443
I0712 13:35:10.387533 140217736775488 train_utils.py:377] train in step: 444
I0712 13:35:10.980413 140217736775488 train_utils.py:377] train in step: 445
I0712 13:35:11.578298 140217736775488 train_utils.py:377] train in step: 446
I0712 13:35:12.173769 140217736775488 train_utils.py:377] train in step: 447
I0712 13:35:12.774783 140217736775488 train_utils.py:377] train in step: 448
I0712 13:35:13.377676 140217736775488 train_utils.py:377] train in step: 449
I0712 13:35:13.969500 140217736775488 train_utils.py:377] train in step: 450
I0712 13:35:14.567141 140217736775488 train_utils.py:377] train in step: 451
I0712 13:35:15.164177 140217736775488 train_utils.py:377] train in step: 452
I0712 13:35:15.761240 140217736775488 train_utils.py:377] train in step: 453
I0712 13:35:16.356084 140217736775488 train_utils.py:377] train in step: 454
I0712 13:35:16.945717 140217736775488 train_utils.py:377] train in step: 455
I0712 13:35:17.536551 140217736775488 train_utils.py:377] train in step: 456
I0712 13:35:18.137851 140217736775488 train_utils.py:377] train in step: 457
I0712 13:35:18.740806 140217736775488 train_utils.py:377] train in step: 458
I0712 13:35:19.350624 140217736775488 train_utils.py:377] train in step: 459
I0712 13:35:19.946703 140217736775488 train_utils.py:377] train in step: 460
I0712 13:35:20.538028 140217736775488 train_utils.py:377] train in step: 461
I0712 13:35:21.130933 140217736775488 train_utils.py:377] train in step: 462
I0712 13:35:21.726577 140217736775488 train_utils.py:377] train in step: 463
I0712 13:35:22.321776 140217736775488 train_utils.py:377] train in step: 464
I0712 13:35:22.930255 140217736775488 train_utils.py:377] train in step: 465
I0712 13:35:23.523718 140217736775488 train_utils.py:377] train in step: 466
I0712 13:35:24.121958 140217736775488 train_utils.py:377] train in step: 467
I0712 13:35:24.713331 140217736775488 train_utils.py:377] train in step: 468
I0712 13:35:25.310366 140217736775488 train_utils.py:377] train in step: 469
I0712 13:35:25.912289 140217736775488 train_utils.py:377] train in step: 470
I0712 13:35:26.504270 140217736775488 train_utils.py:377] train in step: 471
I0712 13:35:27.095953 140217736775488 train_utils.py:377] train in step: 472
I0712 13:35:27.691933 140217736775488 train_utils.py:377] train in step: 473
I0712 13:35:28.293152 140217736775488 train_utils.py:377] train in step: 474
I0712 13:35:28.890029 140217736775488 train_utils.py:377] train in step: 475
I0712 13:35:29.492185 140217736775488 train_utils.py:377] train in step: 476
I0712 13:35:30.082051 140217736775488 train_utils.py:377] train in step: 477
I0712 13:35:30.675441 140217736775488 train_utils.py:377] train in step: 478
I0712 13:35:31.270649 140217736775488 train_utils.py:377] train in step: 479
I0712 13:35:31.864587 140217736775488 train_utils.py:377] train in step: 480
I0712 13:35:32.464909 140217736775488 train_utils.py:377] train in step: 481
I0712 13:35:33.056511 140217736775488 train_utils.py:377] train in step: 482
I0712 13:35:33.654058 140217736775488 train_utils.py:377] train in step: 483
I0712 13:35:34.247435 140217736775488 train_utils.py:377] train in step: 484
I0712 13:35:34.846154 140217736775488 train_utils.py:377] train in step: 485
I0712 13:35:35.437144 140217736775488 train_utils.py:377] train in step: 486
I0712 13:35:36.038914 140217736775488 train_utils.py:377] train in step: 487
I0712 13:35:36.636364 140217736775488 train_utils.py:377] train in step: 488
I0712 13:35:37.240930 140217736775488 train_utils.py:377] train in step: 489
I0712 13:35:37.833716 140217736775488 train_utils.py:377] train in step: 490
I0712 13:35:38.435516 140217736775488 train_utils.py:377] train in step: 491
I0712 13:35:39.020535 140217736775488 train_utils.py:377] train in step: 492
I0712 13:35:39.614616 140217736775488 train_utils.py:377] train in step: 493
I0712 13:35:40.217458 140217736775488 train_utils.py:377] train in step: 494
I0712 13:35:40.820431 140217736775488 train_utils.py:377] train in step: 495
I0712 13:35:41.415280 140217736775488 train_utils.py:377] train in step: 496
I0712 13:35:42.010026 140217736775488 train_utils.py:377] train in step: 497
I0712 13:35:42.604244 140217736775488 train_utils.py:377] train in step: 498
I0712 13:35:43.202349 140217736775488 train_utils.py:377] train in step: 499
I0712 13:35:43.792484 140217736775488 train_utils.py:377] train in step: 500
I0712 13:35:44.395418 140217736775488 train_utils.py:377] train in step: 501
I0712 13:35:44.989501 140217736775488 train_utils.py:377] train in step: 502
I0712 13:35:45.584203 140217736775488 train_utils.py:377] train in step: 503
I0712 13:35:46.176348 140217736775488 train_utils.py:377] train in step: 504
I0712 13:35:46.770918 140217736775488 train_utils.py:377] train in step: 505
I0712 13:35:47.358809 140217736775488 train_utils.py:377] train in step: 506
I0712 13:35:47.952276 140217736775488 train_utils.py:377] train in step: 507
I0712 13:35:48.550717 140217736775488 train_utils.py:377] train in step: 508
I0712 13:35:49.136530 140217736775488 train_utils.py:377] train in step: 509
I0712 13:35:49.728328 140217736775488 train_utils.py:377] train in step: 510
I0712 13:35:50.321333 140217736775488 train_utils.py:377] train in step: 511
I0712 13:35:50.911913 140217736775488 train_utils.py:377] train in step: 512
I0712 13:35:51.500065 140217736775488 train_utils.py:377] train in step: 513
I0712 13:35:52.090563 140217736775488 train_utils.py:377] train in step: 514
I0712 13:35:52.685111 140217736775488 train_utils.py:377] train in step: 515
I0712 13:35:53.280181 140217736775488 train_utils.py:377] train in step: 516
I0712 13:35:53.876598 140217736775488 train_utils.py:377] train in step: 517
I0712 13:35:54.464283 140217736775488 train_utils.py:377] train in step: 518
I0712 13:35:55.064373 140217736775488 train_utils.py:377] train in step: 519
I0712 13:35:55.659816 140217736775488 train_utils.py:377] train in step: 520
I0712 13:35:56.258836 140217736775488 train_utils.py:377] train in step: 521
I0712 13:35:56.846021 140217736775488 train_utils.py:377] train in step: 522
I0712 13:35:57.435285 140217736775488 train_utils.py:377] train in step: 523
I0712 13:35:58.028097 140217736775488 train_utils.py:377] train in step: 524
I0712 13:35:58.619719 140217736775488 train_utils.py:377] train in step: 525
I0712 13:35:59.219605 140217736775488 train_utils.py:377] train in step: 526
I0712 13:35:59.812563 140217736775488 train_utils.py:377] train in step: 527
I0712 13:36:00.403684 140217736775488 train_utils.py:377] train in step: 528
I0712 13:36:00.995792 140217736775488 train_utils.py:377] train in step: 529
I0712 13:36:01.591233 140217736775488 train_utils.py:377] train in step: 530
I0712 13:36:02.194266 140217736775488 train_utils.py:377] train in step: 531
I0712 13:36:02.776541 140217736775488 train_utils.py:377] train in step: 532
I0712 13:36:03.367264 140217736775488 train_utils.py:377] train in step: 533
I0712 13:36:03.964980 140217736775488 train_utils.py:377] train in step: 534
I0712 13:36:04.558481 140217736775488 train_utils.py:377] train in step: 535
I0712 13:36:05.151009 140217736775488 train_utils.py:377] train in step: 536
I0712 13:36:05.744140 140217736775488 train_utils.py:377] train in step: 537
I0712 13:36:06.338258 140217736775488 train_utils.py:377] train in step: 538
I0712 13:36:06.934067 140217736775488 train_utils.py:377] train in step: 539
I0712 13:36:07.531364 140217736775488 train_utils.py:377] train in step: 540
I0712 13:36:08.127020 140217736775488 train_utils.py:377] train in step: 541
I0712 13:36:08.718126 140217736775488 train_utils.py:377] train in step: 542
I0712 13:36:09.317326 140217736775488 train_utils.py:377] train in step: 543
I0712 13:36:09.913371 140217736775488 train_utils.py:377] train in step: 544
I0712 13:36:10.512449 140217736775488 train_utils.py:377] train in step: 545
I0712 13:36:11.111246 140217736775488 train_utils.py:377] train in step: 546
I0712 13:36:11.707338 140217736775488 train_utils.py:377] train in step: 547
I0712 13:36:12.301934 140217736775488 train_utils.py:377] train in step: 548
I0712 13:36:12.897945 140217736775488 train_utils.py:377] train in step: 549
I0712 13:36:13.499064 140217736775488 train_utils.py:377] train in step: 550
I0712 13:36:14.097226 140217736775488 train_utils.py:377] train in step: 551
I0712 13:36:14.724302 140217736775488 train_utils.py:377] train in step: 552
I0712 13:36:15.313441 140217736775488 train_utils.py:377] train in step: 553
I0712 13:36:15.898539 140217736775488 train_utils.py:377] train in step: 554
I0712 13:36:16.498156 140217736775488 train_utils.py:377] train in step: 555
I0712 13:36:17.093329 140217736775488 train_utils.py:377] train in step: 556
I0712 13:36:17.689316 140217736775488 train_utils.py:377] train in step: 557
I0712 13:36:18.286365 140217736775488 train_utils.py:377] train in step: 558
I0712 13:36:18.882565 140217736775488 train_utils.py:377] train in step: 559
I0712 13:36:19.480664 140217736775488 train_utils.py:377] train in step: 560
I0712 13:36:20.079384 140217736775488 train_utils.py:377] train in step: 561
I0712 13:36:20.674533 140217736775488 train_utils.py:377] train in step: 562
I0712 13:36:21.263866 140217736775488 train_utils.py:377] train in step: 563
I0712 13:36:21.855423 140217736775488 train_utils.py:377] train in step: 564
I0712 13:36:22.480856 140217736775488 train_utils.py:377] train in step: 565
I0712 13:36:23.075453 140217736775488 train_utils.py:377] train in step: 566
I0712 13:36:23.671392 140217736775488 train_utils.py:377] train in step: 567
I0712 13:36:24.262615 140217736775488 train_utils.py:377] train in step: 568
I0712 13:36:24.861970 140217736775488 train_utils.py:377] train in step: 569
I0712 13:36:25.456116 140217736775488 train_utils.py:377] train in step: 570
I0712 13:36:26.053931 140217736775488 train_utils.py:377] train in step: 571
I0712 13:36:26.648053 140217736775488 train_utils.py:377] train in step: 572
I0712 13:36:27.238691 140217736775488 train_utils.py:377] train in step: 573
I0712 13:36:27.833478 140217736775488 train_utils.py:377] train in step: 574
I0712 13:36:28.418841 140217736775488 train_utils.py:377] train in step: 575
I0712 13:36:29.008919 140217736775488 train_utils.py:377] train in step: 576
I0712 13:36:29.605692 140217736775488 train_utils.py:377] train in step: 577
I0712 13:36:30.196480 140217736775488 train_utils.py:377] train in step: 578
I0712 13:36:30.793315 140217736775488 train_utils.py:377] train in step: 579
I0712 13:36:31.387773 140217736775488 train_utils.py:377] train in step: 580
I0712 13:36:31.989468 140217736775488 train_utils.py:377] train in step: 581
I0712 13:36:32.586684 140217736775488 train_utils.py:377] train in step: 582
I0712 13:36:33.178223 140217736775488 train_utils.py:377] train in step: 583
I0712 13:36:33.775211 140217736775488 train_utils.py:377] train in step: 584
I0712 13:36:34.380527 140217736775488 train_utils.py:377] train in step: 585
I0712 13:36:34.970859 140217736775488 train_utils.py:377] train in step: 586
I0712 13:36:35.562875 140217736775488 train_utils.py:377] train in step: 587
I0712 13:36:36.154455 140217736775488 train_utils.py:377] train in step: 588
I0712 13:36:36.750314 140217736775488 train_utils.py:377] train in step: 589
I0712 13:36:37.349226 140217736775488 train_utils.py:377] train in step: 590
I0712 13:36:37.939627 140217736775488 train_utils.py:377] train in step: 591
I0712 13:36:38.524074 140217736775488 train_utils.py:377] train in step: 592
I0712 13:36:39.113886 140217736775488 train_utils.py:377] train in step: 593
I0712 13:36:39.707621 140217736775488 train_utils.py:377] train in step: 594
I0712 13:36:40.304537 140217736775488 train_utils.py:377] train in step: 595
I0712 13:36:40.892783 140217736775488 train_utils.py:377] train in step: 596
I0712 13:36:41.481971 140217736775488 train_utils.py:377] train in step: 597
I0712 13:36:42.075186 140217736775488 train_utils.py:377] train in step: 598
I0712 13:36:42.666080 140217736775488 train_utils.py:377] train in step: 599
I0712 13:36:43.259765 140217736775488 train_utils.py:377] train in step: 600
I0712 13:36:44.407241 140217736775488 train_utils.py:396] train in step: 600, loss: 0.7173999547958374, acc: 0.5
I0712 13:36:50.269381 140217736775488 train_utils.py:411] eval in step: 600, loss: 0.7540, acc: 0.4850
I0712 13:36:50.272129 140217736775488 train_utils.py:421] Testing...
I0712 13:36:56.206188 140217736775488 train_utils.py:424] test in step: 600, loss: 0.7115, acc: 0.5500
I0712 13:36:56.234982 140217736775488 train_utils.py:377] train in step: 601
I0712 13:36:56.256158 140217736775488 train_utils.py:377] train in step: 602
I0712 13:36:56.821359 140217736775488 train_utils.py:377] train in step: 603
I0712 13:36:57.418058 140217736775488 train_utils.py:377] train in step: 604
I0712 13:36:58.014909 140217736775488 train_utils.py:377] train in step: 605
I0712 13:36:58.615603 140217736775488 train_utils.py:377] train in step: 606
I0712 13:36:59.208359 140217736775488 train_utils.py:377] train in step: 607
I0712 13:36:59.810704 140217736775488 train_utils.py:377] train in step: 608
I0712 13:37:00.404528 140217736775488 train_utils.py:377] train in step: 609
I0712 13:37:00.995034 140217736775488 train_utils.py:377] train in step: 610
I0712 13:37:01.591799 140217736775488 train_utils.py:377] train in step: 611
I0712 13:37:02.187312 140217736775488 train_utils.py:377] train in step: 612
I0712 13:37:02.777378 140217736775488 train_utils.py:377] train in step: 613
I0712 13:37:03.364960 140217736775488 train_utils.py:377] train in step: 614
I0712 13:37:03.956885 140217736775488 train_utils.py:377] train in step: 615
I0712 13:37:04.551350 140217736775488 train_utils.py:377] train in step: 616
I0712 13:37:05.146795 140217736775488 train_utils.py:377] train in step: 617
I0712 13:37:05.747522 140217736775488 train_utils.py:377] train in step: 618
I0712 13:37:06.346549 140217736775488 train_utils.py:377] train in step: 619
I0712 13:37:06.944506 140217736775488 train_utils.py:377] train in step: 620
I0712 13:37:07.545726 140217736775488 train_utils.py:377] train in step: 621
I0712 13:37:08.131573 140217736775488 train_utils.py:377] train in step: 622
I0712 13:37:08.721059 140217736775488 train_utils.py:377] train in step: 623
I0712 13:37:09.315026 140217736775488 train_utils.py:377] train in step: 624
I0712 13:37:09.912633 140217736775488 train_utils.py:377] train in step: 625
I0712 13:37:10.501814 140217736775488 train_utils.py:377] train in step: 626
I0712 13:37:11.096743 140217736775488 train_utils.py:377] train in step: 627
I0712 13:37:11.698746 140217736775488 train_utils.py:377] train in step: 628
I0712 13:37:12.297562 140217736775488 train_utils.py:377] train in step: 629
I0712 13:37:12.886855 140217736775488 train_utils.py:377] train in step: 630
I0712 13:37:13.471827 140217736775488 train_utils.py:377] train in step: 631
I0712 13:37:14.058376 140217736775488 train_utils.py:377] train in step: 632
I0712 13:37:14.653950 140217736775488 train_utils.py:377] train in step: 633
I0712 13:37:15.250222 140217736775488 train_utils.py:377] train in step: 634
I0712 13:37:15.856146 140217736775488 train_utils.py:377] train in step: 635
I0712 13:37:16.446845 140217736775488 train_utils.py:377] train in step: 636
I0712 13:37:17.038091 140217736775488 train_utils.py:377] train in step: 637
I0712 13:37:17.628960 140217736775488 train_utils.py:377] train in step: 638
I0712 13:37:18.220707 140217736775488 train_utils.py:377] train in step: 639
I0712 13:37:18.821610 140217736775488 train_utils.py:377] train in step: 640
I0712 13:37:19.413965 140217736775488 train_utils.py:377] train in step: 641
I0712 13:37:20.003506 140217736775488 train_utils.py:377] train in step: 642
I0712 13:37:20.593613 140217736775488 train_utils.py:377] train in step: 643
I0712 13:37:21.192984 140217736775488 train_utils.py:377] train in step: 644
I0712 13:37:21.789242 140217736775488 train_utils.py:377] train in step: 645
I0712 13:37:22.378060 140217736775488 train_utils.py:377] train in step: 646
I0712 13:37:22.975085 140217736775488 train_utils.py:377] train in step: 647
I0712 13:37:23.578022 140217736775488 train_utils.py:377] train in step: 648
I0712 13:37:24.173084 140217736775488 train_utils.py:377] train in step: 649
I0712 13:37:24.766757 140217736775488 train_utils.py:377] train in step: 650
I0712 13:37:25.358474 140217736775488 train_utils.py:377] train in step: 651
I0712 13:37:25.947509 140217736775488 train_utils.py:377] train in step: 652
I0712 13:37:26.538193 140217736775488 train_utils.py:377] train in step: 653
I0712 13:37:27.128897 140217736775488 train_utils.py:377] train in step: 654
I0712 13:37:27.723453 140217736775488 train_utils.py:377] train in step: 655
I0712 13:37:28.325507 140217736775488 train_utils.py:377] train in step: 656
I0712 13:37:28.915151 140217736775488 train_utils.py:377] train in step: 657
I0712 13:37:29.508892 140217736775488 train_utils.py:377] train in step: 658
I0712 13:37:30.103797 140217736775488 train_utils.py:377] train in step: 659
I0712 13:37:30.694459 140217736775488 train_utils.py:377] train in step: 660
I0712 13:37:31.293453 140217736775488 train_utils.py:377] train in step: 661
I0712 13:37:31.888646 140217736775488 train_utils.py:377] train in step: 662
I0712 13:37:32.480229 140217736775488 train_utils.py:377] train in step: 663
I0712 13:37:33.087210 140217736775488 train_utils.py:377] train in step: 664
I0712 13:37:33.682715 140217736775488 train_utils.py:377] train in step: 665
I0712 13:37:34.282346 140217736775488 train_utils.py:377] train in step: 666
I0712 13:37:34.877339 140217736775488 train_utils.py:377] train in step: 667
I0712 13:37:35.472709 140217736775488 train_utils.py:377] train in step: 668
I0712 13:37:36.076679 140217736775488 train_utils.py:377] train in step: 669
I0712 13:37:36.674368 140217736775488 train_utils.py:377] train in step: 670
I0712 13:37:37.261349 140217736775488 train_utils.py:377] train in step: 671
I0712 13:37:37.860790 140217736775488 train_utils.py:377] train in step: 672
I0712 13:37:38.450668 140217736775488 train_utils.py:377] train in step: 673
I0712 13:37:39.065760 140217736775488 train_utils.py:377] train in step: 674
I0712 13:37:39.657839 140217736775488 train_utils.py:377] train in step: 675
I0712 13:37:40.255743 140217736775488 train_utils.py:377] train in step: 676
I0712 13:37:40.851603 140217736775488 train_utils.py:377] train in step: 677
I0712 13:37:41.453028 140217736775488 train_utils.py:377] train in step: 678
I0712 13:37:42.043397 140217736775488 train_utils.py:377] train in step: 679
I0712 13:37:42.634749 140217736775488 train_utils.py:377] train in step: 680
I0712 13:37:43.226209 140217736775488 train_utils.py:377] train in step: 681
I0712 13:37:43.817821 140217736775488 train_utils.py:377] train in step: 682
I0712 13:37:44.405291 140217736775488 train_utils.py:377] train in step: 683
I0712 13:37:44.994248 140217736775488 train_utils.py:377] train in step: 684
I0712 13:37:45.590762 140217736775488 train_utils.py:377] train in step: 685
I0712 13:37:46.177774 140217736775488 train_utils.py:377] train in step: 686
I0712 13:37:46.768001 140217736775488 train_utils.py:377] train in step: 687
I0712 13:37:47.362336 140217736775488 train_utils.py:377] train in step: 688
I0712 13:37:47.953783 140217736775488 train_utils.py:377] train in step: 689
I0712 13:37:48.564104 140217736775488 train_utils.py:377] train in step: 690
I0712 13:37:49.150479 140217736775488 train_utils.py:377] train in step: 691
I0712 13:37:49.739025 140217736775488 train_utils.py:377] train in step: 692
I0712 13:37:50.329956 140217736775488 train_utils.py:377] train in step: 693
I0712 13:37:50.922752 140217736775488 train_utils.py:377] train in step: 694
I0712 13:37:51.518349 140217736775488 train_utils.py:377] train in step: 695
I0712 13:37:52.122361 140217736775488 train_utils.py:377] train in step: 696
I0712 13:37:52.712834 140217736775488 train_utils.py:377] train in step: 697
I0712 13:37:53.304564 140217736775488 train_utils.py:377] train in step: 698
I0712 13:37:53.894354 140217736775488 train_utils.py:377] train in step: 699
I0712 13:37:54.483656 140217736775488 train_utils.py:377] train in step: 700
I0712 13:37:55.088755 140217736775488 train_utils.py:377] train in step: 701
I0712 13:37:55.680070 140217736775488 train_utils.py:377] train in step: 702
I0712 13:37:56.266407 140217736775488 train_utils.py:377] train in step: 703
I0712 13:37:56.860504 140217736775488 train_utils.py:377] train in step: 704
I0712 13:37:57.447660 140217736775488 train_utils.py:377] train in step: 705
I0712 13:37:58.037566 140217736775488 train_utils.py:377] train in step: 706
I0712 13:37:58.627672 140217736775488 train_utils.py:377] train in step: 707
I0712 13:37:59.221954 140217736775488 train_utils.py:377] train in step: 708
I0712 13:37:59.809850 140217736775488 train_utils.py:377] train in step: 709
I0712 13:38:00.397612 140217736775488 train_utils.py:377] train in step: 710
I0712 13:38:00.994439 140217736775488 train_utils.py:377] train in step: 711
I0712 13:38:01.586964 140217736775488 train_utils.py:377] train in step: 712
I0712 13:38:02.175254 140217736775488 train_utils.py:377] train in step: 713
I0712 13:38:02.768962 140217736775488 train_utils.py:377] train in step: 714
I0712 13:38:03.358108 140217736775488 train_utils.py:377] train in step: 715
I0712 13:38:03.952376 140217736775488 train_utils.py:377] train in step: 716
I0712 13:38:04.550303 140217736775488 train_utils.py:377] train in step: 717
I0712 13:38:05.144553 140217736775488 train_utils.py:377] train in step: 718
I0712 13:38:05.736785 140217736775488 train_utils.py:377] train in step: 719
I0712 13:38:06.325674 140217736775488 train_utils.py:377] train in step: 720
I0712 13:38:06.919598 140217736775488 train_utils.py:377] train in step: 721
I0712 13:38:07.511679 140217736775488 train_utils.py:377] train in step: 722
I0712 13:38:08.105082 140217736775488 train_utils.py:377] train in step: 723
I0712 13:38:08.698514 140217736775488 train_utils.py:377] train in step: 724
I0712 13:38:09.287838 140217736775488 train_utils.py:377] train in step: 725
I0712 13:38:09.877560 140217736775488 train_utils.py:377] train in step: 726
I0712 13:38:10.475027 140217736775488 train_utils.py:377] train in step: 727
I0712 13:38:11.071644 140217736775488 train_utils.py:377] train in step: 728
I0712 13:38:11.664946 140217736775488 train_utils.py:377] train in step: 729
I0712 13:38:12.254114 140217736775488 train_utils.py:377] train in step: 730
I0712 13:38:12.841258 140217736775488 train_utils.py:377] train in step: 731
I0712 13:38:13.432040 140217736775488 train_utils.py:377] train in step: 732
I0712 13:38:14.034092 140217736775488 train_utils.py:377] train in step: 733
I0712 13:38:14.626050 140217736775488 train_utils.py:377] train in step: 734
I0712 13:38:15.214924 140217736775488 train_utils.py:377] train in step: 735
I0712 13:38:15.813613 140217736775488 train_utils.py:377] train in step: 736
I0712 13:38:16.413038 140217736775488 train_utils.py:377] train in step: 737
I0712 13:38:17.009493 140217736775488 train_utils.py:377] train in step: 738
I0712 13:38:17.599776 140217736775488 train_utils.py:377] train in step: 739
I0712 13:38:18.188850 140217736775488 train_utils.py:377] train in step: 740
I0712 13:38:18.780179 140217736775488 train_utils.py:377] train in step: 741
I0712 13:38:19.368831 140217736775488 train_utils.py:377] train in step: 742
I0712 13:38:19.964750 140217736775488 train_utils.py:377] train in step: 743
I0712 13:38:20.555843 140217736775488 train_utils.py:377] train in step: 744
I0712 13:38:21.148566 140217736775488 train_utils.py:377] train in step: 745
I0712 13:38:21.743656 140217736775488 train_utils.py:377] train in step: 746
I0712 13:38:22.344361 140217736775488 train_utils.py:377] train in step: 747
I0712 13:38:22.940945 140217736775488 train_utils.py:377] train in step: 748
I0712 13:38:23.533560 140217736775488 train_utils.py:377] train in step: 749
I0712 13:38:24.120445 140217736775488 train_utils.py:377] train in step: 750
I0712 13:38:24.719496 140217736775488 train_utils.py:377] train in step: 751
I0712 13:38:25.315810 140217736775488 train_utils.py:377] train in step: 752
I0712 13:38:25.918925 140217736775488 train_utils.py:377] train in step: 753
I0712 13:38:26.521814 140217736775488 train_utils.py:377] train in step: 754
I0712 13:38:27.115340 140217736775488 train_utils.py:377] train in step: 755
I0712 13:38:27.713847 140217736775488 train_utils.py:377] train in step: 756
I0712 13:38:28.313716 140217736775488 train_utils.py:377] train in step: 757
I0712 13:38:28.913151 140217736775488 train_utils.py:377] train in step: 758
I0712 13:38:29.508548 140217736775488 train_utils.py:377] train in step: 759
I0712 13:38:30.100075 140217736775488 train_utils.py:377] train in step: 760
I0712 13:38:30.687363 140217736775488 train_utils.py:377] train in step: 761
I0712 13:38:31.278162 140217736775488 train_utils.py:377] train in step: 762
I0712 13:38:31.869483 140217736775488 train_utils.py:377] train in step: 763
I0712 13:38:32.471810 140217736775488 train_utils.py:377] train in step: 764
I0712 13:38:33.065588 140217736775488 train_utils.py:377] train in step: 765
I0712 13:38:33.657205 140217736775488 train_utils.py:377] train in step: 766
I0712 13:38:34.248009 140217736775488 train_utils.py:377] train in step: 767
I0712 13:38:34.847231 140217736775488 train_utils.py:377] train in step: 768
I0712 13:38:35.444441 140217736775488 train_utils.py:377] train in step: 769
I0712 13:38:36.041175 140217736775488 train_utils.py:377] train in step: 770
I0712 13:38:36.632821 140217736775488 train_utils.py:377] train in step: 771
I0712 13:38:37.222833 140217736775488 train_utils.py:377] train in step: 772
I0712 13:38:37.823168 140217736775488 train_utils.py:377] train in step: 773
I0712 13:38:38.418015 140217736775488 train_utils.py:377] train in step: 774
I0712 13:38:39.016728 140217736775488 train_utils.py:377] train in step: 775
I0712 13:38:39.610566 140217736775488 train_utils.py:377] train in step: 776
I0712 13:38:40.199578 140217736775488 train_utils.py:377] train in step: 777
I0712 13:38:40.793163 140217736775488 train_utils.py:377] train in step: 778
I0712 13:38:41.388789 140217736775488 train_utils.py:377] train in step: 779
I0712 13:38:41.985157 140217736775488 train_utils.py:377] train in step: 780
I0712 13:38:42.579902 140217736775488 train_utils.py:377] train in step: 781
I0712 13:38:43.167530 140217736775488 train_utils.py:377] train in step: 782
I0712 13:38:43.759634 140217736775488 train_utils.py:377] train in step: 783
I0712 13:38:44.354231 140217736775488 train_utils.py:377] train in step: 784
I0712 13:38:44.950485 140217736775488 train_utils.py:377] train in step: 785
I0712 13:38:45.548588 140217736775488 train_utils.py:377] train in step: 786
I0712 13:38:46.139507 140217736775488 train_utils.py:377] train in step: 787
I0712 13:38:46.736179 140217736775488 train_utils.py:377] train in step: 788
I0712 13:38:47.328338 140217736775488 train_utils.py:377] train in step: 789
I0712 13:38:47.925121 140217736775488 train_utils.py:377] train in step: 790
I0712 13:38:48.520440 140217736775488 train_utils.py:377] train in step: 791
I0712 13:38:49.122224 140217736775488 train_utils.py:377] train in step: 792
I0712 13:38:49.715888 140217736775488 train_utils.py:377] train in step: 793
I0712 13:38:50.322141 140217736775488 train_utils.py:377] train in step: 794
I0712 13:38:50.913063 140217736775488 train_utils.py:377] train in step: 795
I0712 13:38:51.504648 140217736775488 train_utils.py:377] train in step: 796
I0712 13:38:52.096709 140217736775488 train_utils.py:377] train in step: 797
I0712 13:38:52.691451 140217736775488 train_utils.py:377] train in step: 798
I0712 13:38:53.291873 140217736775488 train_utils.py:377] train in step: 799
I0712 13:38:53.887959 140217736775488 train_utils.py:377] train in step: 800
I0712 13:38:55.028227 140217736775488 train_utils.py:396] train in step: 800, loss: 0.7087999582290649, acc: 0.4749999940395355
I0712 13:39:00.952950 140217736775488 train_utils.py:411] eval in step: 800, loss: 0.6992, acc: 0.4950
I0712 13:39:00.955696 140217736775488 train_utils.py:421] Testing...
I0712 13:39:06.915263 140217736775488 train_utils.py:424] test in step: 800, loss: 0.6941, acc: 0.5200
I0712 13:39:06.942308 140217736775488 train_utils.py:377] train in step: 801
I0712 13:39:06.964954 140217736775488 train_utils.py:377] train in step: 802
I0712 13:39:07.526955 140217736775488 train_utils.py:377] train in step: 803
I0712 13:39:08.123932 140217736775488 train_utils.py:377] train in step: 804
I0712 13:39:08.720688 140217736775488 train_utils.py:377] train in step: 805
I0712 13:39:09.319148 140217736775488 train_utils.py:377] train in step: 806
I0712 13:39:09.922084 140217736775488 train_utils.py:377] train in step: 807
I0712 13:39:10.517940 140217736775488 train_utils.py:377] train in step: 808
I0712 13:39:11.116463 140217736775488 train_utils.py:377] train in step: 809
I0712 13:39:11.704718 140217736775488 train_utils.py:377] train in step: 810
I0712 13:39:12.304459 140217736775488 train_utils.py:377] train in step: 811
I0712 13:39:12.900366 140217736775488 train_utils.py:377] train in step: 812
I0712 13:39:13.496204 140217736775488 train_utils.py:377] train in step: 813
I0712 13:39:14.085380 140217736775488 train_utils.py:377] train in step: 814
I0712 13:39:14.681192 140217736775488 train_utils.py:377] train in step: 815
I0712 13:39:15.279322 140217736775488 train_utils.py:377] train in step: 816
I0712 13:39:15.898776 140217736775488 train_utils.py:377] train in step: 817
I0712 13:39:16.481318 140217736775488 train_utils.py:377] train in step: 818
I0712 13:39:17.072407 140217736775488 train_utils.py:377] train in step: 819
I0712 13:39:17.670223 140217736775488 train_utils.py:377] train in step: 820
I0712 13:39:18.268248 140217736775488 train_utils.py:377] train in step: 821
I0712 13:39:18.887270 140217736775488 train_utils.py:377] train in step: 822
I0712 13:39:19.487367 140217736775488 train_utils.py:377] train in step: 823
I0712 13:39:20.081182 140217736775488 train_utils.py:377] train in step: 824
I0712 13:39:20.681349 140217736775488 train_utils.py:377] train in step: 825
I0712 13:39:21.278983 140217736775488 train_utils.py:377] train in step: 826
I0712 13:39:21.869946 140217736775488 train_utils.py:377] train in step: 827
I0712 13:39:22.461161 140217736775488 train_utils.py:377] train in step: 828
I0712 13:39:23.056654 140217736775488 train_utils.py:377] train in step: 829
I0712 13:39:23.649342 140217736775488 train_utils.py:377] train in step: 830
I0712 13:39:24.244194 140217736775488 train_utils.py:377] train in step: 831
I0712 13:39:24.839072 140217736775488 train_utils.py:377] train in step: 832
I0712 13:39:25.432429 140217736775488 train_utils.py:377] train in step: 833
I0712 13:39:26.021693 140217736775488 train_utils.py:377] train in step: 834
I0712 13:39:26.612207 140217736775488 train_utils.py:377] train in step: 835
I0712 13:39:27.205130 140217736775488 train_utils.py:377] train in step: 836
I0712 13:39:27.816439 140217736775488 train_utils.py:377] train in step: 837
I0712 13:39:28.403585 140217736775488 train_utils.py:377] train in step: 838
I0712 13:39:29.002051 140217736775488 train_utils.py:377] train in step: 839
I0712 13:39:29.600066 140217736775488 train_utils.py:377] train in step: 840
I0712 13:39:30.195501 140217736775488 train_utils.py:377] train in step: 841
I0712 13:39:30.789555 140217736775488 train_utils.py:377] train in step: 842
I0712 13:39:31.378544 140217736775488 train_utils.py:377] train in step: 843
I0712 13:39:31.966788 140217736775488 train_utils.py:377] train in step: 844
I0712 13:39:32.557644 140217736775488 train_utils.py:377] train in step: 845
I0712 13:39:33.151968 140217736775488 train_utils.py:377] train in step: 846
I0712 13:39:33.747437 140217736775488 train_utils.py:377] train in step: 847
I0712 13:39:34.335696 140217736775488 train_utils.py:377] train in step: 848
I0712 13:39:34.928198 140217736775488 train_utils.py:377] train in step: 849
I0712 13:39:35.514984 140217736775488 train_utils.py:377] train in step: 850
I0712 13:39:36.113184 140217736775488 train_utils.py:377] train in step: 851
I0712 13:39:36.705109 140217736775488 train_utils.py:377] train in step: 852
I0712 13:39:37.297623 140217736775488 train_utils.py:377] train in step: 853
I0712 13:39:37.893354 140217736775488 train_utils.py:377] train in step: 854
I0712 13:39:38.492169 140217736775488 train_utils.py:377] train in step: 855
I0712 13:39:39.093618 140217736775488 train_utils.py:377] train in step: 856
I0712 13:39:39.696945 140217736775488 train_utils.py:377] train in step: 857
I0712 13:39:40.290473 140217736775488 train_utils.py:377] train in step: 858
I0712 13:39:40.889275 140217736775488 train_utils.py:377] train in step: 859
I0712 13:39:41.486579 140217736775488 train_utils.py:377] train in step: 860
I0712 13:39:42.085515 140217736775488 train_utils.py:377] train in step: 861
I0712 13:39:42.686065 140217736775488 train_utils.py:377] train in step: 862
I0712 13:39:43.286719 140217736775488 train_utils.py:377] train in step: 863
I0712 13:39:43.873556 140217736775488 train_utils.py:377] train in step: 864
I0712 13:39:44.469964 140217736775488 train_utils.py:377] train in step: 865
I0712 13:39:45.063481 140217736775488 train_utils.py:377] train in step: 866
I0712 13:39:45.657254 140217736775488 train_utils.py:377] train in step: 867
I0712 13:39:46.246066 140217736775488 train_utils.py:377] train in step: 868
I0712 13:39:46.834058 140217736775488 train_utils.py:377] train in step: 869
I0712 13:39:47.425056 140217736775488 train_utils.py:377] train in step: 870
I0712 13:39:48.018777 140217736775488 train_utils.py:377] train in step: 871
I0712 13:39:48.613407 140217736775488 train_utils.py:377] train in step: 872
I0712 13:39:49.205076 140217736775488 train_utils.py:377] train in step: 873
I0712 13:39:49.794677 140217736775488 train_utils.py:377] train in step: 874
I0712 13:39:50.391124 140217736775488 train_utils.py:377] train in step: 875
I0712 13:39:50.983839 140217736775488 train_utils.py:377] train in step: 876
I0712 13:39:51.574377 140217736775488 train_utils.py:377] train in step: 877
I0712 13:39:52.163772 140217736775488 train_utils.py:377] train in step: 878
I0712 13:39:52.757440 140217736775488 train_utils.py:377] train in step: 879
I0712 13:39:53.350643 140217736775488 train_utils.py:377] train in step: 880
I0712 13:39:53.949271 140217736775488 train_utils.py:377] train in step: 881
I0712 13:39:54.543276 140217736775488 train_utils.py:377] train in step: 882
I0712 13:39:55.131132 140217736775488 train_utils.py:377] train in step: 883
I0712 13:39:55.723148 140217736775488 train_utils.py:377] train in step: 884
I0712 13:39:56.318522 140217736775488 train_utils.py:377] train in step: 885
I0712 13:39:56.910912 140217736775488 train_utils.py:377] train in step: 886
I0712 13:39:57.509228 140217736775488 train_utils.py:377] train in step: 887
I0712 13:39:58.100737 140217736775488 train_utils.py:377] train in step: 888
I0712 13:39:58.694294 140217736775488 train_utils.py:377] train in step: 889
I0712 13:39:59.288214 140217736775488 train_utils.py:377] train in step: 890
I0712 13:39:59.926909 140217736775488 train_utils.py:377] train in step: 891
I0712 13:40:00.515977 140217736775488 train_utils.py:377] train in step: 892
I0712 13:40:01.106897 140217736775488 train_utils.py:377] train in step: 893
I0712 13:40:01.696452 140217736775488 train_utils.py:377] train in step: 894
I0712 13:40:02.290798 140217736775488 train_utils.py:377] train in step: 895
I0712 13:40:02.880694 140217736775488 train_utils.py:377] train in step: 896
I0712 13:40:03.473497 140217736775488 train_utils.py:377] train in step: 897
I0712 13:40:04.059722 140217736775488 train_utils.py:377] train in step: 898
I0712 13:40:04.650364 140217736775488 train_utils.py:377] train in step: 899
I0712 13:40:05.240701 140217736775488 train_utils.py:377] train in step: 900
I0712 13:40:05.830583 140217736775488 train_utils.py:377] train in step: 901
I0712 13:40:06.430072 140217736775488 train_utils.py:377] train in step: 902
I0712 13:40:07.023843 140217736775488 train_utils.py:377] train in step: 903
I0712 13:40:07.614276 140217736775488 train_utils.py:377] train in step: 904
I0712 13:40:08.214729 140217736775488 train_utils.py:377] train in step: 905
I0712 13:40:08.809754 140217736775488 train_utils.py:377] train in step: 906
I0712 13:40:09.397008 140217736775488 train_utils.py:377] train in step: 907
I0712 13:40:09.992126 140217736775488 train_utils.py:377] train in step: 908
I0712 13:40:10.584125 140217736775488 train_utils.py:377] train in step: 909
I0712 13:40:11.179441 140217736775488 train_utils.py:377] train in step: 910
I0712 13:40:11.774563 140217736775488 train_utils.py:377] train in step: 911
I0712 13:40:12.368094 140217736775488 train_utils.py:377] train in step: 912
I0712 13:40:12.949500 140217736775488 train_utils.py:377] train in step: 913
I0712 13:40:13.540529 140217736775488 train_utils.py:377] train in step: 914
I0712 13:40:14.136442 140217736775488 train_utils.py:377] train in step: 915
I0712 13:40:14.734735 140217736775488 train_utils.py:377] train in step: 916
I0712 13:40:15.326473 140217736775488 train_utils.py:377] train in step: 917
I0712 13:40:15.913410 140217736775488 train_utils.py:377] train in step: 918
I0712 13:40:16.505228 140217736775488 train_utils.py:377] train in step: 919
I0712 13:40:17.100015 140217736775488 train_utils.py:377] train in step: 920
I0712 13:40:17.697818 140217736775488 train_utils.py:377] train in step: 921
I0712 13:40:18.288763 140217736775488 train_utils.py:377] train in step: 922
I0712 13:40:18.880691 140217736775488 train_utils.py:377] train in step: 923
I0712 13:40:19.474937 140217736775488 train_utils.py:377] train in step: 924
I0712 13:40:20.075403 140217736775488 train_utils.py:377] train in step: 925
I0712 13:40:20.673647 140217736775488 train_utils.py:377] train in step: 926
I0712 13:40:21.263461 140217736775488 train_utils.py:377] train in step: 927
I0712 13:40:21.852285 140217736775488 train_utils.py:377] train in step: 928
I0712 13:40:22.449930 140217736775488 train_utils.py:377] train in step: 929
I0712 13:40:23.047172 140217736775488 train_utils.py:377] train in step: 930
I0712 13:40:23.652295 140217736775488 train_utils.py:377] train in step: 931
I0712 13:40:24.241405 140217736775488 train_utils.py:377] train in step: 932
I0712 13:40:24.838500 140217736775488 train_utils.py:377] train in step: 933
I0712 13:40:25.431993 140217736775488 train_utils.py:377] train in step: 934
I0712 13:40:26.032160 140217736775488 train_utils.py:377] train in step: 935
I0712 13:40:26.623436 140217736775488 train_utils.py:377] train in step: 936
I0712 13:40:27.218859 140217736775488 train_utils.py:377] train in step: 937
I0712 13:40:27.819526 140217736775488 train_utils.py:377] train in step: 938
I0712 13:40:28.419269 140217736775488 train_utils.py:377] train in step: 939
I0712 13:40:29.015770 140217736775488 train_utils.py:377] train in step: 940
I0712 13:40:29.614272 140217736775488 train_utils.py:377] train in step: 941
I0712 13:40:30.210687 140217736775488 train_utils.py:377] train in step: 942
I0712 13:40:30.803344 140217736775488 train_utils.py:377] train in step: 943
I0712 13:40:31.385718 140217736775488 train_utils.py:377] train in step: 944
I0712 13:40:31.980665 140217736775488 train_utils.py:377] train in step: 945
I0712 13:40:32.572178 140217736775488 train_utils.py:377] train in step: 946
I0712 13:40:33.161188 140217736775488 train_utils.py:377] train in step: 947
I0712 13:40:33.746447 140217736775488 train_utils.py:377] train in step: 948
I0712 13:40:34.335009 140217736775488 train_utils.py:377] train in step: 949
I0712 13:40:34.928698 140217736775488 train_utils.py:377] train in step: 950
I0712 13:40:35.553216 140217736775488 train_utils.py:377] train in step: 951
I0712 13:40:36.143395 140217736775488 train_utils.py:377] train in step: 952
I0712 13:40:36.737591 140217736775488 train_utils.py:377] train in step: 953
I0712 13:40:37.331545 140217736775488 train_utils.py:377] train in step: 954
I0712 13:40:37.931024 140217736775488 train_utils.py:377] train in step: 955
I0712 13:40:38.523372 140217736775488 train_utils.py:377] train in step: 956
I0712 13:40:39.116516 140217736775488 train_utils.py:377] train in step: 957
I0712 13:40:39.704267 140217736775488 train_utils.py:377] train in step: 958
I0712 13:40:40.305240 140217736775488 train_utils.py:377] train in step: 959
I0712 13:40:40.904868 140217736775488 train_utils.py:377] train in step: 960
I0712 13:40:41.510640 140217736775488 train_utils.py:377] train in step: 961
I0712 13:40:42.097802 140217736775488 train_utils.py:377] train in step: 962
I0712 13:40:42.693807 140217736775488 train_utils.py:377] train in step: 963
I0712 13:40:43.286221 140217736775488 train_utils.py:377] train in step: 964
I0712 13:40:43.888734 140217736775488 train_utils.py:377] train in step: 965
I0712 13:40:44.481428 140217736775488 train_utils.py:377] train in step: 966
I0712 13:40:45.076649 140217736775488 train_utils.py:377] train in step: 967
I0712 13:40:45.671544 140217736775488 train_utils.py:377] train in step: 968
I0712 13:40:46.265887 140217736775488 train_utils.py:377] train in step: 969
I0712 13:40:46.854922 140217736775488 train_utils.py:377] train in step: 970
I0712 13:40:47.476553 140217736775488 train_utils.py:377] train in step: 971
I0712 13:40:48.068316 140217736775488 train_utils.py:377] train in step: 972
I0712 13:40:48.658785 140217736775488 train_utils.py:377] train in step: 973
I0712 13:40:49.249903 140217736775488 train_utils.py:377] train in step: 974
I0712 13:40:49.844219 140217736775488 train_utils.py:377] train in step: 975
I0712 13:40:50.456133 140217736775488 train_utils.py:377] train in step: 976
I0712 13:40:51.054733 140217736775488 train_utils.py:377] train in step: 977
I0712 13:40:51.637683 140217736775488 train_utils.py:377] train in step: 978
I0712 13:40:52.229296 140217736775488 train_utils.py:377] train in step: 979
I0712 13:40:52.819330 140217736775488 train_utils.py:377] train in step: 980
I0712 13:40:53.406799 140217736775488 train_utils.py:377] train in step: 981
I0712 13:40:53.994831 140217736775488 train_utils.py:377] train in step: 982
I0712 13:40:54.589024 140217736775488 train_utils.py:377] train in step: 983
I0712 13:40:55.178048 140217736775488 train_utils.py:377] train in step: 984
I0712 13:40:55.771760 140217736775488 train_utils.py:377] train in step: 985
I0712 13:40:56.360903 140217736775488 train_utils.py:377] train in step: 986
I0712 13:40:56.956668 140217736775488 train_utils.py:377] train in step: 987
I0712 13:40:57.543337 140217736775488 train_utils.py:377] train in step: 988
I0712 13:40:58.133749 140217736775488 train_utils.py:377] train in step: 989
I0712 13:40:58.723336 140217736775488 train_utils.py:377] train in step: 990
I0712 13:40:59.318107 140217736775488 train_utils.py:377] train in step: 991
I0712 13:40:59.906241 140217736775488 train_utils.py:377] train in step: 992
I0712 13:41:00.499846 140217736775488 train_utils.py:377] train in step: 993
I0712 13:41:01.088272 140217736775488 train_utils.py:377] train in step: 994
I0712 13:41:01.681774 140217736775488 train_utils.py:377] train in step: 995
I0712 13:41:02.274917 140217736775488 train_utils.py:377] train in step: 996
I0712 13:41:02.864430 140217736775488 train_utils.py:377] train in step: 997
I0712 13:41:03.451117 140217736775488 train_utils.py:377] train in step: 998
I0712 13:41:04.040213 140217736775488 train_utils.py:377] train in step: 999
I0712 13:41:04.634033 140217736775488 train_utils.py:377] train in step: 1000
I0712 13:41:04.639137 140217736775488 checkpoints.py:120] Saving checkpoint at step: 1000
I0712 13:41:27.904701 140217736775488 checkpoints.py:149] Saved checkpoint at trained_models/matching/synthesizer/checkpoint_1000
I0712 13:41:28.112205 140217736775488 train_utils.py:396] train in step: 1000, loss: 0.6997999548912048, acc: 0.4949999749660492
I0712 13:41:34.138398 140217736775488 train_utils.py:411] eval in step: 1000, loss: 0.6924, acc: 0.5250
I0712 13:41:34.141045 140217736775488 train_utils.py:421] Testing...
I0712 13:41:40.300812 140217736775488 train_utils.py:424] test in step: 1000, loss: 0.6959, acc: 0.4300
