2022-07-12 12:51:20.016418: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2022-07-12 12:51:20.016700: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2022-07-12 12:51:20.016817: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2022-07-12 12:51:20.016932: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2022-07-12 12:51:20.017044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2022-07-12 12:51:20.017155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2022-07-12 12:51:20.017266: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2022-07-12 12:51:20.017297: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0712 12:51:20.017724 140282259928896 train.py:67] ===========Config Dict============
I0712 12:51:20.017945 140282259928896 train.py:68] available_devices:
- 0
- 1
- 2
- 3
base_type: dual_encoder
batch_size: 4
checkpoint_freq: 10000
data_dir: google_datasets/doc_retrieval/tsv_data/
emb_dim: 128
eval_frequency: 200
factors: constant * linear_warmup * rsqrt_decay
learning_rate: 0.05
max_eval_target_length: 200
max_length: 4000
max_predict_token_length: 50
max_target_length: 200
mlp_dim: 512
model_type: transformer
num_eval_steps: 50
num_heads: 4
num_layers: 4
num_train_steps: 1001
pooling_mode: CLS
prompt: ''
qkv_dim: 128
random_seed: 0
restore_checkpoints: true
sampling_temperature: 0.6
sampling_top_k: 20
save_checkpoints: true
task_name: aan_pairs
tokenizer: char
trial: 0
vocab_file_path: google_datasets/doc_retrieval/aan/
warmup: 1000
weight_decay: 0.1

I0712 12:51:20.031533 140282259928896 xla_bridge.py:340] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0712 12:51:20.992867 140282259928896 xla_bridge.py:340] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0712 12:51:20.993390 140282259928896 xla_bridge.py:340] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0712 12:51:20.993589 140282259928896 train.py:80] GPU devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)]
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
I0712 12:51:20.993726 140282259928896 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
I0712 12:51:21.061013 140282259928896 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 12:51:21.093573 140282259928896 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 12:51:23.579481 140282259928896 input_pipeline.py:60] Data sample: OrderedDict([('label', 1.0), ('id1', b'W12-3146'), ('id2', b'N07-1064'), ('text1', b"b'1 Introduction The today?s outputs of Machine Translation (MT) often contain serious grammatical errors. This is particularly apparent in statistical MT systems (SMT), which do not employ structural linguistic rules. These systems have been dominating the area in the recent years (Callison-Burch et al, 2011). Such errors make the translated text less fluent and may even lead to unintelligibility or misleading statements. The problem is more evident in languages with rich morphology, such as Czech, where morphological agreement is of a relatively high importance for the interpretation of syntactic relations. The DEPFIX system (Marec?ek et al, 2011) attempts to correct some of the frequent SMT sys?This research has been supported by the European Union Seventh Framework Programme (FP7) under grant agreement n? 247762 (Faust), and by the grants GAUK116310, GA201/09/H057 (Res-Informatica), and LH12093. tems? errors in English-to-Czech translations.1 It analyzes the target sentence (the SMT output in Czech language) using a morphological tagger and a dependency parser and attempts to correct it by applying several rules which enforce consistency with the Czech grammar. Most of the rules use the source sentence (the SMT input in English language) as a source of information about the sentence structure. The source sentence is also tagged and parsed, and word-to-word alignment with the target sentence is determined. In this paper, we present DEPFIX 2012, an improved version of the original DEPFIX 2011 system. It makes use of a new parser, described briefly in Section 3, which is adapted to handle the generally ungrammatical target sentences better. We have also enhanced the set of grammar correction rules, for which we give a detailed description in Section 4. Section 5 gives an account of the experiments performed to evaluate the DEPFIX 2012 system and compare it to DEPFIX 2011. Section 6 then concludes the paper. 2 Related Work Our approach can be regarded as converse to the more common way of using an SMT system to automatically post-edit the output of a rule-based translation system, as described e.g. in (Simard et al, 2007) or (Lagarda et al, 2009). The DEPFIX system is implemented in the 1Although we apply the DEPFIX system just to SMT systems in this paper as it mainly targets the errors induced by this type of MT systems, it can be applied to virtually any MT system (Marec?ek et al, 2011). 362 TectoMT/Treex NLP framework (Popel and Z?abokrtsky?, 2010),2 using the Morc?e tagger (Spoustova? et al, 2007) and the MST parser (McDonald et al, 2005) trained on the CoNLL 2007 Shared Task English data (Nivre et al, 2007) to analyze the source sentences. The source and target sentences are aligned using GIZA++ (Och and Ney, 2003). 3 Parsing The DEPFIX 2011 system used the MST parser (McDonald et al, 2005) with an improved feature set for Czech (Nova?k and Z?abokrtsky?, 2007) trained on the Prague Dependency Treebank (PDT) 2.0 (Hajic? and others, 2006) to analyze the target sentences. DEPFIX 2012 uses a reimplementation of the MST parser capable of utilizing parallel features from the source side in the parsing of the target sentence. The sourcetext is usually grammatical and therefore is likely to be analyzed more reliably. The source structure obtained in this way can then provide hints for the target parser. We use local features projected through the GIZA++ word alignment ? i.e. for each target word, we add features computed over its aligned source word, if there is one. To address the differences between the gold standard training data and SMT outputs, we ?worsen? the treebank used to train the parser, i.e. introduce errors similar to those found in target sentences: The trees retain their correct structure, only the word forms are modified to resemble SMT output. We have computed a ?part-of-speech tag error model? on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al, 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al, 2007) and estimating the Maximum Likelihood probabilities of errors for each part-ofspeech tag. We then applied this error model to the Czech PCEDT 2.0 sentences and used the resulting ?worsened? treebank to train the parser. 4 Rules DEPFIX 2012 uses 20 hand-written rules, addressing various frequent errors in MT output. Each rule takes an analyzed target sentence as its input, often together with its analyzed source sen2http://ufal.mff.cuni.cz/treex tence, and attempts to correct any errors found ? usually by changing morphosyntactic categories of a word (such as number, gender, case, person and dependency label) and regenerating the corresponding word form if necessary, more rarely by deleting superfluous particles or auxiliary words or changing the target dependency tree structure. However, neither word order problems nor bad lexical choices are corrected. Many rules were already present in DEPFIX 2011. However, most were modified in DEPFIX 2012 to achieve better performance (denoted as modified), and new rules were added (new). Rules not modified since DEPFIX 2011 are denoted as reused. The order of rule application is important as there are dependencies among the rules, e.g. FixPrepositionNounAgreement (enforcing noun-preposition congruency) depends on FixPrepositionalCase (fixing incorrectly tagged prepositional case). The rules are applied in the order listed in Table 2. 4.1 Analysis Fixing Rules Analysis fixing rules try to detect and rectify tagger and parser errors. They do not change word forms and are therefore invisible on the output as such; however, rules of other types benefit from their corrections. FixPrepositionalCase (new) This rule corrects part-of-speech-tag errors in prepositional phrases. It looks for all words that depend on a preposition and do not match its part-ofspeech tag case. It tries to find and assign a common morphological case fitting for both the word form and the preposition. Infrequent prepositioncase combinations are not considered. FixReflexiveTantum (new) If the word form ?se? or ?si? is classified as reflexive tantum particle by the parser, but does not belong to an actual reflexive tantum verb (or a deverbative noun or an adjective), its dependency label is changed to a different value, based on the context. FixNounNumber (reused) If a noun is tagged as singular in target but as plural in source, the tag is likely to be incorrect.This rule tries to find a tag that would match both the 363 source number and the target word form, changing the target case if necessary. FixPrepositionWithoutChildren (reused) A target preposition with no child nodes is clearly an analysis error. This rule tries to find children for childless prepositions by projecting the children of the aligned source preposition to the target side. FixAuxVChildren (new) Since auxiliary verbs must not have child nodes, we rehang all their children to the governing full verb. 4.2 Agreement Fixing Rules These rules relate to morphological agreement required by Czech grammar, which they try to enforce in case it is violated. Czech grammar requires agreement in morphological gender, number, case and person where applicable. These rules typically use the source sentence only for confirmation. FixRelativePronoun (new) The Czech word relative pronoun ?ktery?? is assigned gender and number identical to the closest preceding noun or pronoun, if the source analysis confirms that it depends on this noun/pronoun. FixSubject (modified) The subject (if the subject dependency label is confirmed by the source analysis) will have its case set to nominative; the number is changed if this leads to the word form staying unchanged. FixVerbAuxBeAgreement (modified) If an auxiliary verb is a child of an infinitive, the auxiliary verb receives the gender and number of the subject, which is a child of the infinitive (see also FixAuxVChildren). FixSubjectPredicateAgreement (modified) An active verb form receives the number and person from its subject (whose relation to the verb must be confirmed by the source). FixSubjectPastParticipleAgreement (modified) A past participle verb form receives the number and gender from its subject (confirmed by the source analysis). FixPassiveAuxBeAgreement (modified) An auxiliary verb ?by?t? (?to be?) depending on a passive verb form receives its gender and number. FixPrepositionNounAgreement (modified) A noun or adjective depending on a preposition receives its case. The dependency must be confirmed in the source. FixNounAdjectiveAgreement (modified) An adjective (or an adjective-like pronoun or numeral) preceding its governing noun receives its gender, number and case. 4.3 Translation Fixing Rules The following rules detect and correct structures often mistranslated by SMT systems. They usually depend heavily on the source sentence. FixBy (new) English preposition ?by? is translated to Czech using the instrumental case (if modifying a verb, e.g. ?built by David?: ?postaveno Davidem?) or using the genitive case (if modifying a noun, e.g. ?songs by David?: ?p??sne? Davida?). FixPresentContinuous (modified) If the source sentence is in a continuous tense (e.g. ?Ondr?ej isn?t experimenting.?), the auxiliary verb ?to be? must not appear on the output, which is often the case (e.g. *?Ondr?ej nen?? experimentovat.?). This rule deletes the auxiliary verb in target and transfers its morphological categories to the main verb (e.g. ?Ondr?ej neexperimentuje.?). FixVerbByEnSubject (new) If the subject of the source sentence is a personal pronoun, its following morphological categeries are propagated to the target predicate: ? person ? number (except for ?you?, which does not exhibit number) ? gender (only in case of ?he? or ?she?, which exhibit the natural gender) FixOf (new) English preposition ?of? modifying a noun is translated to Czechusing the genitive case (e.g. ?pictures of Rudolf?: ?obra?zky Rudolfa?). 364 FixAuxT (reused) Reflexive tantum particles ?se? or ?si? not belonging to any verb or adjective are deleted. This situation usually occurs when the meaning of the source verb/adjective is lost in translation and only the particle is produced. 4.4 Other Rules VocalizePrepos (reused) Prepositions ?k?, ?s?, ?v?, ?z? are vocalized (i.e. changed to ?ke?, ?se?, ?ve?, ?ze?) where necessary. The vocalization rules in Czech are similar to ?a?/?an? distinction in English. FixFirstWordCapitalization (new) If the first word of source is capitalized and the first word of target is not, this rule capitalizes it. 5 Experiments and Results For parameter tuning, we used datasets from the WMT10 translation task and translations by ONLINEB and CU-BOJAR systems. 5.1 Manual Evaluation Manual evaluation of both DEPFIX 2011 and DEPFIX 2012 was performed on the WMT113 test set translated by ONLINEB. 500 sentences were randomly selected and blind-evaluated by two independent annotators, who were presented with outputs of ONLINEB, DEPFIX 2011 and DEPFIX 2012. (For 246 sentences, at least one of the DEPFIX setups modified the ONLINEB translation.) They provided us with a pairwise comparison of the three setups, with the possibility to mark the sentence as ?indefinite? if translations were of equal quality. The results are given in Table 1. In Table 2, we use the manual evaluation to measure the performance of the individual rules in DEPFIX 2012. For each rule, we ran DEPFIX 2012 with this rule disabled and compared the output to the output of the full DEPFIX 2012. The number of affected sentences on the whole WMT11 test set, given as ?changed?, represents the impact of the rule. The number of affected sentences selected for manual evaluation is listed as ?evaluated?. Finally, the annotators? ratings of the ?evaluated? sentences 3http://www.statmt.org/wmt11 A / B Setup 1 Setup 2 Indefinite better better Setup 1 better 55% 1% 11% Setup 2 better 1% 8% 4% Indefinite 3% 2% 15% Table 3: Inter-annotator agreement matrix for ONLINEB + DEPFIX 2012 as Setup 1 and ONLINEB as Setup 2. (suggesting whether the rule improved or worsened the translation, or whether the result was indefinite) were counted and divided by the number of annotators to get the average performance of each rule. Please note that the lower the ?evaluated? number, the lower the confidence of the results. The inter-annotator agreement matrix for comparison of ONLINEB + DEPFIX 2012 (denoted as Setup 1) with ONLINEB (Setup 2) is given in Table 3. The results for the other two setup pairs were similar, with the average inter-annotator agreement being 77%. 5.2 Automatic Evaluation We also performed several experiments with automatic evaluation using the standard BLEU metric (Papineni et al, 2002). As the effect of DEPFIX in terms of BLEU is rather small, the results are not as confident as the results of manual evaluation.4 In Table 4, we compare the DEPFIX 2011 and DEPFIX 2012 systems and measure the contribution of parser adaptation (Section 3) and rule improvements (Section 4). It can be seen that the combined effectof applying both system modifications is greater than when they are applied alone. The improvement of DEPFIX 2012 over ONLINEB without DEPFIX is statistically significant at 95% confidence level. The effect of DEPFIX 2012 on the outputs of some of the best-scoring SMT systems in the WMT12 Translation Task5 is shown in Table 5. Although DEPFIX 2012 was tuned only on ONLINEB and CUBOJAR system outputs, it improves the BLEU score of all the best-scoring systems, which suggests that 4As already noted by Marec?ek et al (2011), BLEU seems not to be very suitable for evaluation of DEPFIX. See (Kos and Bojar, 2009) for a detailed study of BLEU performance when applied to evaluation of MT systems with Czech as the target language. 5http://www.statmt.org/wmt12 365 Setup 1 Setup 2 Differing Annotator Setup 1 Setup 2 Indefinite sentences better better ONLINEB ONLINEB 169 A 58% 13% 29% + DEPFIX 2011 B 47% 11% 42% ONLINEB ONLINEB 234 A 65% 14% 21% + DEPFIX 2012 B 59% 11% 30% ONLINEB ONLINEB 148 A 54% 24% 22% + DEPFIX 2012 + DEPFIX 2011 B 56% 22% 22% Table 1: Manual pairwise comparison on 500 sentences from WMT11 test set processed by ONLINEB, ONLINEB + DEPFIX 2011 and ONLINEB + DEPFIX 2012. Evaluated by two independent annotators. Sentences Rule changed evaluated impr. % wors. % indef. % FixPrepositionalCase 34 5 3 60 2 40 0 0 FixReflexiveTantum 1 0 ? ? ? ? ? ? FixNounNumber 80 11 5 45 5 45 1 9 FixPrepositionWithoutChildren 16 6 3 50 3 50 0 0 FixBy 75 13 10.5 81 1 8 1.5 12 FixAuxVChildren 26 6 4.5 75 0 0 1.5 25 FixRelativePronoun 56 8 6 75 2 25 0 0 FixSubject 142 18 13.5 75 3 17 1.5 8 FixVerbAuxBeAgreement 8 2 1 50 1 50 0 0 FixPresentContinuous 30 7 5.5 79 1 14 0.5 7 FixSubjectPredicateAgreement 87 10 5.5 55 1 10 3.5 35 FixSubjectPastParticipleAgreement 396 63 46.5 74 9.5 15 7 11 FixVerbByEnSubject 25 6 5 83 0 0 1 17 FixPassiveAuxBeAgreement 43 8 6 75 0.5 6 1.5 19 FixPrepositionNounAgreement 388 62 40 65 13 21 9 15 FixOf 84 13 11.5 88 0 0 1.5 12 FixNounAdjectiveAgreement 575 108 69.5 64 20 19 18.5 17 FixAuxT 38 7 4 57 1 14 2 29 VocalizePrepos 53 12 6 50 2.5 21 3.5 29 FixFirstWordCapitalization 0 0 ? ? ? ? ? ? Table 2: Impact and accuracy of individual DEPFIX 2012 rules using manual evaluation on 500 sentences from WMT11 test set translated by ONLINEB. The number of changed sentences is counted on the whole WMT11 test set, i.e. 3003 sentences. The numbers of improved, worsened and indefinite translations are averaged over the annotators. 366 DEPFIX setup BLEU without DEPFIX 19.37 DEPFIX 2011 19.41 DEPFIX 2011 + new parser 19.42 DEPFIX 2011 + new rules 19.48 DEPFIX 2012 19.56 Table 4: Performance of ONLINEB and various DEPFIX setups on the WMT11 test set. System BLEU ONLINEB 16.25 ONLINEB + DEPFIX 2012 16.31 UEDIN 15.54 UEDIN + DEPFIX 2012 15.75 CU-BOJAR 15.41CU-BOJAR + DEPFIX 2012 15.45 CU-TAMCH-BOJ 15.35 CU-TAMCH-BOJ + DEPFIX 2012 15.39 Table 5: Comparison of BLEU of baseline system output and corrected system output on WMT12 test set. it is able to improve the quality of various SMT systems when applied to their outputs. (The improvement on UEDIN is statistically significant at 95% confidence level.) We submitted the ONLINEB + DEPFIX 2012 system to the WMT12 Translation Task as CU-DEPFIX.6 Conclusion We have presented two improvements to DEPFIX, a system of rule-based post-editing of English-toCzech Machine Translation outputs proven by manual and automatic evaluation to improve the quality of the translations produced by state-of-the-art SMT systems. First, improvements in the existing rules and implementation of new ones, which can be regarded as an additive, evolutionary change. Second, a modified dependency parser, adjusted to parsing of SMT outputs by training it on a parallel treebank with worsened word forms on the Czech side. We showed that both changes led to a better performance of the new DEPFIX 2012, both individually and combined. In future, we are planning to incorporate deeper analysis, devising rules that would operate on the deep-syntactic, or tectogrammatical, layer. The Czech and English tectogrammatical trees are more similar to each other, which should enable us to exploit more information from the source sentences. We also hope to be able to perform more complex corrections, such as changing the part of speech of a word when necessary. Following the success of our modified parser, we would also like to modify the tagger in a similar way, since incorrect analyses produced by the tagger often hinder the correct function of our rules, sometimes leading to a rule worsening the translation instead of improving it. As observed e.g. by Groves and Schmidtke (2009) for English-to-German and English-to-French translations, SMT systems for other language pairs also tend to produce reoccurring grammatical errors. We believe that these could be easily detected and corrected in a rule-based way, using an approach similar to ours.'"), ('text2', b"b'1 Introduction The quality of machine translation (MT) is generally considered insufficient for use in the field without a significant amount of human correction. In the translation world, the term post-editing is often used to refer to the process of manually correcting MT output. While the conventional wisdom is that postediting MT is usually not cost-efficient compared to full human translation, there appear to be situations where it is appropriate and even profitable. Unfortunately, there are few reports in the literature about such experiences (but see Allen (2004) for examples). One of the characteristics of the post-editing task, as opposed to the revision of human translation for example, is its partly repetitive nature. Most MT systems invariably produce the same output when confronted with the same input; in particular, this means that they tend to make the same mistakes over and over again, which the post-editors must correct repeatedly. Batch corrections are sometimes possible when multiple occurrences of the same mistake appear in the same document, but when it is repeated over several documents, or equivalently, when the output of the same machine translation system is handled by multiple post-editors, then the opportunities for factoring corrections become much more complex. MT users typically try to reduce the post-editing load by customizing their MT systems. However, in Rule-based Machine Translation (RBMT), which still constitutes the bulk of the current commercial offering, customization is usually restricted to the development of ?user dictionaries?. Not only is this time-consuming and expensive, it can only fix a subset of the MT system?s problems. The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem: because the MT system learns directly from existing translations, it can be automatically customized to new domains and tasks. However, the success of this operation cru508 cially depends on the amount of training data available. Moreover, the current state of the technology is still insufficient for consistently producing human readable translations. This state of affairs has prompted some to examine the possibility of automating the post-editing process itself, at least as far as ?repetitive errors? are concerned. Allen and Hogan (2000) sketch the outline of such an automated post-editing (APE) system, which would automatically learn post-editing rules from a tri-parallel corpus of source, raw MT and post-edited text. Elming (2006) suggests using tranformation-based learning to automatically acquire error-correcting rules from such data; however, the proposed method only applies to lexical choice errors. Knight and Chander (1994) also argue in favor of using a separate APE module, which is then portable across multiple MT systems and language pairs, and suggest that the post-editing task could be performed using statistical machine translation techniques. To the best of our knowledge, however, this idea has never been implemented. In this paper, we explore the idea of using a PBMT system as an automated post-editor. The underlying intuition is simple: if we collect a parallel corpus of raw machine-translation output, along with itshuman-post-edited counterpart, we can train the system to translate from the former into the latter. In section 2, we present the case study that motivates our work and the associated data. In section 3, we describe the phrase-based post-editing model that we use for improving the output of the automatic translation system. In section 4, we illustrate this on a dataset of moderate size containing job ads and their translation. With less than 500k words of training material, the phrase-based MT system already outperforms the rule-based MT baseline. However, a phrase-based post-editing model trained on the output of that baseline outperforms both by a fairly consistent margin. The resulting BLEU score increases by up to 50% (relative) and the TER is cut by one third. 2 Background 2.1 Context The Canadian government?s department of Human Resources and Social Development (HRSDC) maintains a web site called Job Bank,1 where potential employers can post ads for open positions in Canada. Over one million ads are posted on Job Bank every year, totalling more than 180 million words. By virtue of Canada?s Official Language Act, HRSDC is under legal obligation to post all ads in both French and English. In practice, this means that ads submitted in English must be translated into French, and vice-versa. To address this task, the department has put together a complex setup, involving text databases, translation memories, machine translation and human post-editing. Employers submit ads to the Job Bank website by means of HTML forms containing ?free text? data fields. Some employers do periodical postings of identical ads; the department therefore maintains a database of previously posted ads, along with their translations, and new ads are systematically checked against this database. The translation of one third of all ads posted on the Job Bank is actually recuperated this way. Also, employers will often post ads which, while not entirely identical, still contain identical sentences. The department therefore also maintains a translation memory of individual sentence pairs from previously posted ads; another third of all text is typically found verbatim in this way. The remaining text is submitted to machine translation, and the output is post-edited by human experts. Overall, only a third of all submitted text requires human intervention. This is nevertheless very labour-intensive, as the department tries to ensure that ads are posted at most 24 hours after submission. The Job Bank currently employs as many as 20 post-editors working full-time, most of whom are junior translators. 2.2 The Data HRSDC kindly provided us with a sample of data from the Job Bank. This corpus consists in a collection of parallel ?blocks? of textual data. Each block contains three parts: the source language text, as submitted by the employer, its machine-translation, produced by a commercial rule-based MT system, and its final post-edited version, as posted on the website. 1http://www.jobbank.gc.ca 509 The entire corpus contains less than one million words in each language. This corresponds to the data processed in less than a week by the Job Bank. Basic statistics are given in Table 1 (see Section 4.1). Most blockscontain only one sentence, but some blocks may contain many sentences. The longest block contains 401 tokens over several sentences. Overall, blocks are quite short: the median number of tokens per source block is only 9 for French-toEnglish and 7 for English-to-French. As a consequence, no effort was made to segment the blocks further for processing. We evaluated the quality of the Machine Translation contained in the corpus using the Translation Edit Rate (TER, cf. Snover et al (2006)). The TER counts the number of edit operations, including phrasal shifts, needed to change a hypothesis translation into an adequate and fluent sentence, and normalised by the length of the final sentence. Note that this closely corresponds to the post-editing operation performed on the Job Bank application. This motivates the choice of TER as the main metric in our case, although we also report BLEU scores in our experiments. Note that the emphasis of our work is on reducing the post-edition effort, which is well estimated by TER. It is not directly on quality so the question of which metric better estimates translation quality is not so relevant here. The global TER (over all blocks) are 58.77% for French-to-English and 53.33% for English-toFrench. This means that more than half the words have to be post-edited in some way (delete / substitute / insert / shift). This apparently harsh result is somewhat mitigated by two factors. First, the distribution of the block-based TER2 shows a large disparity in performance, cf. Figure 1. About 12% of blocks have a TER higher than 100%: this is because the TER normalises on the length of the references, and if the raw MT output is longer than its post-edited counterpart, then the number of edit operations may be larger than that length.3 At the other end of the spectrum, it is also clear that many blocks have low TER. In fact more than 10% 2Contrary to BLEU or NIST, the TER naturally decomposes into block-based scores. 3A side effect of the normalisation is that larger TER are measured on small sentences, e.g. 3 errors for 2 reference words. Histogram of TER for rule?based MT TER for rule?based MT Fr eq ue nc y 0 50 100 150 0 10 00 20 00 30 00 40 00 Figure 1: Distribution of TER on 39005 blocks from the French-English corpus (thresholded at 150%). have a TER of 0. The global score therefore hides a large range of performance. The second factor is that the TER measures the distance to an adequate and \\x03uent result. A high TER does not mean that the raw MT output is not understandable. However, many edit operations may be needed to make it fluent. 3 Phrase-based Post-editing Translation post-editing can be viewed as a simple transformation process, which takes as input raw target-language text coming from a MT system, and produces as output target-language text in which ?errors? have been corrected. While the automation of this process can be envisaged in many different ways, the task is not conceptually very different from the translation taskitself. Therefore, there doesn?t seem to be any good reason why a machine translation system could not handle the post-editing task. In particular, given such data as described in Section 2.2, the idea of using a statistical MT system for post-editing is appealing. Portage is precisely such a system, which we describe here. Portage is a phrase-based, statistical machine translation system, developed at the National Research Council of Canada (NRC) (Sadat et al, 510 2005). A version of the Portage system is made available by the NRC to Canadian universities for research and education purposes. Like other SMT systems, it learns to translate from existing parallel corpora. The system translates text in three main phases: preprocessing of raw data into tokens; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. For languages such as French and English, the first of these phases (tokenization) is mostly a straightforward process; we do not describe it any further here. Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P (t|s). Portage implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose sourcelanguage part matches the input. These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora. Portage?s model for P (t|s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P (s?|t?) are smoothed using the Good-Turing technique (Foster et al, 2006). The distortion model is also very similar to Koehn?s, with the exception of a final cost to account for sentence endings. Feature function weights in the loglinear model are set using Och?s minium error rate algorithm (Och, 2003). This is essentially an iterative two-step process: for a given set of source sentences, generate n-best translation hypotheses, that are representative of the entire decoding search space; then, apply a variant of Powell?s algorithm to find weights that optimize the BLEU score over these hypotheses, compared to reference translations. This process is repeated until the set of translations stabilizes, i.e. no new translations are produced at the decoding step. To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder, the system reorders this list, this time using a more elaborate loglinear model, incorporating more feature functions, in addition to those of the decoding model: these typically include IBM-1 and IBM-2 model probabilities (Brown et al, 1993) and an IBM-1-based feature function designed to detect whether any word in one language appears to have been left without satisfactory translation in the other language; all ofthese feature functions can be used in both language directions, i.e. source-to-target and target-to-source. In the experiments reported in the next section, the Portage system is used both as a translation and as an APE system. While we can think of a number of modifications to such a system to better adapt it to the post-editing task (some of which are discussed later on), we have done no such modifications to the system. In fact, whether the system is used for translation or post-editing, we have used exactly the same translation model configuration and training procedure. 4 Evaluation 4.1 Data and experimental setting The corpus described in section 2.2 is available for two language pairs: English-to-French and Frenchto-English.4 In each direction, each block is available in three versions (or slices): the original text (or source), the output of the commercial rule-based MT system (or baseline) and the final, post-edited version (or reference). In each direction (French-to-English and Englishto-French), we held out two subsets of approximately 1000 randomly picked blocks. The validation set is used for testing the impact of various highlevel choices such as pre-processing, or for obtaining preliminary results based on which we setup new experiments. The test set is used only once, in order to obtain the final experimental results reported here. The rest of the data constitutes the training set, which is split in two. We sampled a subset of 1000 blocks as train-2, which is used for optimiz4Note that, in a post-editing context, translation direction is crucially important. It is not possible to use the same corpus in both directions. 511 English-to-French French-to-English Corpus words: words: blocks source baseline reference blocks source baseline reference train-1 28577 310k 382k 410k 36005 485k 501k 456k train-2 1000 11k 14k 14k 1000 13k 14k 12k validation 881 10k 13k 13k 966 13k 14k 12k test 899 10k 12k 13k 953 13k 13k 12k Table 1: Data and split used in our experiments, (in thousand words). ?baseline? is the output of the commercial rule-based MT system and ?reference? is the final, post-edited text. ing the log-linear model parameters used for decoding and rescoring. The rest is the train-1 set, used for estimating IBM translation models, constructing phrasetables and estimating a target language model. The composition of the various sets is detailed in Table 1. All data was tokenized and lowercased; all evaluations were performed independent of case. Note that the validation and test sets were originally made out of 1000 blocks sampled randomly from the data. These sets turned out to contain blocks identical to blocks from the training sets. Considering that these would normally have been handled by the translation memory component (see the HRSDC workflow description in Section 2.1), we removed those blocks for which the source part was already found in the training set (in either train-1 or train-2), hence their smaller sizes. In order to check the sensitivity of experimental results to the choice of the train-2 set, we did a run of preliminary experiments using different subsets of 1000 blocks. The experimental results were nearly identical andhighly consistent, showing that the choice of a particular train-2 subset has no influence on our conclusions. In the experiments reported below, we therefore use a single identical train-2 set. We initially performed two sets of experiments on this data. The first was intended to compare the performance of the Portage PBMT system as an alternative to the commercial rule-based MT system on this type of data. In these experiments, Englishto-French and French-to-English translation systems were trained on the source and reference (manually post-edited target language) slices of the training set. In addition to the target language model estimated on the train-1 data, we used an external contribution, Language TER BLEU English-to-French Baseline 53.5 32.9 Portage translation 53.7 36.0 Baseline + Portage APE 47.3 41.6 French-to-English Baseline 59.3 31.2 Portage translation 43.9 41.0 Baseline + Portage APE 41.0 44.9 Table 2: Experimental Results: For TER, lower (error) is better, while for BLEU, higher (score) is better. Best results are in bold. a trigram target language model trained on a fairly large quantity of data from the Canadian Hansard. The goal of the second set of experiments was to assess the potential of the Portage technology in automatic post-editing mode. Again, we built systems for both language directions, but this time using the existing rule-based MT output as source and the reference as target. Apart from the use of different source data, the training procedure and system configurations of the translation and post-editing systems were in all points identical. 4.2 Experimental results The results of both experiments are presented in Table 2. Results are reported both in terms of the TER and BLEU metrics; Baseline refers to the commercial rule-based MT output. The first observation from these results is that, while the performance of Portage in translation mode is approximately equivalent to that of the baseline system when translating into French, its performance is much better than the baseline when translating into English. Two factors possibly contribute 512 to this result: first, the fact that the baseline system itself performs better when translating into French; second, and possibly more importantly, the fact that we had access to less training data for English-toFrench translation. The second observation is that when Portage is used in automatic post-editing mode, on top of the baseline MT system, it achieves better quality than either of the two translation systems used on its own. This appears to be true regardless of the translation direction or metric. This is an extremely interesting result, especially in light of how little data was actually available to train the post-editing system. One aspect of statistical MT systems is that, contrary to rule-based systems, their performance (usually) increases as more training data is available. In order to quantify this effect in our setting, we have computed learning curves by training the Portage translation and Portage APE systems on subsets of the training data of increasing sizes. We start with as little as 1000 blocks, which corresponds to around 10-15k words. Figure 2 (next page) compares the learning rates of the two competing approaches (Portagetranslation vs. Portage APE). Both approaches display very steady learning rates (note the logarithmic scale for training data size). These graphs strongly suggest that both systems would continue to improve given more training data. The most impressive aspect is how little data is necessary to improve upon the baseline, especially when translating into English: as little as 8000 blocks (around 100k words) for direct translation and 2000 blocks (around 25k words) for automatic post-editing. This suggests that such a post-editing setup might be worth implementing even for specialized domains with very small volumes of data. 4.3 Extensions Given the encouraging results of the Portage APE approach in the above experiments, we were curious to see whether a Portage+Portage combination might be as successful: after all, if Portage was good at correcting some other system?s output, could it not manage to correct the output of another Portage translator? We tested this in two settings. First, we actually use the output of the Portage translation sysLanguage TER BLEU English-to-French Portage Job Bank 53.7 36.0 + Portage APE 53.7 36.2 Portage Hansard 76.9 13.0 + Portage APE 64.6 26.2 French-to-English Portage Job Bank 43.9 41.0 + Portage APE 43.9 41.4 Portage Hansard 80.1 14.0 + Portage APE 57.7 28.6 Table 3: Portage translation - Portage APE system combination experimental results. tem obtained above, i.e. trained on the same data. In our second experiment, we use the output of a Portage translator trained on different domain data (the Canadian Hansard), but with much larger amounts of training material (over 85 million words per language). In both sets of experiments, the Portage APE system was trained as previously, but using Portage translations of the Job Bank data as input text. The results of both experiments are presented in Table 3. The first observation in these results is that there is nothing to be gained from post-editing when both the translation and APE systems are trained on the same data sets (Portage Job Bank + Portage APE experiments). In other words, the translation system is apparently already making the best possible use of the training data, and additional layers do not help (but nor do they hurt, interestingly). However, when the translation system has been trained using distinct data (Portage Hansard + Portage APE experiments), post-editing makes a large difference, comparable to that observed with the rule-based MT output provided with the Job Bank data. In this case, however, the Portage translation system behaves very poorly in spite of the important size of the training set for this system, much worse in fact than the ?baseline? system. This highlights the fact that both the Job Bank and Hansard data are very much domain-specific, and that access to appropriate training material is crucial for phrasebased translation technology. In this context, combining two phrase-based sys513 1000 2000 5000 10000 20000 40 45 50 55 60 TER learning curves Training set size TE R to English to French Post?edition Translation 1000 2000 5000 10000 20000 0. 30 0. 35 0. 40 0. 45 BLEU learning curves Training set size BL EUto English to French Post?edition Translation Figure 2: TER and BLEU scores of the phrase-based post-editing models as the amount of training data increases (log scale). The horizontal lines correspond to the performance of the baseline system (rule-based translation). tems as done here can be seen as a way of adapting an existing MT system to a new text domain; the APE system then acts as an ?adapter?, so to speak. Note however that, in our experiments, this setup doesn?t perform as well as a single Portage translation system, trained directly and exclusively on the Job Bank data. Such an adaptation strategy should be contrasted with one in which the translation models of the old and new domains are ?merged? to create a new translation system. As mentioned earlier, Portage allows using multiple phrase translation tables and language models concurrently. For example, in the current context, we can extract phrase tables and language models from the Job Bank data, as when training the ?Portage Job Bank? translation system, and then build a Portage translation model using both the Hansard and Job Bank model components. Loglinear model parameters are then optimized on the Job Bank data, so as to find the model weights that best fit the new domain. In a straightforward implementation of this idea, we obtained performances almost identical to those of the Portage translation system trained solely on Job Bank data. Upon closer examination of the model parameters, we observed that Hansard model components (language model, phrase tables, IBM translation models) were systematically attributed negligeable weights. Again, the amount of training material for the new domain may be critical in chosing between alternative adaptation mechanisms. 5 Conclusions and Future Work We have proposed using a phrase-based MT system to automatically post-edit the output of another MT system, and have tested this idea with the Portage MT system on the Job Bank data set, a corpus of manually post-edited French-English machine translations. In our experiments, not only does phrase-based APE significantly improve the quality of the output translations, this approach outperforms a standalone phrase-based translation system. While these results are very encouraging, the learning curves of Figure 2 suggest that the output quality of the PBMT systems increases faster than that of the APE systems as more data is used for training. So while the combination strategy clearly performs better with limited amounts of training data, there is reason to believe that, given sufficient training data, it would eventually be outperformed 514 by a direct phrase-based translation strategy. Of course, this remains to be verified empirically, something which will obviously require more data than is currently available to us. But this sort of behavior is expectable: while both types of system improve as more training data is used, inevitably some details of the source text will be lost by the front-end MT system, which the APE system will never be able to retrieve.5 Ultimately, the APE system will be weighted down by the inherent limitations of the front-end MT system. One way around this problem would be to modify the APEsystem so that it not only uses the baseline MT output, but also the source-language input. In the Portage system, this could be achieved, for example, by introducing feature functions into the log-linear model that relate target-language phrases with the source-language text. This is one research avenue that we are currently exploring. Alternatively, we could combine these two inputs differently within Portage: for example, use the source-language text as the primary input, and use the raw MT output as a secondary source. In this perspective, if we have multiple MT systems available, nothing precludes using all of them as providers of secondary inputs. In such a setting, the phrase-based system becomes a sort of combination MT system. We intend to explore such alternatives in the near future as well.Acknowledgements The work reported here was part of a collaboration between the National Research Council of Canada and the department of Human Resources and Social Development Canada. Special thanks go to Souad Benayyoub, Jean-Fre?de?ric Hu?bsch and the rest of the Job Bank team at HRSDC for preparing data that was essential to this project.'")])
INFO:tensorflow:Finished getting dataset.
I0712 12:51:23.638406 140282259928896 input_pipeline.py:91] Finished getting dataset.
I0712 12:51:23.638580 140282259928896 input_pipeline.py:94] Using char-level/byte dataset..
I0712 12:51:23.751744 140282259928896 train.py:106] Vocab Size: 257
I0712 12:51:36.952665 140282259928896 checkpoints.py:249] Found no checkpoint files in trained_models/matching/transformer with prefix checkpoint_
I0712 12:51:37.582949 140282259928896 train_utils.py:370] Starting training
I0712 12:51:37.583159 140282259928896 train_utils.py:371] ====================
I0712 12:52:06.168496 140282259928896 train_utils.py:377] train in step: 0
I0712 12:52:06.191806 140282259928896 train_utils.py:377] train in step: 1
I0712 12:52:06.240790 140282259928896 train_utils.py:377] train in step: 2
I0712 12:52:06.461934 140282259928896 train_utils.py:377] train in step: 3
I0712 12:52:06.681176 140282259928896 train_utils.py:377] train in step: 4
I0712 12:52:06.902703 140282259928896 train_utils.py:377] train in step: 5
I0712 12:52:07.125182 140282259928896 train_utils.py:377] train in step: 6
I0712 12:52:07.345227 140282259928896 train_utils.py:377] train in step: 7
I0712 12:52:07.565276 140282259928896 train_utils.py:377] train in step: 8
I0712 12:52:07.786201 140282259928896 train_utils.py:377] train in step: 9
I0712 12:52:08.007366 140282259928896 train_utils.py:377] train in step: 10
I0712 12:52:08.235139 140282259928896 train_utils.py:377] train in step: 11
I0712 12:52:08.458384 140282259928896 train_utils.py:377] train in step: 12
I0712 12:52:08.679997 140282259928896 train_utils.py:377] train in step: 13
I0712 12:52:08.904032 140282259928896 train_utils.py:377] train in step: 14
I0712 12:52:09.131189 140282259928896 train_utils.py:377] train in step: 15
I0712 12:52:09.350355 140282259928896 train_utils.py:377] train in step: 16
I0712 12:52:09.574511 140282259928896 train_utils.py:377] train in step: 17
I0712 12:52:09.798541 140282259928896 train_utils.py:377] train in step: 18
I0712 12:52:10.016299 140282259928896 train_utils.py:377] train in step: 19
I0712 12:52:10.237581 140282259928896 train_utils.py:377] train in step: 20
I0712 12:52:10.461479 140282259928896 train_utils.py:377] train in step: 21
I0712 12:52:10.683409 140282259928896 train_utils.py:377] train in step: 22
I0712 12:52:10.903891 140282259928896 train_utils.py:377] train in step: 23
I0712 12:52:11.128966 140282259928896 train_utils.py:377] train in step: 24
I0712 12:52:11.361458 140282259928896 train_utils.py:377] train in step: 25
I0712 12:52:11.590434 140282259928896 train_utils.py:377] train in step: 26
I0712 12:52:11.805811 140282259928896 train_utils.py:377] train in step: 27
I0712 12:52:12.026685 140282259928896 train_utils.py:377] train in step: 28
I0712 12:52:12.250419 140282259928896 train_utils.py:377] train in step: 29
I0712 12:52:12.469814 140282259928896 train_utils.py:377] train in step: 30
I0712 12:52:12.691428 140282259928896 train_utils.py:377] train in step: 31
I0712 12:52:12.913868 140282259928896 train_utils.py:377] train in step: 32
I0712 12:52:13.144285 140282259928896 train_utils.py:377] train in step: 33
I0712 12:52:13.364317 140282259928896 train_utils.py:377] train in step: 34
I0712 12:52:13.585888 140282259928896 train_utils.py:377] train in step: 35
I0712 12:52:13.811771 140282259928896 train_utils.py:377] train in step: 36
I0712 12:52:14.030376 140282259928896 train_utils.py:377] train in step: 37
I0712 12:52:14.253427 140282259928896 train_utils.py:377] train in step: 38
I0712 12:52:14.475803 140282259928896 train_utils.py:377] train in step: 39
I0712 12:52:14.698879 140282259928896 train_utils.py:377] train in step: 40
I0712 12:52:14.920160 140282259928896 train_utils.py:377] train in step: 41
I0712 12:52:15.141850 140282259928896 train_utils.py:377] train in step: 42
I0712 12:52:15.365206 140282259928896 train_utils.py:377] train in step: 43
I0712 12:52:15.587425 140282259928896 train_utils.py:377] train in step: 44
I0712 12:52:15.809370 140282259928896 train_utils.py:377] train in step: 45
I0712 12:52:16.032281 140282259928896 train_utils.py:377] train in step: 46
I0712 12:52:16.253775 140282259928896 train_utils.py:377] train in step: 47
I0712 12:52:16.474857 140282259928896 train_utils.py:377] train in step: 48
I0712 12:52:16.702355 140282259928896 train_utils.py:377] train in step: 49
I0712 12:52:16.926322 140282259928896 train_utils.py:377] train in step: 50
I0712 12:52:17.148491 140282259928896 train_utils.py:377] train in step: 51
I0712 12:52:17.373998 140282259928896 train_utils.py:377] train in step: 52
I0712 12:52:17.596581 140282259928896 train_utils.py:377] train in step: 53
I0712 12:52:17.814916 140282259928896 train_utils.py:377] train in step: 54
I0712 12:52:18.041251 140282259928896 train_utils.py:377] train in step: 55
I0712 12:52:18.262656 140282259928896 train_utils.py:377] train in step: 56
I0712 12:52:18.485574 140282259928896 train_utils.py:377] train in step: 57
I0712 12:52:18.706880 140282259928896 train_utils.py:377] train in step: 58
I0712 12:52:18.932968 140282259928896 train_utils.py:377] train in step: 59
I0712 12:52:19.151103 140282259928896 train_utils.py:377] train in step: 60
I0712 12:52:19.371607 140282259928896 train_utils.py:377] train in step: 61
I0712 12:52:19.596434 140282259928896 train_utils.py:377] train in step: 62
I0712 12:52:19.818478 140282259928896 train_utils.py:377] train in step: 63
I0712 12:52:20.039731 140282259928896 train_utils.py:377] train in step: 64
I0712 12:52:20.264158 140282259928896 train_utils.py:377] train in step: 65
I0712 12:52:20.486417 140282259928896 train_utils.py:377] train in step: 66
I0712 12:52:20.707798 140282259928896 train_utils.py:377] train in step: 67
I0712 12:52:20.930642 140282259928896 train_utils.py:377] train in step: 68
I0712 12:52:21.152944 140282259928896 train_utils.py:377] train in step: 69
I0712 12:52:21.374026 140282259928896 train_utils.py:377] train in step: 70
I0712 12:52:21.595801 140282259928896 train_utils.py:377] train in step: 71
I0712 12:52:21.818715 140282259928896 train_utils.py:377] train in step: 72
I0712 12:52:22.040395 140282259928896 train_utils.py:377] train in step: 73
I0712 12:52:22.263184 140282259928896 train_utils.py:377] train in step: 74
I0712 12:52:22.484201 140282259928896 train_utils.py:377] train in step: 75
I0712 12:52:22.707470 140282259928896 train_utils.py:377] train in step: 76
I0712 12:52:22.928226 140282259928896 train_utils.py:377] train in step: 77
I0712 12:52:23.150655 140282259928896 train_utils.py:377] train in step: 78
I0712 12:52:23.382758 140282259928896 train_utils.py:377] train in step: 79
I0712 12:52:23.597375 140282259928896 train_utils.py:377] train in step: 80
I0712 12:52:23.819072 140282259928896 train_utils.py:377] train in step: 81
I0712 12:52:24.042597 140282259928896 train_utils.py:377] train in step: 82
I0712 12:52:24.263362 140282259928896 train_utils.py:377] train in step: 83
I0712 12:52:24.485600 140282259928896 train_utils.py:377] train in step: 84
I0712 12:52:24.707824 140282259928896 train_utils.py:377] train in step: 85
I0712 12:52:24.929212 140282259928896 train_utils.py:377] train in step: 86
I0712 12:52:25.152289 140282259928896 train_utils.py:377] train in step: 87
I0712 12:52:25.380518 140282259928896 train_utils.py:377] train in step: 88
I0712 12:52:25.601017 140282259928896 train_utils.py:377] train in step: 89
I0712 12:52:25.823886 140282259928896 train_utils.py:377] train in step: 90
I0712 12:52:26.045378 140282259928896 train_utils.py:377] train in step: 91
I0712 12:52:26.270692 140282259928896 train_utils.py:377] train in step: 92
I0712 12:52:26.489705 140282259928896 train_utils.py:377] train in step: 93
I0712 12:52:26.711671 140282259928896 train_utils.py:377] train in step: 94
I0712 12:52:26.933616 140282259928896 train_utils.py:377] train in step: 95
I0712 12:52:27.155370 140282259928896 train_utils.py:377] train in step: 96
I0712 12:52:27.377568 140282259928896 train_utils.py:377] train in step: 97
I0712 12:52:27.599831 140282259928896 train_utils.py:377] train in step: 98
I0712 12:52:27.820708 140282259928896 train_utils.py:377] train in step: 99
I0712 12:52:28.044408 140282259928896 train_utils.py:377] train in step: 100
I0712 12:52:28.265459 140282259928896 train_utils.py:377] train in step: 101
I0712 12:52:28.486578 140282259928896 train_utils.py:377] train in step: 102
I0712 12:52:28.708520 140282259928896 train_utils.py:377] train in step: 103
I0712 12:52:28.933238 140282259928896 train_utils.py:377] train in step: 104
I0712 12:52:29.152821 140282259928896 train_utils.py:377] train in step: 105
I0712 12:52:29.374591 140282259928896 train_utils.py:377] train in step: 106
I0712 12:52:29.599848 140282259928896 train_utils.py:377] train in step: 107
I0712 12:52:29.823731 140282259928896 train_utils.py:377] train in step: 108
I0712 12:52:30.042611 140282259928896 train_utils.py:377] train in step: 109
I0712 12:52:30.272193 140282259928896 train_utils.py:377] train in step: 110
I0712 12:52:30.488872 140282259928896 train_utils.py:377] train in step: 111
I0712 12:52:30.710096 140282259928896 train_utils.py:377] train in step: 112
I0712 12:52:30.932072 140282259928896 train_utils.py:377] train in step: 113
I0712 12:52:31.153910 140282259928896 train_utils.py:377] train in step: 114
I0712 12:52:31.376604 140282259928896 train_utils.py:377] train in step: 115
I0712 12:52:31.597370 140282259928896 train_utils.py:377] train in step: 116
I0712 12:52:31.819946 140282259928896 train_utils.py:377] train in step: 117
I0712 12:52:32.041681 140282259928896 train_utils.py:377] train in step: 118
I0712 12:52:32.262739 140282259928896 train_utils.py:377] train in step: 119
I0712 12:52:32.484501 140282259928896 train_utils.py:377] train in step: 120
I0712 12:52:32.706433 140282259928896 train_utils.py:377] train in step: 121
I0712 12:52:32.929321 140282259928896 train_utils.py:377] train in step: 122
I0712 12:52:33.152731 140282259928896 train_utils.py:377] train in step: 123
I0712 12:52:33.376944 140282259928896 train_utils.py:377] train in step: 124
I0712 12:52:33.597007 140282259928896 train_utils.py:377] train in step: 125
I0712 12:52:33.826438 140282259928896 train_utils.py:377] train in step: 126
I0712 12:52:34.050572 140282259928896 train_utils.py:377] train in step: 127
I0712 12:52:34.269155 140282259928896 train_utils.py:377] train in step: 128
I0712 12:52:34.504936 140282259928896 train_utils.py:377] train in step: 129
I0712 12:52:34.728810 140282259928896 train_utils.py:377] train in step: 130
I0712 12:52:34.947790 140282259928896 train_utils.py:377] train in step: 131
I0712 12:52:35.170126 140282259928896 train_utils.py:377] train in step: 132
I0712 12:52:35.391646 140282259928896 train_utils.py:377] train in step: 133
I0712 12:52:35.613210 140282259928896 train_utils.py:377] train in step: 134
I0712 12:52:35.835424 140282259928896 train_utils.py:377] train in step: 135
I0712 12:52:36.056805 140282259928896 train_utils.py:377] train in step: 136
I0712 12:52:36.278790 140282259928896 train_utils.py:377] train in step: 137
I0712 12:52:36.500594 140282259928896 train_utils.py:377] train in step: 138
I0712 12:52:36.722528 140282259928896 train_utils.py:377] train in step: 139
I0712 12:52:36.943950 140282259928896 train_utils.py:377] train in step: 140
I0712 12:52:37.166374 140282259928896 train_utils.py:377] train in step: 141
I0712 12:52:37.388649 140282259928896 train_utils.py:377] train in step: 142
I0712 12:52:37.609070 140282259928896 train_utils.py:377] train in step: 143
I0712 12:52:37.831229 140282259928896 train_utils.py:377] train in step: 144
I0712 12:52:38.055965 140282259928896 train_utils.py:377] train in step: 145
I0712 12:52:38.278225 140282259928896 train_utils.py:377] train in step: 146
I0712 12:52:38.499540 140282259928896 train_utils.py:377] train in step: 147
I0712 12:52:38.721764 140282259928896 train_utils.py:377] train in step: 148
I0712 12:52:38.943236 140282259928896 train_utils.py:377] train in step: 149
I0712 12:52:39.165258 140282259928896 train_utils.py:377] train in step: 150
I0712 12:52:39.395260 140282259928896 train_utils.py:377] train in step: 151
I0712 12:52:39.619897 140282259928896 train_utils.py:377] train in step: 152
I0712 12:52:39.839485 140282259928896 train_utils.py:377] train in step: 153
I0712 12:52:40.071416 140282259928896 train_utils.py:377] train in step: 154
I0712 12:52:40.292334 140282259928896 train_utils.py:377] train in step: 155
I0712 12:52:40.516827 140282259928896 train_utils.py:377] train in step: 156
I0712 12:52:40.736922 140282259928896 train_utils.py:377] train in step: 157
I0712 12:52:40.958554 140282259928896 train_utils.py:377] train in step: 158
I0712 12:52:41.180966 140282259928896 train_utils.py:377] train in step: 159
I0712 12:52:41.405221 140282259928896 train_utils.py:377] train in step: 160
I0712 12:52:41.625773 140282259928896 train_utils.py:377] train in step: 161
I0712 12:52:41.848320 140282259928896 train_utils.py:377] train in step: 162
I0712 12:52:42.071567 140282259928896 train_utils.py:377] train in step: 163
I0712 12:52:42.292245 140282259928896 train_utils.py:377] train in step: 164
I0712 12:52:42.513337 140282259928896 train_utils.py:377] train in step: 165
I0712 12:52:42.737727 140282259928896 train_utils.py:377] train in step: 166
I0712 12:52:42.972553 140282259928896 train_utils.py:377] train in step: 167
I0712 12:52:43.189147 140282259928896 train_utils.py:377] train in step: 168
I0712 12:52:43.411368 140282259928896 train_utils.py:377] train in step: 169
I0712 12:52:43.632738 140282259928896 train_utils.py:377] train in step: 170
I0712 12:52:43.855095 140282259928896 train_utils.py:377] train in step: 171
I0712 12:52:44.077160 140282259928896 train_utils.py:377] train in step: 172
I0712 12:52:44.298507 140282259928896 train_utils.py:377] train in step: 173
I0712 12:52:44.520375 140282259928896 train_utils.py:377] train in step: 174
I0712 12:52:44.743631 140282259928896 train_utils.py:377] train in step: 175
I0712 12:52:44.972699 140282259928896 train_utils.py:377] train in step: 176
I0712 12:52:45.194733 140282259928896 train_utils.py:377] train in step: 177
I0712 12:52:45.415928 140282259928896 train_utils.py:377] train in step: 178
I0712 12:52:45.645657 140282259928896 train_utils.py:377] train in step: 179
I0712 12:52:45.868961 140282259928896 train_utils.py:377] train in step: 180
I0712 12:52:46.090939 140282259928896 train_utils.py:377] train in step: 181
I0712 12:52:46.322272 140282259928896 train_utils.py:377] train in step: 182
I0712 12:52:46.544073 140282259928896 train_utils.py:377] train in step: 183
I0712 12:52:46.766734 140282259928896 train_utils.py:377] train in step: 184
I0712 12:52:47.004811 140282259928896 train_utils.py:377] train in step: 185
I0712 12:52:47.224506 140282259928896 train_utils.py:377] train in step: 186
I0712 12:52:47.446734 140282259928896 train_utils.py:377] train in step: 187
I0712 12:52:47.668439 140282259928896 train_utils.py:377] train in step: 188
I0712 12:52:47.891909 140282259928896 train_utils.py:377] train in step: 189
I0712 12:52:48.111739 140282259928896 train_utils.py:377] train in step: 190
I0712 12:52:48.333727 140282259928896 train_utils.py:377] train in step: 191
I0712 12:52:48.555629 140282259928896 train_utils.py:377] train in step: 192
I0712 12:52:48.776782 140282259928896 train_utils.py:377] train in step: 193
I0712 12:52:48.999400 140282259928896 train_utils.py:377] train in step: 194
I0712 12:52:49.222285 140282259928896 train_utils.py:377] train in step: 195
I0712 12:52:49.443681 140282259928896 train_utils.py:377] train in step: 196
I0712 12:52:49.665364 140282259928896 train_utils.py:377] train in step: 197
I0712 12:52:49.893383 140282259928896 train_utils.py:377] train in step: 198
I0712 12:52:50.115390 140282259928896 train_utils.py:377] train in step: 199
I0712 12:52:50.337183 140282259928896 train_utils.py:377] train in step: 200
I0712 12:52:51.271871 140282259928896 train_utils.py:396] train in step: 200, loss: 0.7423999905586243, acc: 0.4551999866962433
I0712 12:53:04.043000 140282259928896 train_utils.py:411] eval in step: 200, loss: 0.6892, acc: 0.5500
I0712 12:53:04.048568 140282259928896 train_utils.py:421] Testing...
I0712 12:53:12.346689 140282259928896 train_utils.py:424] test in step: 200, loss: 0.6892, acc: 0.5800
I0712 12:53:12.380998 140282259928896 train_utils.py:377] train in step: 201
I0712 12:53:12.426122 140282259928896 train_utils.py:377] train in step: 202
I0712 12:53:12.646687 140282259928896 train_utils.py:377] train in step: 203
I0712 12:53:12.867774 140282259928896 train_utils.py:377] train in step: 204
I0712 12:53:13.090206 140282259928896 train_utils.py:377] train in step: 205
I0712 12:53:13.311780 140282259928896 train_utils.py:377] train in step: 206
I0712 12:53:13.534282 140282259928896 train_utils.py:377] train in step: 207
I0712 12:53:13.757147 140282259928896 train_utils.py:377] train in step: 208
I0712 12:53:13.978986 140282259928896 train_utils.py:377] train in step: 209
I0712 12:53:14.200395 140282259928896 train_utils.py:377] train in step: 210
I0712 12:53:14.422944 140282259928896 train_utils.py:377] train in step: 211
I0712 12:53:14.645829 140282259928896 train_utils.py:377] train in step: 212
I0712 12:53:14.867479 140282259928896 train_utils.py:377] train in step: 213
I0712 12:53:15.089075 140282259928896 train_utils.py:377] train in step: 214
I0712 12:53:15.313706 140282259928896 train_utils.py:377] train in step: 215
I0712 12:53:15.536344 140282259928896 train_utils.py:377] train in step: 216
I0712 12:53:15.757484 140282259928896 train_utils.py:377] train in step: 217
I0712 12:53:15.982172 140282259928896 train_utils.py:377] train in step: 218
I0712 12:53:16.203502 140282259928896 train_utils.py:377] train in step: 219
I0712 12:53:16.425096 140282259928896 train_utils.py:377] train in step: 220
I0712 12:53:16.646849 140282259928896 train_utils.py:377] train in step: 221
I0712 12:53:16.869185 140282259928896 train_utils.py:377] train in step: 222
I0712 12:53:17.092035 140282259928896 train_utils.py:377] train in step: 223
I0712 12:53:17.314066 140282259928896 train_utils.py:377] train in step: 224
I0712 12:53:17.550969 140282259928896 train_utils.py:377] train in step: 225
I0712 12:53:17.773250 140282259928896 train_utils.py:377] train in step: 226
I0712 12:53:17.993715 140282259928896 train_utils.py:377] train in step: 227
I0712 12:53:18.218215 140282259928896 train_utils.py:377] train in step: 228
I0712 12:53:18.439259 140282259928896 train_utils.py:377] train in step: 229
I0712 12:53:18.660354 140282259928896 train_utils.py:377] train in step: 230
I0712 12:53:18.882926 140282259928896 train_utils.py:377] train in step: 231
I0712 12:53:19.104928 140282259928896 train_utils.py:377] train in step: 232
I0712 12:53:19.326340 140282259928896 train_utils.py:377] train in step: 233
I0712 12:53:19.547932 140282259928896 train_utils.py:377] train in step: 234
I0712 12:53:19.776617 140282259928896 train_utils.py:377] train in step: 235
I0712 12:53:19.999070 140282259928896 train_utils.py:377] train in step: 236
I0712 12:53:20.220463 140282259928896 train_utils.py:377] train in step: 237
I0712 12:53:20.442616 140282259928896 train_utils.py:377] train in step: 238
I0712 12:53:20.663949 140282259928896 train_utils.py:377] train in step: 239
I0712 12:53:20.886265 140282259928896 train_utils.py:377] train in step: 240
I0712 12:53:21.117045 140282259928896 train_utils.py:377] train in step: 241
I0712 12:53:21.338721 140282259928896 train_utils.py:377] train in step: 242
I0712 12:53:21.561032 140282259928896 train_utils.py:377] train in step: 243
I0712 12:53:21.784032 140282259928896 train_utils.py:377] train in step: 244
I0712 12:53:22.005674 140282259928896 train_utils.py:377] train in step: 245
I0712 12:53:22.227038 140282259928896 train_utils.py:377] train in step: 246
I0712 12:53:22.449636 140282259928896 train_utils.py:377] train in step: 247
I0712 12:53:22.671088 140282259928896 train_utils.py:377] train in step: 248
I0712 12:53:22.893145 140282259928896 train_utils.py:377] train in step: 249
I0712 12:53:23.115651 140282259928896 train_utils.py:377] train in step: 250
I0712 12:53:23.350441 140282259928896 train_utils.py:377] train in step: 251
I0712 12:53:23.565160 140282259928896 train_utils.py:377] train in step: 252
I0712 12:53:23.786966 140282259928896 train_utils.py:377] train in step: 253
I0712 12:53:24.010269 140282259928896 train_utils.py:377] train in step: 254
I0712 12:53:24.232078 140282259928896 train_utils.py:377] train in step: 255
I0712 12:53:24.452999 140282259928896 train_utils.py:377] train in step: 256
I0712 12:53:24.678933 140282259928896 train_utils.py:377] train in step: 257
I0712 12:53:24.899335 140282259928896 train_utils.py:377] train in step: 258
I0712 12:53:25.122988 140282259928896 train_utils.py:377] train in step: 259
I0712 12:53:25.345026 140282259928896 train_utils.py:377] train in step: 260
I0712 12:53:25.568175 140282259928896 train_utils.py:377] train in step: 261
I0712 12:53:25.792213 140282259928896 train_utils.py:377] train in step: 262
I0712 12:53:26.011529 140282259928896 train_utils.py:377] train in step: 263
I0712 12:53:26.233776 140282259928896 train_utils.py:377] train in step: 264
I0712 12:53:26.455707 140282259928896 train_utils.py:377] train in step: 265
I0712 12:53:26.678417 140282259928896 train_utils.py:377] train in step: 266
I0712 12:53:26.900248 140282259928896 train_utils.py:377] train in step: 267
I0712 12:53:27.122679 140282259928896 train_utils.py:377] train in step: 268
I0712 12:53:27.344905 140282259928896 train_utils.py:377] train in step: 269
I0712 12:53:27.566525 140282259928896 train_utils.py:377] train in step: 270
I0712 12:53:27.788809 140282259928896 train_utils.py:377] train in step: 271
I0712 12:53:28.011904 140282259928896 train_utils.py:377] train in step: 272
I0712 12:53:28.234722 140282259928896 train_utils.py:377] train in step: 273
I0712 12:53:28.456768 140282259928896 train_utils.py:377] train in step: 274
I0712 12:53:28.679086 140282259928896 train_utils.py:377] train in step: 275
I0712 12:53:28.900876 140282259928896 train_utils.py:377] train in step: 276
I0712 12:53:29.123466 140282259928896 train_utils.py:377] train in step: 277
I0712 12:53:29.345255 140282259928896 train_utils.py:377] train in step: 278
I0712 12:53:29.567154 140282259928896 train_utils.py:377] train in step: 279
I0712 12:53:29.789715 140282259928896 train_utils.py:377] train in step: 280
I0712 12:53:30.011585 140282259928896 train_utils.py:377] train in step: 281
I0712 12:53:30.233341 140282259928896 train_utils.py:377] train in step: 282
I0712 12:53:30.454591 140282259928896 train_utils.py:377] train in step: 283
I0712 12:53:30.676475 140282259928896 train_utils.py:377] train in step: 284
I0712 12:53:30.903712 140282259928896 train_utils.py:377] train in step: 285
I0712 12:53:31.125200 140282259928896 train_utils.py:377] train in step: 286
I0712 12:53:31.345730 140282259928896 train_utils.py:377] train in step: 287
I0712 12:53:31.580886 140282259928896 train_utils.py:377] train in step: 288
I0712 12:53:31.800732 140282259928896 train_utils.py:377] train in step: 289
I0712 12:53:32.023732 140282259928896 train_utils.py:377] train in step: 290
I0712 12:53:32.247102 140282259928896 train_utils.py:377] train in step: 291
I0712 12:53:32.469089 140282259928896 train_utils.py:377] train in step: 292
I0712 12:53:32.690176 140282259928896 train_utils.py:377] train in step: 293
I0712 12:53:32.913303 140282259928896 train_utils.py:377] train in step: 294
I0712 12:53:33.138643 140282259928896 train_utils.py:377] train in step: 295
I0712 12:53:33.356906 140282259928896 train_utils.py:377] train in step: 296
I0712 12:53:33.578943 140282259928896 train_utils.py:377] train in step: 297
I0712 12:53:33.801659 140282259928896 train_utils.py:377] train in step: 298
I0712 12:53:34.024050 140282259928896 train_utils.py:377] train in step: 299
I0712 12:53:34.248785 140282259928896 train_utils.py:377] train in step: 300
I0712 12:53:34.476169 140282259928896 train_utils.py:377] train in step: 301
I0712 12:53:34.713629 140282259928896 train_utils.py:377] train in step: 302
I0712 12:53:34.937182 140282259928896 train_utils.py:377] train in step: 303
I0712 12:53:35.158344 140282259928896 train_utils.py:377] train in step: 304
I0712 12:53:35.380033 140282259928896 train_utils.py:377] train in step: 305
I0712 12:53:35.601781 140282259928896 train_utils.py:377] train in step: 306
I0712 12:53:35.830101 140282259928896 train_utils.py:377] train in step: 307
I0712 12:53:36.052102 140282259928896 train_utils.py:377] train in step: 308
I0712 12:53:36.278576 140282259928896 train_utils.py:377] train in step: 309
I0712 12:53:36.507230 140282259928896 train_utils.py:377] train in step: 310
I0712 12:53:36.726214 140282259928896 train_utils.py:377] train in step: 311
I0712 12:53:36.948184 140282259928896 train_utils.py:377] train in step: 312
I0712 12:53:37.170074 140282259928896 train_utils.py:377] train in step: 313
I0712 12:53:37.394503 140282259928896 train_utils.py:377] train in step: 314
I0712 12:53:37.641039 140282259928896 train_utils.py:377] train in step: 315
I0712 12:53:37.879314 140282259928896 train_utils.py:377] train in step: 316
I0712 12:53:38.104065 140282259928896 train_utils.py:377] train in step: 317
I0712 12:53:38.338499 140282259928896 train_utils.py:377] train in step: 318
I0712 12:53:38.646976 140282259928896 train_utils.py:377] train in step: 319
I0712 12:53:38.946316 140282259928896 train_utils.py:377] train in step: 320
I0712 12:53:39.198025 140282259928896 train_utils.py:377] train in step: 321
I0712 12:53:39.432014 140282259928896 train_utils.py:377] train in step: 322
I0712 12:53:39.654891 140282259928896 train_utils.py:377] train in step: 323
I0712 12:53:39.886807 140282259928896 train_utils.py:377] train in step: 324
I0712 12:53:40.113699 140282259928896 train_utils.py:377] train in step: 325
I0712 12:53:40.340783 140282259928896 train_utils.py:377] train in step: 326
I0712 12:53:40.564578 140282259928896 train_utils.py:377] train in step: 327
I0712 12:53:40.800272 140282259928896 train_utils.py:377] train in step: 328
I0712 12:53:41.030109 140282259928896 train_utils.py:377] train in step: 329
I0712 12:53:41.260219 140282259928896 train_utils.py:377] train in step: 330
I0712 12:53:41.496487 140282259928896 train_utils.py:377] train in step: 331
I0712 12:53:41.728384 140282259928896 train_utils.py:377] train in step: 332
I0712 12:53:41.969527 140282259928896 train_utils.py:377] train in step: 333
I0712 12:53:42.198739 140282259928896 train_utils.py:377] train in step: 334
I0712 12:53:42.437433 140282259928896 train_utils.py:377] train in step: 335
I0712 12:53:42.695015 140282259928896 train_utils.py:377] train in step: 336
I0712 12:53:42.959961 140282259928896 train_utils.py:377] train in step: 337
I0712 12:53:43.190848 140282259928896 train_utils.py:377] train in step: 338
I0712 12:53:43.431295 140282259928896 train_utils.py:377] train in step: 339
I0712 12:53:43.695895 140282259928896 train_utils.py:377] train in step: 340
I0712 12:53:44.043922 140282259928896 train_utils.py:377] train in step: 341
I0712 12:53:44.329854 140282259928896 train_utils.py:377] train in step: 342
I0712 12:53:44.574667 140282259928896 train_utils.py:377] train in step: 343
I0712 12:53:44.799850 140282259928896 train_utils.py:377] train in step: 344
I0712 12:53:45.021838 140282259928896 train_utils.py:377] train in step: 345
I0712 12:53:45.300122 140282259928896 train_utils.py:377] train in step: 346
I0712 12:53:45.560636 140282259928896 train_utils.py:377] train in step: 347
I0712 12:53:45.789802 140282259928896 train_utils.py:377] train in step: 348
I0712 12:53:46.020588 140282259928896 train_utils.py:377] train in step: 349
I0712 12:53:46.279873 140282259928896 train_utils.py:377] train in step: 350
I0712 12:53:46.526450 140282259928896 train_utils.py:377] train in step: 351
I0712 12:53:46.783920 140282259928896 train_utils.py:377] train in step: 352
I0712 12:53:47.027019 140282259928896 train_utils.py:377] train in step: 353
I0712 12:53:47.267279 140282259928896 train_utils.py:377] train in step: 354
I0712 12:53:47.516929 140282259928896 train_utils.py:377] train in step: 355
I0712 12:53:47.831017 140282259928896 train_utils.py:377] train in step: 356
I0712 12:53:48.142298 140282259928896 train_utils.py:377] train in step: 357
I0712 12:53:48.413960 140282259928896 train_utils.py:377] train in step: 358
I0712 12:53:48.655730 140282259928896 train_utils.py:377] train in step: 359
I0712 12:53:48.877525 140282259928896 train_utils.py:377] train in step: 360
I0712 12:53:49.104192 140282259928896 train_utils.py:377] train in step: 361
I0712 12:53:49.360380 140282259928896 train_utils.py:377] train in step: 362
I0712 12:53:49.645149 140282259928896 train_utils.py:377] train in step: 363
I0712 12:53:49.944441 140282259928896 train_utils.py:377] train in step: 364
I0712 12:53:50.217146 140282259928896 train_utils.py:377] train in step: 365
I0712 12:53:50.459037 140282259928896 train_utils.py:377] train in step: 366
I0712 12:53:50.685050 140282259928896 train_utils.py:377] train in step: 367
I0712 12:53:50.908292 140282259928896 train_utils.py:377] train in step: 368
I0712 12:53:51.138812 140282259928896 train_utils.py:377] train in step: 369
I0712 12:53:51.414531 140282259928896 train_utils.py:377] train in step: 370
I0712 12:53:51.717062 140282259928896 train_utils.py:377] train in step: 371
I0712 12:53:51.974046 140282259928896 train_utils.py:377] train in step: 372
I0712 12:53:52.262362 140282259928896 train_utils.py:377] train in step: 373
I0712 12:53:52.517804 140282259928896 train_utils.py:377] train in step: 374
I0712 12:53:52.771633 140282259928896 train_utils.py:377] train in step: 375
I0712 12:53:53.004870 140282259928896 train_utils.py:377] train in step: 376
I0712 12:53:53.313153 140282259928896 train_utils.py:377] train in step: 377
I0712 12:53:53.627929 140282259928896 train_utils.py:377] train in step: 378
I0712 12:53:53.908034 140282259928896 train_utils.py:377] train in step: 379
I0712 12:53:54.156735 140282259928896 train_utils.py:377] train in step: 380
I0712 12:53:54.407559 140282259928896 train_utils.py:377] train in step: 381
I0712 12:53:54.644796 140282259928896 train_utils.py:377] train in step: 382
I0712 12:53:54.916250 140282259928896 train_utils.py:377] train in step: 383
I0712 12:53:55.190874 140282259928896 train_utils.py:377] train in step: 384
I0712 12:53:55.447264 140282259928896 train_utils.py:377] train in step: 385
I0712 12:53:55.704790 140282259928896 train_utils.py:377] train in step: 386
I0712 12:53:55.956228 140282259928896 train_utils.py:377] train in step: 387
I0712 12:53:56.179396 140282259928896 train_utils.py:377] train in step: 388
I0712 12:53:56.503373 140282259928896 train_utils.py:377] train in step: 389
I0712 12:53:56.858211 140282259928896 train_utils.py:377] train in step: 390
I0712 12:53:57.256680 140282259928896 train_utils.py:377] train in step: 391
I0712 12:53:57.866822 140282259928896 train_utils.py:377] train in step: 392
I0712 12:53:58.480406 140282259928896 train_utils.py:377] train in step: 393
I0712 12:53:59.092576 140282259928896 train_utils.py:377] train in step: 394
I0712 12:53:59.731284 140282259928896 train_utils.py:377] train in step: 395
I0712 12:54:00.343388 140282259928896 train_utils.py:377] train in step: 396
I0712 12:54:00.950705 140282259928896 train_utils.py:377] train in step: 397
I0712 12:54:01.563869 140282259928896 train_utils.py:377] train in step: 398
I0712 12:54:02.174960 140282259928896 train_utils.py:377] train in step: 399
I0712 12:54:02.785759 140282259928896 train_utils.py:377] train in step: 400
I0712 12:54:04.034338 140282259928896 train_utils.py:396] train in step: 400, loss: 0.7181000113487244, acc: 0.550000011920929
I0712 12:54:12.206436 140282259928896 train_utils.py:411] eval in step: 400, loss: 0.7134, acc: 0.5550
I0712 12:54:12.209300 140282259928896 train_utils.py:421] Testing...
I0712 12:54:20.398680 140282259928896 train_utils.py:424] test in step: 400, loss: 0.7585, acc: 0.4600
I0712 12:54:20.440829 140282259928896 train_utils.py:377] train in step: 401
I0712 12:54:20.481053 140282259928896 train_utils.py:377] train in step: 402
I0712 12:54:20.701861 140282259928896 train_utils.py:377] train in step: 403
I0712 12:54:20.923670 140282259928896 train_utils.py:377] train in step: 404
I0712 12:54:21.145345 140282259928896 train_utils.py:377] train in step: 405
I0712 12:54:21.368889 140282259928896 train_utils.py:377] train in step: 406
I0712 12:54:21.589970 140282259928896 train_utils.py:377] train in step: 407
I0712 12:54:21.824307 140282259928896 train_utils.py:377] train in step: 408
I0712 12:54:22.045102 140282259928896 train_utils.py:377] train in step: 409
I0712 12:54:22.266444 140282259928896 train_utils.py:377] train in step: 410
I0712 12:54:22.496961 140282259928896 train_utils.py:377] train in step: 411
I0712 12:54:22.717498 140282259928896 train_utils.py:377] train in step: 412
I0712 12:54:22.941901 140282259928896 train_utils.py:377] train in step: 413
I0712 12:54:23.164747 140282259928896 train_utils.py:377] train in step: 414
I0712 12:54:23.387047 140282259928896 train_utils.py:377] train in step: 415
I0712 12:54:23.604684 140282259928896 train_utils.py:377] train in step: 416
I0712 12:54:23.827062 140282259928896 train_utils.py:377] train in step: 417
I0712 12:54:24.047890 140282259928896 train_utils.py:377] train in step: 418
I0712 12:54:24.270275 140282259928896 train_utils.py:377] train in step: 419
I0712 12:54:24.491867 140282259928896 train_utils.py:377] train in step: 420
I0712 12:54:24.747779 140282259928896 train_utils.py:377] train in step: 421
I0712 12:54:25.041053 140282259928896 train_utils.py:377] train in step: 422
I0712 12:54:25.315958 140282259928896 train_utils.py:377] train in step: 423
I0712 12:54:25.939889 140282259928896 train_utils.py:377] train in step: 424
I0712 12:54:26.602920 140282259928896 train_utils.py:377] train in step: 425
I0712 12:54:27.219690 140282259928896 train_utils.py:377] train in step: 426
I0712 12:54:27.832402 140282259928896 train_utils.py:377] train in step: 427
I0712 12:54:28.450501 140282259928896 train_utils.py:377] train in step: 428
I0712 12:54:29.064340 140282259928896 train_utils.py:377] train in step: 429
I0712 12:54:29.674540 140282259928896 train_utils.py:377] train in step: 430
I0712 12:54:30.287237 140282259928896 train_utils.py:377] train in step: 431
I0712 12:54:30.903744 140282259928896 train_utils.py:377] train in step: 432
I0712 12:54:31.521133 140282259928896 train_utils.py:377] train in step: 433
I0712 12:54:32.136710 140282259928896 train_utils.py:377] train in step: 434
I0712 12:54:32.749516 140282259928896 train_utils.py:377] train in step: 435
I0712 12:54:33.366909 140282259928896 train_utils.py:377] train in step: 436
I0712 12:54:33.980210 140282259928896 train_utils.py:377] train in step: 437
I0712 12:54:34.596138 140282259928896 train_utils.py:377] train in step: 438
I0712 12:54:35.224054 140282259928896 train_utils.py:377] train in step: 439
I0712 12:54:35.573596 140282259928896 train_utils.py:377] train in step: 440
I0712 12:54:35.796011 140282259928896 train_utils.py:377] train in step: 441
I0712 12:54:36.018933 140282259928896 train_utils.py:377] train in step: 442
I0712 12:54:36.238774 140282259928896 train_utils.py:377] train in step: 443
I0712 12:54:36.460524 140282259928896 train_utils.py:377] train in step: 444
I0712 12:54:36.686351 140282259928896 train_utils.py:377] train in step: 445
I0712 12:54:36.911328 140282259928896 train_utils.py:377] train in step: 446
I0712 12:54:37.130386 140282259928896 train_utils.py:377] train in step: 447
I0712 12:54:37.352006 140282259928896 train_utils.py:377] train in step: 448
I0712 12:54:37.573459 140282259928896 train_utils.py:377] train in step: 449
I0712 12:54:37.796921 140282259928896 train_utils.py:377] train in step: 450
I0712 12:54:38.023973 140282259928896 train_utils.py:377] train in step: 451
I0712 12:54:38.246153 140282259928896 train_utils.py:377] train in step: 452
I0712 12:54:38.465793 140282259928896 train_utils.py:377] train in step: 453
I0712 12:54:38.689173 140282259928896 train_utils.py:377] train in step: 454
I0712 12:54:38.910521 140282259928896 train_utils.py:377] train in step: 455
I0712 12:54:39.132148 140282259928896 train_utils.py:377] train in step: 456
I0712 12:54:39.354194 140282259928896 train_utils.py:377] train in step: 457
I0712 12:54:39.575587 140282259928896 train_utils.py:377] train in step: 458
I0712 12:54:39.797531 140282259928896 train_utils.py:377] train in step: 459
I0712 12:54:40.020167 140282259928896 train_utils.py:377] train in step: 460
I0712 12:54:40.248445 140282259928896 train_utils.py:377] train in step: 461
I0712 12:54:40.501823 140282259928896 train_utils.py:377] train in step: 462
I0712 12:54:40.734107 140282259928896 train_utils.py:377] train in step: 463
I0712 12:54:40.961839 140282259928896 train_utils.py:377] train in step: 464
I0712 12:54:41.187229 140282259928896 train_utils.py:377] train in step: 465
I0712 12:54:41.416953 140282259928896 train_utils.py:377] train in step: 466
I0712 12:54:41.639264 140282259928896 train_utils.py:377] train in step: 467
I0712 12:54:41.861200 140282259928896 train_utils.py:377] train in step: 468
I0712 12:54:42.083083 140282259928896 train_utils.py:377] train in step: 469
I0712 12:54:42.304661 140282259928896 train_utils.py:377] train in step: 470
I0712 12:54:42.525507 140282259928896 train_utils.py:377] train in step: 471
I0712 12:54:42.760872 140282259928896 train_utils.py:377] train in step: 472
I0712 12:54:43.019477 140282259928896 train_utils.py:377] train in step: 473
I0712 12:54:43.256021 140282259928896 train_utils.py:377] train in step: 474
I0712 12:54:43.476894 140282259928896 train_utils.py:377] train in step: 475
I0712 12:54:43.698342 140282259928896 train_utils.py:377] train in step: 476
I0712 12:54:43.932526 140282259928896 train_utils.py:377] train in step: 477
I0712 12:54:44.149569 140282259928896 train_utils.py:377] train in step: 478
I0712 12:54:44.385263 140282259928896 train_utils.py:377] train in step: 479
I0712 12:54:44.695469 140282259928896 train_utils.py:377] train in step: 480
I0712 12:54:44.970920 140282259928896 train_utils.py:377] train in step: 481
I0712 12:54:45.209700 140282259928896 train_utils.py:377] train in step: 482
I0712 12:54:45.433680 140282259928896 train_utils.py:377] train in step: 483
I0712 12:54:45.656670 140282259928896 train_utils.py:377] train in step: 484
I0712 12:54:45.876754 140282259928896 train_utils.py:377] train in step: 485
I0712 12:54:46.104149 140282259928896 train_utils.py:377] train in step: 486
I0712 12:54:46.377149 140282259928896 train_utils.py:377] train in step: 487
I0712 12:54:46.706161 140282259928896 train_utils.py:377] train in step: 488
I0712 12:54:47.004773 140282259928896 train_utils.py:377] train in step: 489
I0712 12:54:47.254445 140282259928896 train_utils.py:377] train in step: 490
I0712 12:54:47.477512 140282259928896 train_utils.py:377] train in step: 491
I0712 12:54:47.705662 140282259928896 train_utils.py:377] train in step: 492
I0712 12:54:47.934492 140282259928896 train_utils.py:377] train in step: 493
I0712 12:54:48.169923 140282259928896 train_utils.py:377] train in step: 494
I0712 12:54:48.405398 140282259928896 train_utils.py:377] train in step: 495
I0712 12:54:48.632423 140282259928896 train_utils.py:377] train in step: 496
I0712 12:54:48.919606 140282259928896 train_utils.py:377] train in step: 497
I0712 12:54:49.348646 140282259928896 train_utils.py:377] train in step: 498
I0712 12:54:49.961539 140282259928896 train_utils.py:377] train in step: 499
I0712 12:54:50.573521 140282259928896 train_utils.py:377] train in step: 500
I0712 12:54:51.189699 140282259928896 train_utils.py:377] train in step: 501
I0712 12:54:51.797869 140282259928896 train_utils.py:377] train in step: 502
I0712 12:54:52.411137 140282259928896 train_utils.py:377] train in step: 503
I0712 12:54:53.028967 140282259928896 train_utils.py:377] train in step: 504
I0712 12:54:53.636311 140282259928896 train_utils.py:377] train in step: 505
I0712 12:54:54.250333 140282259928896 train_utils.py:377] train in step: 506
I0712 12:54:54.860289 140282259928896 train_utils.py:377] train in step: 507
I0712 12:54:55.474710 140282259928896 train_utils.py:377] train in step: 508
I0712 12:54:56.085557 140282259928896 train_utils.py:377] train in step: 509
I0712 12:54:56.698489 140282259928896 train_utils.py:377] train in step: 510
I0712 12:54:57.311283 140282259928896 train_utils.py:377] train in step: 511
I0712 12:54:57.924307 140282259928896 train_utils.py:377] train in step: 512
I0712 12:54:58.537686 140282259928896 train_utils.py:377] train in step: 513
I0712 12:54:59.152765 140282259928896 train_utils.py:377] train in step: 514
I0712 12:54:59.767682 140282259928896 train_utils.py:377] train in step: 515
I0712 12:55:00.377769 140282259928896 train_utils.py:377] train in step: 516
I0712 12:55:00.991075 140282259928896 train_utils.py:377] train in step: 517
I0712 12:55:01.610032 140282259928896 train_utils.py:377] train in step: 518
I0712 12:55:02.229322 140282259928896 train_utils.py:377] train in step: 519
I0712 12:55:02.837640 140282259928896 train_utils.py:377] train in step: 520
I0712 12:55:03.452778 140282259928896 train_utils.py:377] train in step: 521
I0712 12:55:04.066055 140282259928896 train_utils.py:377] train in step: 522
I0712 12:55:04.679234 140282259928896 train_utils.py:377] train in step: 523
I0712 12:55:05.288996 140282259928896 train_utils.py:377] train in step: 524
I0712 12:55:05.902297 140282259928896 train_utils.py:377] train in step: 525
I0712 12:55:06.516281 140282259928896 train_utils.py:377] train in step: 526
I0712 12:55:07.127621 140282259928896 train_utils.py:377] train in step: 527
I0712 12:55:07.740013 140282259928896 train_utils.py:377] train in step: 528
I0712 12:55:08.352567 140282259928896 train_utils.py:377] train in step: 529
I0712 12:55:08.965901 140282259928896 train_utils.py:377] train in step: 530
I0712 12:55:09.578812 140282259928896 train_utils.py:377] train in step: 531
I0712 12:55:10.190366 140282259928896 train_utils.py:377] train in step: 532
I0712 12:55:10.804914 140282259928896 train_utils.py:377] train in step: 533
I0712 12:55:11.420770 140282259928896 train_utils.py:377] train in step: 534
I0712 12:55:12.028645 140282259928896 train_utils.py:377] train in step: 535
I0712 12:55:12.642782 140282259928896 train_utils.py:377] train in step: 536
I0712 12:55:13.256250 140282259928896 train_utils.py:377] train in step: 537
I0712 12:55:13.868322 140282259928896 train_utils.py:377] train in step: 538
I0712 12:55:14.480933 140282259928896 train_utils.py:377] train in step: 539
I0712 12:55:15.094159 140282259928896 train_utils.py:377] train in step: 540
I0712 12:55:15.703438 140282259928896 train_utils.py:377] train in step: 541
I0712 12:55:16.317959 140282259928896 train_utils.py:377] train in step: 542
I0712 12:55:16.934925 140282259928896 train_utils.py:377] train in step: 543
I0712 12:55:17.547755 140282259928896 train_utils.py:377] train in step: 544
I0712 12:55:18.160438 140282259928896 train_utils.py:377] train in step: 545
I0712 12:55:18.774008 140282259928896 train_utils.py:377] train in step: 546
I0712 12:55:19.386048 140282259928896 train_utils.py:377] train in step: 547
I0712 12:55:19.998982 140282259928896 train_utils.py:377] train in step: 548
I0712 12:55:20.611673 140282259928896 train_utils.py:377] train in step: 549
I0712 12:55:21.223165 140282259928896 train_utils.py:377] train in step: 550
I0712 12:55:21.835455 140282259928896 train_utils.py:377] train in step: 551
I0712 12:55:22.450191 140282259928896 train_utils.py:377] train in step: 552
I0712 12:55:23.059230 140282259928896 train_utils.py:377] train in step: 553
I0712 12:55:23.674954 140282259928896 train_utils.py:377] train in step: 554
I0712 12:55:24.284776 140282259928896 train_utils.py:377] train in step: 555
I0712 12:55:24.898481 140282259928896 train_utils.py:377] train in step: 556
I0712 12:55:25.510637 140282259928896 train_utils.py:377] train in step: 557
I0712 12:55:26.123343 140282259928896 train_utils.py:377] train in step: 558
I0712 12:55:26.736248 140282259928896 train_utils.py:377] train in step: 559
I0712 12:55:27.349951 140282259928896 train_utils.py:377] train in step: 560
I0712 12:55:27.963235 140282259928896 train_utils.py:377] train in step: 561
I0712 12:55:28.575757 140282259928896 train_utils.py:377] train in step: 562
I0712 12:55:29.190192 140282259928896 train_utils.py:377] train in step: 563
I0712 12:55:29.797067 140282259928896 train_utils.py:377] train in step: 564
I0712 12:55:30.224117 140282259928896 train_utils.py:377] train in step: 565
I0712 12:55:30.503783 140282259928896 train_utils.py:377] train in step: 566
I0712 12:55:30.734163 140282259928896 train_utils.py:377] train in step: 567
I0712 12:55:30.949625 140282259928896 train_utils.py:377] train in step: 568
I0712 12:55:31.174190 140282259928896 train_utils.py:377] train in step: 569
I0712 12:55:31.396164 140282259928896 train_utils.py:377] train in step: 570
I0712 12:55:31.628162 140282259928896 train_utils.py:377] train in step: 571
I0712 12:55:31.849771 140282259928896 train_utils.py:377] train in step: 572
I0712 12:55:32.071303 140282259928896 train_utils.py:377] train in step: 573
I0712 12:55:32.293740 140282259928896 train_utils.py:377] train in step: 574
I0712 12:55:32.516440 140282259928896 train_utils.py:377] train in step: 575
I0712 12:55:32.745535 140282259928896 train_utils.py:377] train in step: 576
I0712 12:55:32.968365 140282259928896 train_utils.py:377] train in step: 577
I0712 12:55:33.188347 140282259928896 train_utils.py:377] train in step: 578
I0712 12:55:33.409895 140282259928896 train_utils.py:377] train in step: 579
I0712 12:55:33.630991 140282259928896 train_utils.py:377] train in step: 580
I0712 12:55:33.852009 140282259928896 train_utils.py:377] train in step: 581
I0712 12:55:34.074643 140282259928896 train_utils.py:377] train in step: 582
I0712 12:55:34.295294 140282259928896 train_utils.py:377] train in step: 583
I0712 12:55:34.516437 140282259928896 train_utils.py:377] train in step: 584
I0712 12:55:34.741114 140282259928896 train_utils.py:377] train in step: 585
I0712 12:55:34.960389 140282259928896 train_utils.py:377] train in step: 586
I0712 12:55:35.181924 140282259928896 train_utils.py:377] train in step: 587
I0712 12:55:35.403600 140282259928896 train_utils.py:377] train in step: 588
I0712 12:55:35.633303 140282259928896 train_utils.py:377] train in step: 589
I0712 12:55:35.861597 140282259928896 train_utils.py:377] train in step: 590
I0712 12:55:36.077951 140282259928896 train_utils.py:377] train in step: 591
I0712 12:55:36.301083 140282259928896 train_utils.py:377] train in step: 592
I0712 12:55:36.522318 140282259928896 train_utils.py:377] train in step: 593
I0712 12:55:36.743866 140282259928896 train_utils.py:377] train in step: 594
I0712 12:55:36.965712 140282259928896 train_utils.py:377] train in step: 595
I0712 12:55:37.191167 140282259928896 train_utils.py:377] train in step: 596
I0712 12:55:37.408979 140282259928896 train_utils.py:377] train in step: 597
I0712 12:55:37.630356 140282259928896 train_utils.py:377] train in step: 598
I0712 12:55:37.873885 140282259928896 train_utils.py:377] train in step: 599
I0712 12:55:38.096619 140282259928896 train_utils.py:377] train in step: 600
I0712 12:55:38.494881 140282259928896 train_utils.py:396] train in step: 600, loss: 0.7252999544143677, acc: 0.5087000131607056
I0712 12:55:46.672725 140282259928896 train_utils.py:411] eval in step: 600, loss: 0.7203, acc: 0.4950
I0712 12:55:46.676899 140282259928896 train_utils.py:421] Testing...
I0712 12:55:54.893828 140282259928896 train_utils.py:424] test in step: 600, loss: 0.7090, acc: 0.5200
I0712 12:55:54.933938 140282259928896 train_utils.py:377] train in step: 601
I0712 12:55:54.979673 140282259928896 train_utils.py:377] train in step: 602
I0712 12:55:55.200862 140282259928896 train_utils.py:377] train in step: 603
I0712 12:55:55.427929 140282259928896 train_utils.py:377] train in step: 604
I0712 12:55:55.650015 140282259928896 train_utils.py:377] train in step: 605
I0712 12:55:55.871145 140282259928896 train_utils.py:377] train in step: 606
I0712 12:55:56.093310 140282259928896 train_utils.py:377] train in step: 607
I0712 12:55:56.315464 140282259928896 train_utils.py:377] train in step: 608
I0712 12:55:56.537272 140282259928896 train_utils.py:377] train in step: 609
I0712 12:55:56.759111 140282259928896 train_utils.py:377] train in step: 610
I0712 12:55:56.985399 140282259928896 train_utils.py:377] train in step: 611
I0712 12:55:57.206451 140282259928896 train_utils.py:377] train in step: 612
I0712 12:55:57.432130 140282259928896 train_utils.py:377] train in step: 613
I0712 12:55:57.657085 140282259928896 train_utils.py:377] train in step: 614
I0712 12:55:57.877009 140282259928896 train_utils.py:377] train in step: 615
I0712 12:55:58.101919 140282259928896 train_utils.py:377] train in step: 616
I0712 12:55:58.323198 140282259928896 train_utils.py:377] train in step: 617
I0712 12:55:58.544033 140282259928896 train_utils.py:377] train in step: 618
I0712 12:55:58.766493 140282259928896 train_utils.py:377] train in step: 619
I0712 12:55:58.991877 140282259928896 train_utils.py:377] train in step: 620
I0712 12:55:59.212934 140282259928896 train_utils.py:377] train in step: 621
I0712 12:55:59.434970 140282259928896 train_utils.py:377] train in step: 622
I0712 12:55:59.657345 140282259928896 train_utils.py:377] train in step: 623
I0712 12:55:59.879300 140282259928896 train_utils.py:377] train in step: 624
I0712 12:56:00.101489 140282259928896 train_utils.py:377] train in step: 625
I0712 12:56:00.329319 140282259928896 train_utils.py:377] train in step: 626
I0712 12:56:00.548502 140282259928896 train_utils.py:377] train in step: 627
I0712 12:56:00.769411 140282259928896 train_utils.py:377] train in step: 628
I0712 12:56:01.005488 140282259928896 train_utils.py:377] train in step: 629
I0712 12:56:01.241473 140282259928896 train_utils.py:377] train in step: 630
I0712 12:56:01.463253 140282259928896 train_utils.py:377] train in step: 631
I0712 12:56:01.685044 140282259928896 train_utils.py:377] train in step: 632
I0712 12:56:01.910421 140282259928896 train_utils.py:377] train in step: 633
I0712 12:56:02.131816 140282259928896 train_utils.py:377] train in step: 634
I0712 12:56:02.354444 140282259928896 train_utils.py:377] train in step: 635
I0712 12:56:02.584087 140282259928896 train_utils.py:377] train in step: 636
I0712 12:56:02.809123 140282259928896 train_utils.py:377] train in step: 637
I0712 12:56:03.029960 140282259928896 train_utils.py:377] train in step: 638
I0712 12:56:03.256561 140282259928896 train_utils.py:377] train in step: 639
I0712 12:56:03.538443 140282259928896 train_utils.py:377] train in step: 640
I0712 12:56:03.800543 140282259928896 train_utils.py:377] train in step: 641
I0712 12:56:04.029147 140282259928896 train_utils.py:377] train in step: 642
I0712 12:56:04.250682 140282259928896 train_utils.py:377] train in step: 643
I0712 12:56:04.474148 140282259928896 train_utils.py:377] train in step: 644
I0712 12:56:04.698500 140282259928896 train_utils.py:377] train in step: 645
I0712 12:56:04.946837 140282259928896 train_utils.py:377] train in step: 646
I0712 12:56:05.192039 140282259928896 train_utils.py:377] train in step: 647
I0712 12:56:05.474414 140282259928896 train_utils.py:377] train in step: 648
I0712 12:56:05.766952 140282259928896 train_utils.py:377] train in step: 649
I0712 12:56:06.056188 140282259928896 train_utils.py:377] train in step: 650
I0712 12:56:06.326472 140282259928896 train_utils.py:377] train in step: 651
I0712 12:56:06.571323 140282259928896 train_utils.py:377] train in step: 652
I0712 12:56:06.824588 140282259928896 train_utils.py:377] train in step: 653
I0712 12:56:07.069944 140282259928896 train_utils.py:377] train in step: 654
I0712 12:56:07.304341 140282259928896 train_utils.py:377] train in step: 655
I0712 12:56:07.531661 140282259928896 train_utils.py:377] train in step: 656
I0712 12:56:07.821832 140282259928896 train_utils.py:377] train in step: 657
I0712 12:56:08.098155 140282259928896 train_utils.py:377] train in step: 658
I0712 12:56:08.361851 140282259928896 train_utils.py:377] train in step: 659
I0712 12:56:08.606592 140282259928896 train_utils.py:377] train in step: 660
I0712 12:56:08.845744 140282259928896 train_utils.py:377] train in step: 661
I0712 12:56:09.101464 140282259928896 train_utils.py:377] train in step: 662
I0712 12:56:09.417404 140282259928896 train_utils.py:377] train in step: 663
I0712 12:56:09.698225 140282259928896 train_utils.py:377] train in step: 664
I0712 12:56:09.966610 140282259928896 train_utils.py:377] train in step: 665
I0712 12:56:10.293052 140282259928896 train_utils.py:377] train in step: 666
I0712 12:56:10.601261 140282259928896 train_utils.py:377] train in step: 667
I0712 12:56:10.882509 140282259928896 train_utils.py:377] train in step: 668
I0712 12:56:11.154350 140282259928896 train_utils.py:377] train in step: 669
I0712 12:56:11.423854 140282259928896 train_utils.py:377] train in step: 670
I0712 12:56:11.663793 140282259928896 train_utils.py:377] train in step: 671
I0712 12:56:11.902073 140282259928896 train_utils.py:377] train in step: 672
I0712 12:56:12.125078 140282259928896 train_utils.py:377] train in step: 673
I0712 12:56:12.371958 140282259928896 train_utils.py:377] train in step: 674
I0712 12:56:12.653234 140282259928896 train_utils.py:377] train in step: 675
I0712 12:56:13.145526 140282259928896 train_utils.py:377] train in step: 676
I0712 12:56:13.760559 140282259928896 train_utils.py:377] train in step: 677
I0712 12:56:14.372891 140282259928896 train_utils.py:377] train in step: 678
I0712 12:56:15.197174 140282259928896 train_utils.py:377] train in step: 679
I0712 12:56:16.240035 140282259928896 train_utils.py:377] train in step: 680
I0712 12:56:16.903382 140282259928896 train_utils.py:377] train in step: 681
I0712 12:56:17.518124 140282259928896 train_utils.py:377] train in step: 682
I0712 12:56:18.132888 140282259928896 train_utils.py:377] train in step: 683
I0712 12:56:18.745780 140282259928896 train_utils.py:377] train in step: 684
I0712 12:56:19.357034 140282259928896 train_utils.py:377] train in step: 685
I0712 12:56:19.972789 140282259928896 train_utils.py:377] train in step: 686
I0712 12:56:20.583207 140282259928896 train_utils.py:377] train in step: 687
I0712 12:56:21.195981 140282259928896 train_utils.py:377] train in step: 688
I0712 12:56:21.817550 140282259928896 train_utils.py:377] train in step: 689
I0712 12:56:22.443374 140282259928896 train_utils.py:377] train in step: 690
I0712 12:56:23.056147 140282259928896 train_utils.py:377] train in step: 691
I0712 12:56:23.667905 140282259928896 train_utils.py:377] train in step: 692
I0712 12:56:24.287218 140282259928896 train_utils.py:377] train in step: 693
I0712 12:56:24.903885 140282259928896 train_utils.py:377] train in step: 694
I0712 12:56:25.514849 140282259928896 train_utils.py:377] train in step: 695
I0712 12:56:26.128264 140282259928896 train_utils.py:377] train in step: 696
I0712 12:56:26.742333 140282259928896 train_utils.py:377] train in step: 697
I0712 12:56:27.357939 140282259928896 train_utils.py:377] train in step: 698
I0712 12:56:27.969460 140282259928896 train_utils.py:377] train in step: 699
I0712 12:56:28.582940 140282259928896 train_utils.py:377] train in step: 700
I0712 12:56:29.194001 140282259928896 train_utils.py:377] train in step: 701
I0712 12:56:29.806016 140282259928896 train_utils.py:377] train in step: 702
I0712 12:56:30.424162 140282259928896 train_utils.py:377] train in step: 703
I0712 12:56:31.037533 140282259928896 train_utils.py:377] train in step: 704
I0712 12:56:31.650102 140282259928896 train_utils.py:377] train in step: 705
I0712 12:56:32.262947 140282259928896 train_utils.py:377] train in step: 706
I0712 12:56:32.875144 140282259928896 train_utils.py:377] train in step: 707
I0712 12:56:33.487874 140282259928896 train_utils.py:377] train in step: 708
I0712 12:56:34.101509 140282259928896 train_utils.py:377] train in step: 709
I0712 12:56:34.712381 140282259928896 train_utils.py:377] train in step: 710
I0712 12:56:35.326461 140282259928896 train_utils.py:377] train in step: 711
I0712 12:56:35.940676 140282259928896 train_utils.py:377] train in step: 712
I0712 12:56:36.553757 140282259928896 train_utils.py:377] train in step: 713
I0712 12:56:37.167147 140282259928896 train_utils.py:377] train in step: 714
I0712 12:56:37.782362 140282259928896 train_utils.py:377] train in step: 715
I0712 12:56:38.395081 140282259928896 train_utils.py:377] train in step: 716
I0712 12:56:39.007662 140282259928896 train_utils.py:377] train in step: 717
I0712 12:56:39.620418 140282259928896 train_utils.py:377] train in step: 718
I0712 12:56:40.233519 140282259928896 train_utils.py:377] train in step: 719
I0712 12:56:40.862155 140282259928896 train_utils.py:377] train in step: 720
I0712 12:56:41.469400 140282259928896 train_utils.py:377] train in step: 721
I0712 12:56:42.081591 140282259928896 train_utils.py:377] train in step: 722
I0712 12:56:42.695519 140282259928896 train_utils.py:377] train in step: 723
I0712 12:56:43.309211 140282259928896 train_utils.py:377] train in step: 724
I0712 12:56:43.922732 140282259928896 train_utils.py:377] train in step: 725
I0712 12:56:44.534285 140282259928896 train_utils.py:377] train in step: 726
I0712 12:56:45.145909 140282259928896 train_utils.py:377] train in step: 727
I0712 12:56:45.761739 140282259928896 train_utils.py:377] train in step: 728
I0712 12:56:46.374987 140282259928896 train_utils.py:377] train in step: 729
I0712 12:56:46.983870 140282259928896 train_utils.py:377] train in step: 730
I0712 12:56:47.595915 140282259928896 train_utils.py:377] train in step: 731
I0712 12:56:48.211480 140282259928896 train_utils.py:377] train in step: 732
I0712 12:56:48.824556 140282259928896 train_utils.py:377] train in step: 733
I0712 12:56:49.437217 140282259928896 train_utils.py:377] train in step: 734
I0712 12:56:50.049430 140282259928896 train_utils.py:377] train in step: 735
I0712 12:56:50.659896 140282259928896 train_utils.py:377] train in step: 736
I0712 12:56:51.271689 140282259928896 train_utils.py:377] train in step: 737
I0712 12:56:51.882857 140282259928896 train_utils.py:377] train in step: 738
I0712 12:56:52.511489 140282259928896 train_utils.py:377] train in step: 739
I0712 12:56:53.122816 140282259928896 train_utils.py:377] train in step: 740
I0712 12:56:53.737430 140282259928896 train_utils.py:377] train in step: 741
I0712 12:56:54.346998 140282259928896 train_utils.py:377] train in step: 742
I0712 12:56:54.957689 140282259928896 train_utils.py:377] train in step: 743
I0712 12:56:55.569627 140282259928896 train_utils.py:377] train in step: 744
I0712 12:56:56.182578 140282259928896 train_utils.py:377] train in step: 745
I0712 12:56:56.794600 140282259928896 train_utils.py:377] train in step: 746
I0712 12:56:57.407028 140282259928896 train_utils.py:377] train in step: 747
I0712 12:56:58.020226 140282259928896 train_utils.py:377] train in step: 748
I0712 12:56:58.633687 140282259928896 train_utils.py:377] train in step: 749
I0712 12:56:59.254878 140282259928896 train_utils.py:377] train in step: 750
I0712 12:56:59.867514 140282259928896 train_utils.py:377] train in step: 751
I0712 12:57:00.480541 140282259928896 train_utils.py:377] train in step: 752
I0712 12:57:01.094644 140282259928896 train_utils.py:377] train in step: 753
I0712 12:57:01.702900 140282259928896 train_utils.py:377] train in step: 754
I0712 12:57:02.316173 140282259928896 train_utils.py:377] train in step: 755
I0712 12:57:02.928045 140282259928896 train_utils.py:377] train in step: 756
I0712 12:57:03.538419 140282259928896 train_utils.py:377] train in step: 757
I0712 12:57:04.151198 140282259928896 train_utils.py:377] train in step: 758
I0712 12:57:04.763194 140282259928896 train_utils.py:377] train in step: 759
I0712 12:57:05.375715 140282259928896 train_utils.py:377] train in step: 760
I0712 12:57:05.987153 140282259928896 train_utils.py:377] train in step: 761
I0712 12:57:06.600031 140282259928896 train_utils.py:377] train in step: 762
I0712 12:57:07.212396 140282259928896 train_utils.py:377] train in step: 763
I0712 12:57:07.824090 140282259928896 train_utils.py:377] train in step: 764
I0712 12:57:08.435798 140282259928896 train_utils.py:377] train in step: 765
I0712 12:57:09.047888 140282259928896 train_utils.py:377] train in step: 766
I0712 12:57:09.660889 140282259928896 train_utils.py:377] train in step: 767
I0712 12:57:10.275019 140282259928896 train_utils.py:377] train in step: 768
I0712 12:57:10.886102 140282259928896 train_utils.py:377] train in step: 769
I0712 12:57:11.497444 140282259928896 train_utils.py:377] train in step: 770
I0712 12:57:12.108546 140282259928896 train_utils.py:377] train in step: 771
I0712 12:57:12.721199 140282259928896 train_utils.py:377] train in step: 772
I0712 12:57:13.333240 140282259928896 train_utils.py:377] train in step: 773
I0712 12:57:13.945027 140282259928896 train_utils.py:377] train in step: 774
I0712 12:57:14.558497 140282259928896 train_utils.py:377] train in step: 775
I0712 12:57:15.170345 140282259928896 train_utils.py:377] train in step: 776
I0712 12:57:15.781292 140282259928896 train_utils.py:377] train in step: 777
I0712 12:57:16.396363 140282259928896 train_utils.py:377] train in step: 778
I0712 12:57:17.002672 140282259928896 train_utils.py:377] train in step: 779
I0712 12:57:17.614431 140282259928896 train_utils.py:377] train in step: 780
I0712 12:57:18.226202 140282259928896 train_utils.py:377] train in step: 781
I0712 12:57:18.838717 140282259928896 train_utils.py:377] train in step: 782
I0712 12:57:19.451796 140282259928896 train_utils.py:377] train in step: 783
I0712 12:57:20.063056 140282259928896 train_utils.py:377] train in step: 784
I0712 12:57:20.675247 140282259928896 train_utils.py:377] train in step: 785
I0712 12:57:21.285870 140282259928896 train_utils.py:377] train in step: 786
I0712 12:57:21.897308 140282259928896 train_utils.py:377] train in step: 787
I0712 12:57:22.507622 140282259928896 train_utils.py:377] train in step: 788
I0712 12:57:23.118779 140282259928896 train_utils.py:377] train in step: 789
I0712 12:57:23.731057 140282259928896 train_utils.py:377] train in step: 790
I0712 12:57:24.343967 140282259928896 train_utils.py:377] train in step: 791
I0712 12:57:24.955772 140282259928896 train_utils.py:377] train in step: 792
I0712 12:57:25.567222 140282259928896 train_utils.py:377] train in step: 793
I0712 12:57:26.179418 140282259928896 train_utils.py:377] train in step: 794
I0712 12:57:26.791100 140282259928896 train_utils.py:377] train in step: 795
I0712 12:57:27.401070 140282259928896 train_utils.py:377] train in step: 796
I0712 12:57:28.012873 140282259928896 train_utils.py:377] train in step: 797
I0712 12:57:28.626509 140282259928896 train_utils.py:377] train in step: 798
I0712 12:57:29.237730 140282259928896 train_utils.py:377] train in step: 799
I0712 12:57:29.848341 140282259928896 train_utils.py:377] train in step: 800
I0712 12:57:30.937908 140282259928896 train_utils.py:396] train in step: 800, loss: 0.7008999586105347, acc: 0.5525000095367432
I0712 12:57:39.091454 140282259928896 train_utils.py:411] eval in step: 800, loss: 0.6904, acc: 0.5800
I0712 12:57:39.094032 140282259928896 train_utils.py:421] Testing...
I0712 12:57:47.347053 140282259928896 train_utils.py:424] test in step: 800, loss: 0.7097, acc: 0.5100
I0712 12:57:47.377215 140282259928896 train_utils.py:377] train in step: 801
I0712 12:57:47.425666 140282259928896 train_utils.py:377] train in step: 802
I0712 12:57:47.648221 140282259928896 train_utils.py:377] train in step: 803
I0712 12:57:47.871933 140282259928896 train_utils.py:377] train in step: 804
I0712 12:57:48.091641 140282259928896 train_utils.py:377] train in step: 805
I0712 12:57:48.314793 140282259928896 train_utils.py:377] train in step: 806
I0712 12:57:48.536713 140282259928896 train_utils.py:377] train in step: 807
I0712 12:57:48.757820 140282259928896 train_utils.py:377] train in step: 808
I0712 12:57:48.992070 140282259928896 train_utils.py:377] train in step: 809
I0712 12:57:49.213843 140282259928896 train_utils.py:377] train in step: 810
I0712 12:57:49.438489 140282259928896 train_utils.py:377] train in step: 811
I0712 12:57:49.661843 140282259928896 train_utils.py:377] train in step: 812
I0712 12:57:49.883701 140282259928896 train_utils.py:377] train in step: 813
I0712 12:57:50.104547 140282259928896 train_utils.py:377] train in step: 814
I0712 12:57:50.327453 140282259928896 train_utils.py:377] train in step: 815
I0712 12:57:50.549326 140282259928896 train_utils.py:377] train in step: 816
I0712 12:57:50.771272 140282259928896 train_utils.py:377] train in step: 817
I0712 12:57:50.991580 140282259928896 train_utils.py:377] train in step: 818
I0712 12:57:51.213770 140282259928896 train_utils.py:377] train in step: 819
I0712 12:57:51.436691 140282259928896 train_utils.py:377] train in step: 820
I0712 12:57:51.659357 140282259928896 train_utils.py:377] train in step: 821
I0712 12:57:51.881912 140282259928896 train_utils.py:377] train in step: 822
I0712 12:57:52.103778 140282259928896 train_utils.py:377] train in step: 823
I0712 12:57:52.325410 140282259928896 train_utils.py:377] train in step: 824
I0712 12:57:52.547305 140282259928896 train_utils.py:377] train in step: 825
I0712 12:57:52.769043 140282259928896 train_utils.py:377] train in step: 826
I0712 12:57:52.991805 140282259928896 train_utils.py:377] train in step: 827
I0712 12:57:53.213963 140282259928896 train_utils.py:377] train in step: 828
I0712 12:57:53.434984 140282259928896 train_utils.py:377] train in step: 829
I0712 12:57:53.658674 140282259928896 train_utils.py:377] train in step: 830
I0712 12:57:53.880596 140282259928896 train_utils.py:377] train in step: 831
I0712 12:57:54.101668 140282259928896 train_utils.py:377] train in step: 832
I0712 12:57:54.324070 140282259928896 train_utils.py:377] train in step: 833
I0712 12:57:54.546211 140282259928896 train_utils.py:377] train in step: 834
I0712 12:57:54.767878 140282259928896 train_utils.py:377] train in step: 835
I0712 12:57:54.988938 140282259928896 train_utils.py:377] train in step: 836
I0712 12:57:55.211460 140282259928896 train_utils.py:377] train in step: 837
I0712 12:57:55.434048 140282259928896 train_utils.py:377] train in step: 838
I0712 12:57:55.655539 140282259928896 train_utils.py:377] train in step: 839
I0712 12:57:55.877734 140282259928896 train_utils.py:377] train in step: 840
I0712 12:57:56.111160 140282259928896 train_utils.py:377] train in step: 841
I0712 12:57:56.332964 140282259928896 train_utils.py:377] train in step: 842
I0712 12:57:56.555340 140282259928896 train_utils.py:377] train in step: 843
I0712 12:57:56.782989 140282259928896 train_utils.py:377] train in step: 844
I0712 12:57:57.005732 140282259928896 train_utils.py:377] train in step: 845
I0712 12:57:57.226979 140282259928896 train_utils.py:377] train in step: 846
I0712 12:57:57.449509 140282259928896 train_utils.py:377] train in step: 847
I0712 12:57:57.671146 140282259928896 train_utils.py:377] train in step: 848
I0712 12:57:57.892792 140282259928896 train_utils.py:377] train in step: 849
I0712 12:57:58.115248 140282259928896 train_utils.py:377] train in step: 850
I0712 12:57:58.337528 140282259928896 train_utils.py:377] train in step: 851
I0712 12:57:58.559057 140282259928896 train_utils.py:377] train in step: 852
I0712 12:57:58.781406 140282259928896 train_utils.py:377] train in step: 853
I0712 12:57:59.004681 140282259928896 train_utils.py:377] train in step: 854
I0712 12:57:59.228183 140282259928896 train_utils.py:377] train in step: 855
I0712 12:57:59.458771 140282259928896 train_utils.py:377] train in step: 856
I0712 12:57:59.682782 140282259928896 train_utils.py:377] train in step: 857
I0712 12:57:59.904325 140282259928896 train_utils.py:377] train in step: 858
I0712 12:58:00.127021 140282259928896 train_utils.py:377] train in step: 859
I0712 12:58:00.349630 140282259928896 train_utils.py:377] train in step: 860
I0712 12:58:00.572016 140282259928896 train_utils.py:377] train in step: 861
I0712 12:58:00.794312 140282259928896 train_utils.py:377] train in step: 862
I0712 12:58:01.016466 140282259928896 train_utils.py:377] train in step: 863
I0712 12:58:01.238220 140282259928896 train_utils.py:377] train in step: 864
I0712 12:58:01.497585 140282259928896 train_utils.py:377] train in step: 865
I0712 12:58:01.750383 140282259928896 train_utils.py:377] train in step: 866
I0712 12:58:01.976007 140282259928896 train_utils.py:377] train in step: 867
I0712 12:58:02.199287 140282259928896 train_utils.py:377] train in step: 868
I0712 12:58:02.420761 140282259928896 train_utils.py:377] train in step: 869
I0712 12:58:02.645669 140282259928896 train_utils.py:377] train in step: 870
I0712 12:58:02.867996 140282259928896 train_utils.py:377] train in step: 871
I0712 12:58:03.169644 140282259928896 train_utils.py:377] train in step: 872
I0712 12:58:03.448134 140282259928896 train_utils.py:377] train in step: 873
I0712 12:58:03.685498 140282259928896 train_utils.py:377] train in step: 874
I0712 12:58:03.905454 140282259928896 train_utils.py:377] train in step: 875
I0712 12:58:04.129922 140282259928896 train_utils.py:377] train in step: 876
I0712 12:58:04.358034 140282259928896 train_utils.py:377] train in step: 877
I0712 12:58:04.613945 140282259928896 train_utils.py:377] train in step: 878
I0712 12:58:04.937090 140282259928896 train_utils.py:377] train in step: 879
I0712 12:58:05.203666 140282259928896 train_utils.py:377] train in step: 880
I0712 12:58:05.433525 140282259928896 train_utils.py:377] train in step: 881
I0712 12:58:05.662770 140282259928896 train_utils.py:377] train in step: 882
I0712 12:58:05.884143 140282259928896 train_utils.py:377] train in step: 883
I0712 12:58:06.106093 140282259928896 train_utils.py:377] train in step: 884
I0712 12:58:06.355031 140282259928896 train_utils.py:377] train in step: 885
I0712 12:58:06.615162 140282259928896 train_utils.py:377] train in step: 886
I0712 12:58:06.937730 140282259928896 train_utils.py:377] train in step: 887
I0712 12:58:07.296784 140282259928896 train_utils.py:377] train in step: 888
I0712 12:58:07.619847 140282259928896 train_utils.py:377] train in step: 889
I0712 12:58:07.881912 140282259928896 train_utils.py:377] train in step: 890
I0712 12:58:08.114142 140282259928896 train_utils.py:377] train in step: 891
I0712 12:58:08.339486 140282259928896 train_utils.py:377] train in step: 892
I0712 12:58:08.625442 140282259928896 train_utils.py:377] train in step: 893
I0712 12:58:08.903344 140282259928896 train_utils.py:377] train in step: 894
I0712 12:58:09.148457 140282259928896 train_utils.py:377] train in step: 895
I0712 12:58:09.396117 140282259928896 train_utils.py:377] train in step: 896
I0712 12:58:09.692519 140282259928896 train_utils.py:377] train in step: 897
I0712 12:58:09.997839 140282259928896 train_utils.py:377] train in step: 898
I0712 12:58:10.273754 140282259928896 train_utils.py:377] train in step: 899
I0712 12:58:10.532173 140282259928896 train_utils.py:377] train in step: 900
I0712 12:58:10.806629 140282259928896 train_utils.py:377] train in step: 901
I0712 12:58:11.276629 140282259928896 train_utils.py:377] train in step: 902
I0712 12:58:11.890666 140282259928896 train_utils.py:377] train in step: 903
I0712 12:58:12.503484 140282259928896 train_utils.py:377] train in step: 904
I0712 12:58:13.116381 140282259928896 train_utils.py:377] train in step: 905
I0712 12:58:13.732981 140282259928896 train_utils.py:377] train in step: 906
I0712 12:58:14.346816 140282259928896 train_utils.py:377] train in step: 907
I0712 12:58:14.960448 140282259928896 train_utils.py:377] train in step: 908
I0712 12:58:15.574742 140282259928896 train_utils.py:377] train in step: 909
I0712 12:58:16.191014 140282259928896 train_utils.py:377] train in step: 910
I0712 12:58:16.806413 140282259928896 train_utils.py:377] train in step: 911
I0712 12:58:17.422322 140282259928896 train_utils.py:377] train in step: 912
I0712 12:58:18.040660 140282259928896 train_utils.py:377] train in step: 913
I0712 12:58:18.666195 140282259928896 train_utils.py:377] train in step: 914
I0712 12:58:19.280056 140282259928896 train_utils.py:377] train in step: 915
I0712 12:58:19.893370 140282259928896 train_utils.py:377] train in step: 916
I0712 12:58:20.511592 140282259928896 train_utils.py:377] train in step: 917
I0712 12:58:21.137023 140282259928896 train_utils.py:377] train in step: 918
I0712 12:58:21.469991 140282259928896 train_utils.py:377] train in step: 919
I0712 12:58:21.691509 140282259928896 train_utils.py:377] train in step: 920
I0712 12:58:21.915061 140282259928896 train_utils.py:377] train in step: 921
I0712 12:58:22.134739 140282259928896 train_utils.py:377] train in step: 922
I0712 12:58:22.358390 140282259928896 train_utils.py:377] train in step: 923
I0712 12:58:22.584706 140282259928896 train_utils.py:377] train in step: 924
I0712 12:58:22.807377 140282259928896 train_utils.py:377] train in step: 925
I0712 12:58:23.029095 140282259928896 train_utils.py:377] train in step: 926
I0712 12:58:23.260366 140282259928896 train_utils.py:377] train in step: 927
I0712 12:58:23.480725 140282259928896 train_utils.py:377] train in step: 928
I0712 12:58:23.702948 140282259928896 train_utils.py:377] train in step: 929
I0712 12:58:23.927196 140282259928896 train_utils.py:377] train in step: 930
I0712 12:58:24.150615 140282259928896 train_utils.py:377] train in step: 931
I0712 12:58:24.373704 140282259928896 train_utils.py:377] train in step: 932
I0712 12:58:24.620018 140282259928896 train_utils.py:377] train in step: 933
I0712 12:58:24.864354 140282259928896 train_utils.py:377] train in step: 934
I0712 12:58:25.099682 140282259928896 train_utils.py:377] train in step: 935
I0712 12:58:25.323427 140282259928896 train_utils.py:377] train in step: 936
I0712 12:58:25.545296 140282259928896 train_utils.py:377] train in step: 937
I0712 12:58:25.767167 140282259928896 train_utils.py:377] train in step: 938
I0712 12:58:25.990411 140282259928896 train_utils.py:377] train in step: 939
I0712 12:58:26.249599 140282259928896 train_utils.py:377] train in step: 940
I0712 12:58:26.508662 140282259928896 train_utils.py:377] train in step: 941
I0712 12:58:26.760234 140282259928896 train_utils.py:377] train in step: 942
I0712 12:58:27.010227 140282259928896 train_utils.py:377] train in step: 943
I0712 12:58:27.327313 140282259928896 train_utils.py:377] train in step: 944
I0712 12:58:27.634683 140282259928896 train_utils.py:377] train in step: 945
I0712 12:58:27.897707 140282259928896 train_utils.py:377] train in step: 946
I0712 12:58:28.132238 140282259928896 train_utils.py:377] train in step: 947
I0712 12:58:28.355498 140282259928896 train_utils.py:377] train in step: 948
I0712 12:58:28.599273 140282259928896 train_utils.py:377] train in step: 949
I0712 12:58:28.938624 140282259928896 train_utils.py:377] train in step: 950
I0712 12:58:29.285750 140282259928896 train_utils.py:377] train in step: 951
I0712 12:58:29.555593 140282259928896 train_utils.py:377] train in step: 952
I0712 12:58:29.786683 140282259928896 train_utils.py:377] train in step: 953
I0712 12:58:30.007635 140282259928896 train_utils.py:377] train in step: 954
I0712 12:58:30.247720 140282259928896 train_utils.py:377] train in step: 955
I0712 12:58:30.581274 140282259928896 train_utils.py:377] train in step: 956
I0712 12:58:30.916955 140282259928896 train_utils.py:377] train in step: 957
I0712 12:58:31.231727 140282259928896 train_utils.py:377] train in step: 958
I0712 12:58:31.479126 140282259928896 train_utils.py:377] train in step: 959
I0712 12:58:31.710633 140282259928896 train_utils.py:377] train in step: 960
I0712 12:58:31.993303 140282259928896 train_utils.py:377] train in step: 961
I0712 12:58:32.341794 140282259928896 train_utils.py:377] train in step: 962
I0712 12:58:32.697441 140282259928896 train_utils.py:377] train in step: 963
I0712 12:58:33.017940 140282259928896 train_utils.py:377] train in step: 964
I0712 12:58:33.497041 140282259928896 train_utils.py:377] train in step: 965
I0712 12:58:34.112213 140282259928896 train_utils.py:377] train in step: 966
I0712 12:58:34.728632 140282259928896 train_utils.py:377] train in step: 967
I0712 12:58:35.342314 140282259928896 train_utils.py:377] train in step: 968
I0712 12:58:35.956713 140282259928896 train_utils.py:377] train in step: 969
I0712 12:58:36.584887 140282259928896 train_utils.py:377] train in step: 970
I0712 12:58:37.200648 140282259928896 train_utils.py:377] train in step: 971
I0712 12:58:37.814802 140282259928896 train_utils.py:377] train in step: 972
I0712 12:58:38.429835 140282259928896 train_utils.py:377] train in step: 973
I0712 12:58:39.042155 140282259928896 train_utils.py:377] train in step: 974
I0712 12:58:39.657264 140282259928896 train_utils.py:377] train in step: 975
I0712 12:58:40.274515 140282259928896 train_utils.py:377] train in step: 976
I0712 12:58:40.893093 140282259928896 train_utils.py:377] train in step: 977
I0712 12:58:41.508271 140282259928896 train_utils.py:377] train in step: 978
I0712 12:58:42.121411 140282259928896 train_utils.py:377] train in step: 979
I0712 12:58:42.748089 140282259928896 train_utils.py:377] train in step: 980
I0712 12:58:43.369048 140282259928896 train_utils.py:377] train in step: 981
I0712 12:58:43.985508 140282259928896 train_utils.py:377] train in step: 982
I0712 12:58:44.602648 140282259928896 train_utils.py:377] train in step: 983
I0712 12:58:45.217312 140282259928896 train_utils.py:377] train in step: 984
I0712 12:58:45.828681 140282259928896 train_utils.py:377] train in step: 985
I0712 12:58:46.442823 140282259928896 train_utils.py:377] train in step: 986
I0712 12:58:47.058962 140282259928896 train_utils.py:377] train in step: 987
I0712 12:58:47.675876 140282259928896 train_utils.py:377] train in step: 988
I0712 12:58:48.291409 140282259928896 train_utils.py:377] train in step: 989
I0712 12:58:48.906336 140282259928896 train_utils.py:377] train in step: 990
I0712 12:58:49.525276 140282259928896 train_utils.py:377] train in step: 991
I0712 12:58:50.147210 140282259928896 train_utils.py:377] train in step: 992
I0712 12:58:50.764057 140282259928896 train_utils.py:377] train in step: 993
I0712 12:58:51.381448 140282259928896 train_utils.py:377] train in step: 994
I0712 12:58:51.998171 140282259928896 train_utils.py:377] train in step: 995
I0712 12:58:52.617350 140282259928896 train_utils.py:377] train in step: 996
I0712 12:58:53.232681 140282259928896 train_utils.py:377] train in step: 997
I0712 12:58:53.846001 140282259928896 train_utils.py:377] train in step: 998
I0712 12:58:54.459203 140282259928896 train_utils.py:377] train in step: 999
I0712 12:58:55.073263 140282259928896 train_utils.py:377] train in step: 1000
I0712 12:58:55.077358 140282259928896 checkpoints.py:120] Saving checkpoint at step: 1000
I0712 12:58:56.210567 140282259928896 checkpoints.py:149] Saved checkpoint at trained_models/matching/transformer/checkpoint_1000
I0712 12:58:56.251282 140282259928896 train_utils.py:396] train in step: 1000, loss: 0.6782000064849854, acc: 0.5874999761581421
I0712 12:59:04.472181 140282259928896 train_utils.py:411] eval in step: 1000, loss: 0.7305, acc: 0.4850
I0712 12:59:04.475222 140282259928896 train_utils.py:421] Testing...
I0712 12:59:12.743332 140282259928896 train_utils.py:424] test in step: 1000, loss: 0.7341, acc: 0.5600
