2022-07-12 13:01:19.944507: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:01:19.944716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:01:19.944837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2022-07-12 13:01:19.944970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2022-07-12 13:01:19.945083: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:01:19.945217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:01:19.945332: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2022-07-12 13:01:19.945375: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0712 13:01:19.945802 140680526767936 train.py:67] ===========Config Dict============
I0712 13:01:19.946059 140680526767936 train.py:68] available_devices:
- 0
- 1
- 2
- 3
base_type: dual_encoder
batch_size: 4
checkpoint_freq: 10000
data_dir: google_datasets/doc_retrieval/tsv_data/
emb_dim: 128
eval_frequency: 200
factors: constant * linear_warmup * rsqrt_decay
learning_rate: 0.05
max_eval_target_length: 200
max_length: 4000
max_predict_token_length: 50
max_target_length: 200
mlp_dim: 512
model_type: longformer
num_eval_steps: 50
num_heads: 4
num_layers: 4
num_train_steps: 1001
pooling_mode: CLS
prompt: ''
qkv_dim: 128
random_seed: 0
restore_checkpoints: true
sampling_temperature: 0.6
sampling_top_k: 20
save_checkpoints: true
task_name: aan_pairs
tokenizer: char
trial: 0
vocab_file_path: google_datasets/doc_retrieval/aan/
warmup: 1000
weight_decay: 0.1

I0712 13:01:19.957318 140680526767936 xla_bridge.py:340] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0712 13:01:21.061049 140680526767936 xla_bridge.py:340] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0712 13:01:21.061857 140680526767936 xla_bridge.py:340] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0712 13:01:21.062086 140680526767936 train.py:80] GPU devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)]
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
I0712 13:01:21.062229 140680526767936 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
I0712 13:01:21.129286 140680526767936 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 13:01:21.164830 140680526767936 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 13:01:23.586038 140680526767936 input_pipeline.py:60] Data sample: OrderedDict([('label', 0.0), ('id1', b'H05-1061'), ('id2', b'W99-0635'), ('text1', b'b"1 Introduction  Key phrases such as named entities (person, location and organization names), book and movie titles, science, medical or military terms and others  1, are usually among the most information-bearing  linguistic structures. Translating them correctly  will improve the performance of cross-lingual information retrieval, question answering and machine translation systems. However, these key  phrases are often domain-specific, and people con                                                                                                                     1 Some name and terminology is a single word, which could  be regarded as a one-word phrase.  stantly create new key phrases which are not covered by existing bilingual dictionaries or parallel  corpora, therefore standard data-driven or knowledge-based machine translation systems cannot  translate them correctly.   As an increasing amount of web information becomes available, exploiting such a huge information resource is becoming more attractive. (Resnik  1999) searched the web for parallel corpora while  (Lu et al 2002) extracted translation pairs from  anchor texts pointing to the same webpage. However, parallel webpages or anchor texts are quite  limited, and these approaches greatly suffer from  the lack of data.   However, there are many web pages containing  useful bilingual information where key phrases and  their translations both occur. See the example in  Figure 1. This example demonstrates web page  snippets2 containing both a Chinese key phrase ?? ??? and its translation, ?Faust?.  We thus can transform the translation problem  into a data mining problem by retrieving these  mixed-language web pages and extracting their  translations. We propose a new framework to mine  key phrase translations from web corpora. Given a  source key phrase (here a Chinese phrase), we first  retrieve web page snippets containing this phrase  using the Google search engine. We then expand  queries by adding the translations of topic-relevant  hint words from the returned snippets. We submit  the source key phrase and expanded queries again  to Google to retrieve mixed-language web page  snippets.  Finally, we extract the key phrase translation from the second-round returned snippets  with phonetic, semantic and frequency-distance  features.   2A snippet is a sentence or paragraph containing the key  phrase, returned with the web page URLs.  483                             Figure 1. Returned mixed-language web page snippets using source query    We achieve 46% phrase translation accuracy  when using 10 returned snippets, and 80% accuracy with 165 snippets. Both results are significantly better than several existing methods.     The reminder of this paper is organized as follows: cross-lingual query expansion is discussed in  section 2; key phrase translation extraction is addressed in section 3. In section 4 we present experimental results, which is followed by relevant  works and conclusions.  2 Retrieving Web Page Snippets through  Cross-lingual Query Expansion  For a Chinese key phrase f, we want to find its  translation e from the web, more specifically, from  the mixed-language web pages or web page snippets containing both f and e. As we do not know e,  we are unable to directly retrieve such mixedlanguage web page using (f,e) as the query.                                                            Figure 2. Returned mixed-language web page snippets using cross-lingual query expansion  However, we observed that when the author of a  web page lists both f and e in a page, it is very  likely that f\' and e\' are listed in the same page,  where f? is a Chinese hint word topically relevant  to f, and e? is f?s translation. Therefore if we know  a Chinese hint word f?, and we know its reliable  translation, e?, we can send (f, e?) as a query to retrieve mixed language web pages containing (f, e).     For example, to find web pages which contain  translations of ?????(Faust), we expand the  query to ????+goethe? since ???? (Goethe)  is the author of ?????(Faust). Figure 2 illustrates retrieved web page snippets with expanded  queries. We find that newly returned snippets contain more correct translations with higher ranks.     To propose a ?good? English hint e\' for f, first we  need to find a Chinese hint word f\' that is relevant  to f. Because f is often an OOV word, it is unlikely  that such information can be obtained from existing Chinese monolingual corpora. Instead, we  484 query Google for web pages containing f. From the  returned snippets we select Chinese words f\' based  on the following criteria:    1. f\' should be relevant to f based on the cooccurrence frequency. On average, 300  Chinese words are returned for each query  f. We only consider those words that occur  at least twice to be relevant.  2. f\' can be reliably translated given thecurrent bilingual resources (e.g. the LDC  Chinese-English lexicon 3  with 81,945  translation entries).  3. The meaning of f\' should not be too ambiguous. Words with many translations  are not used.  4. f\' should be translated into noun or noun  phrases. Given the fact that most OOV  words are noun or noun phrases, we ignore those source words which are translated into other part-of-speech words. The  British National Corpus4 is used to generate the English noun lists.    For each f, the top Chinese words f\' with the  highest frequency are selected. Their corresponding translations are then used as the cross-lingual  hint words for f. For example, for OOV word f =  ??? (Faust), the top candidate f\'s are ??? (Goethe)?, ? ?? (introduction)?, ??? (literature)? and ???(tragedy)?. We expand  the original query ????? to ???? +  goethe?, ???? + introduction?, ???? + literature?, ???? + tragic?, and then query Google  again for web page snippets containing the correct  translation ?Faust?.  3 Extracting Key Phrase Translation  When the Chinese key phrase and its English hint  words are sent to Google as the query, returned  web page snippets contain the source query and  possibly its translation. We preprocess the snippets  to remove irrelevant information. The preprocessing steps are:  1. Filter out HTML tags;                                                             3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogI d=LDC2002L27  4 http://www.natcorp.ox.ac.uk/  2. Convert HTML special characters (e.g.,  ?&lt?) to corresponding ASCII code (?>?);  3. Segment Chinese words based on a maximum string matching algorithm, which is  used to calculate the translation probability  between a Chinese key phrase and an English translation candidate.  4. Replace punctuation marks with phrase separator ?|?;  5. Replace non-query Chinese words with  placeholder mark ?+?, as they indicate the  distance between an English phrase and the  Chinese key phrase.  For example, the snippet   ? <b>???? </b>? (the bridges of  madison county)[review]. ????anjing |  ?????2004-01-25 ??? 02:13 | ? ?????  is converted into  | <b> ?  ?  ?  ? </b> |  the_bridges_of_Madison_county | review |  ++ + | anjing | ++ ++  | 2004-01-25 +++ 02  13 | + + ++ ++,  where ?<b>? and ?</b>? mark the start and end  positions of the Chinese key phrase. The candidate  English phrases, ?the bridges of madison county?,  ?review? and ?anjing?, will be aligned to the  source key phrase according to a combined feature  set using a transliteration model which captures the  pronunciation similarity, a translation model which  capturesthe semantic similarity and a frequencydistance model reflecting their relevancy. These  models are described below.  3.1 Transliteration Model  The transliteration model captures the phonetic  similarity between a Chinese phrase and an English translation candidate via string alignment.  Many key phrases are person and location names,  which are phonetically translated and whose written forms resemble their pronunciations. Therefore  it is possible to discover these translation pairs  through their surface strings. Surface string transliteration does not need a pronunciation lexicon to  map words into phoneme sequences; thus it is especially appealing for OOV word translation. For  non-Latin languages like Chinese, a romanization  485 script called ?pinyin? maps each Chinese character  into Latin letter strings. This normalization makes  the string alignment possible.       We adopt the transliteration model proposed in  (Huang, et al 2003). This model calculates the  probabilistic Levinstein distance between a romanized source string and a target string. Unlike the  traditional Levinstein distance calculation, the  character alignment cost is not binary (0/1); rather  it is the logarithm of character alignment probability, which ensures that characters with similar pronunciations (e.g. `p` and `b`) have higher  alignment probabilities and lower cost. These  probabilities are automatically learned from bilingual name lists using EM.  Assume the Chinese phrase f has J Chinese  characters, , and the English candidate  phrase e has L English words, . The  transliteration cost between a Chinese query and  an English translation candidate  is calculated as:  Jfff ,..., 21 Leee ,...,, 21 f e     where is the pinyin of Chinese character ,   is the i th letter in , and and are their  aligned English letters, respectively.   is the letter transliteration probability. The transliteration costs between a Chinese phrase and an  English phrase is approximated by the sum of their  letter transliteration cost along the optimal alignment path, which is identified based on dynamic  programming.    jy jf ijy , jy jae ),( ijae )|( ,),( jiji yep 3.2 Translation Model  The translation model measures the semantic  equivalence between a Chinese phrase and an English candidate. One widely used model is the IBM  model (Brown et al 1993). The phrase translation  probability is computed using the IBM model-1 as:         where is the lexical translation probabilities, which can be calculated according to the IBM  models. This alignment model is asymmetric, as  one source word can only be aligned to one target  word, while one target word can be aligned to multiple source words. We estimate both   and , and define the NE translation  cost as:  )|( lj efp )|( efPtrans )|( fePtrans ).|(log)|(log),( efPfePfeC transtranstrans += 3.3 Frequency-Distance Model  The more often a bilingual phrase pair co-occurs,  or the closer a bilingual phrase pair is within a snippet, the more likely they are translations of  each other. The frequency-distance model measures this correlation.      Suppose S is the set of returned snippets for  query , and a single returned snippet isf Ssi ? .  The source phrase occurs in si as  ( since f  may occur several times in a snippet). The frequency-distance weight of an English candidate  is   jif , 1?j e ??= i jis f ji efd ew , ),( 1 )( ,     .)|(log)|(log),( ,),(??? =? j i jia j jatrl yepyepfe where is the distance between phrase    and e, i.e., how many words are there between the  two phrases (the separator `|` is not counted).   ),( efd jif , 3.4 Feature Combination  Define the confidence measure for the transliteration model as:      where e and e? are English candidate phrases, and  m is the weight of the distance model. We empirically choose m=2 in our experiments. This  measure indicates how good the English phrase e is  compared with other candidates based on transliteration model. Similarly the translation model confidence measure is defined as:             The overall feature cost is the linear combination  of transliteration cost and translation cost, which  are weighted by their confidence scores respectively:      C jij , )\'()],\'(exp[ )()],(exp[ )|( \' ?= e m trl m trl trl ewfeC ewfeC fe? . )\'()],\'(exp[ )()],(exp[ )|( \' ?= e m trans m trans trans ewfeC ewfeC fe??? = = = J j L l ljJtrans efpL efP 1 1 )|( 1 )|( 486  ???? the Bridges of MadisonCounty                                                                                    where the linear combination weight ?  is chosen  empirically. While trl? and trans?  represent the relative rank of the current candidate among all compared candidates, C and  indicate its  absolute likelihood, which is useful to reject the  top 1 incorrect candidate if the true translation does  not occur in any returned snippets.   trl transC                                                            4 Experiments We evaluated our approach by translating a set of  key phrases from different domains. We selected  310 Chinese key phrases from 12 domains as the  test set, which were almost equally distributed  within these domains. We also manually translated  them as the reference translations. Table 1 shows  some typical phrases and their translations, where  one may find that correct key phrase translations  need both phonetic transliterations and semantic  translations. We evaluated inclusion rate, defined  as the percentage of correct key phrase translations  which can be retrieved in the returned snippets;  alignment accuracy, defined as the percentage of  key phrase translations which can be correctly  aligned given that these translations are included in  the snippets; and overall translation accuracy, defined as the percentage of key phrases which can  be translated correctly. We compared our approach  with the LiveTrans5 (Cheng et.al. 2004) system, an  unknown word translator using web corpora, and  we observed better translation performance using  our approach.  4.1 Query Translation Inclusion Rate  In the first round query search, for each Chinese  key phrase f, on average 13 unique snippets were  returned to identify relevant Chinese hint words f?,  and the top 5 f\'s were selected to generate hint  words e?s. In the second round f and e?s were sent  to Google again to retrieve mixed language snippets, which were used to extract e, the correct  translation of f.  Figure 3 shows the inclusion rate vs. the number  of snippets used for three mixed-language web  page searching strategies:                                                             5 http://livetrans.iis.sinica.edu.tw/lt.html   Table 1. Test set key phrases  ? Search any web pages containing f (Zhang  and Vines 2004);  ? Only search English web pages6 containing f (Cheng et al 2004);  ? Search any web pages containing f and  hint words e?, as proposed in this paper.        The first search strategy resulted in a relatively  low inclusion rate; the second achieved a much  higher inclusion rate. However, because such English pages were limited, and on average only 45  unique snippets could be found for each f, which  resulted in a maximum inclusion rate of 85.8%. In  the case of the cross-lingual query expansion, the  search space was much larger but more focused  and we achieved a high inclusion rate of 89.7%  using 32 mixed-language snippets and 95.2% using  165 snippets, both from the second round retrieval.   6 These web pages are labeled by Google as ?English? web pages, though they may contain non-English characters.  Movie Title  ????            Forrest Gump  Book Title  ???   Dream of the Red Mansion  ???    La Dame aux camellias  Organization  Name  ????   University of Notre Dame   ??????????? David and  Lucile Packard Foundation   Person  Name  ???          Ludwig Van Beethoven  ????? Audrey Hepburn  Location  Name  ????? Kamchatka  ??????? Taklamakan desert  Company / Brand  ???? Lufthansa German  Airlines  ???? Estee Lauder  Sci&Tech  Terms  ???? genetic algorithm  ???? speech recognition   Specie Term  ??               Aegypius monachus  ???              Manispentadactyla  Military  Term  ???              Aegis   ???              Phalcon  Medical  Term  ?????? SARS  ???? Arteriosclerosis  Music Term  ????     Bird-call in the Mountain  ???        Bassoon  Sports Term  ?????? Houston Rockets  ?????? Tour de France  )]()|()( ff?? exp[1 )],(exp[)|(),( eCe feCfefeC transtrans trltrl= ?? + ,? 487 Table 2. Alignment accuracies using different features    These search strategies are further discussed in the  section 5.  4.2 Translation Alignment Accuracy  We evaluated our key phrase extraction model by  testing queries whose correct translations were included in the returned snippets. We used different  feature combinations on differently sized snippets  to compare their alignment accuracies. Table 2  shows the result. Here ?Trl? means using the transliteration model, ?Trans? means using the translation model, and ?Fq-dis? means using FrequencyDistance model. The frequency-distance model  seemed to be the strongest single model in both  cases (with and without hint words), while incorporating phonetic and semantic features provided  additional strength to the overall performance.  Combining all three features together yielded the  best accuracy. Note that when more candidate  translations were available through query expansion, the alignment accuracy improved by 30%  relative due to the frequency-distance model.  However, using transliteration and/or translation  models alone decreased performance because of  more incorrect translation candidates from returned  snippets. After incorporating the frequencydistance model, correct translations have the  maximum frequency-distance weights and are  more likely to be selected as the top hypothesis.  Therefore the combined model obtained the highest translation accuracy.  4.3 Overall Translation Quality   The overall translation qualities are listed in Table  3, where we showed the translation accuracies of     No Hints  (Inc = 44.19%)  With Hints (Inc = 95.16%)  Table 3. Overall translation accuracy  the top 5 hypotheses using different number of  snippets. A hypothesized translation was considered to be correct when it matched one of the reference translations.  Using more snippets always  increased the overall translation accuracy, and with  all the 165 snippets (on average per query), our  approach achieved 80% top-1 translation accuracy,  and 90% top-5 accuracy.  We compared the translations from a research  statistical machine translation system (CMU-SMT,  Vogel et al 2003) and a web-based MT engine  (BabelFish). Due to the lack of topic-relevant contexts and many OOV words occurring in the source  key phrases, their results were not satisfactory. We  also compare our system with LiveTrans, which  only searched within English web pages, thus with  limited search space and more noises (incorrect  English candidates). Therefore it was more difficult to select the correct translation. Table 4 lists  some example key phrase translations mined from  web corpora, as well as translations from the BabelFish.   5 Relevant Work  Both (Cheng et al 2004) and (Zhang and Vines  2004) exploited web corpora for translating OOV  terms and queries. Compared with their work, our  proposed method differs in both webpage search                                                             7 http://babelfish.altavista.com/  Features (avg. snippets =  10)  (avg. snippets=130)  Trl 51.45 17.97  Trans 51.45 40.68  Fq-dis 53.62 73.22  Trl+Trans 63.04 51.36  Trl+Trans+  Fq-dis 65.94 86.73  Accuracy of the Top-N Hyp. (%) Snippets  Used Top1 Top2 Top3 Top4 Top5  10 46.1 55.2 59.0 61.3 62.3  20 57.4 64.2 69.7 72.6 72.9  50 63.2 74.5 77.7 79.7 80.6  100 75.2 84.5 85.8 87.4 87.4  165 80.0 86.5 89.0 90.0 90.0  BabelFish7 MT 31.3 N/A N/A N/A N/A  CMUSMT 21.9 N/A N/A N/A N/A  LiveTrans (Fast) 20.6 30.0 36.8 41.9 45.2  LiveTrans (Smart) 30.0 41.9 48.7 51.0 52.9  488                                                                              Figure 3. Inclusion rate vs. the number of snippets used    Examples Category  Chinese Key Phrase Web-Mining Translation BabelFish? Result  Movie   Title ????  the Bridges of Madison  County *Love has gone and only good  memory has left in the dream  Book   Title ????? Sense and Sensibility *Reason and emotion  Organization  Name  Woodrow Wilson National  Fellowship Foundation  *Wood the Wilson nation gets together the foundation  ?????????? ??  Person  ???? Seiji Ozawa *Young Ze drafts you Name  Location  Name ????? Tsaidam Basin Qaidam Basin  Company / ?? Clinique *Attractive blue Brand  Sci&Tech  Terms ????? Bayesian network *Shell Ye Si network  Specie  ?? walrus walrus Term  Military  Term ????? stratofortress stratofortress  Medical  Term ??? glaucoma glaucoma  Music  ??? bassoon bassoon Term  Sports  ?????? Km Tour de France *Link law bicycle match Term  *: Incorrect translations    Table 4. Key phrase translation from web mining and a MT engine    489 space and translation extraction features. Figure 4  illustrates three different search strategies. Suppose  we want to translate the Chinese query ?????.  (Cheng et al 2004) only searched 188 English web  pages which contained the source query, and 53%  of them (100 pages) had the correct translations.   (Zhang and Vines 2004) searched the whole  55,100 web pages, 10% of them (5490 pages) had  the correct translation. Our approach used query  expansion to search any web pages containing ?? ??? and English hint words, which was a larger  search space than (Cheng et al 2004) and more  focused compared with (Zhang and Vines 2004),  as illustrated with the shaded region in Figure 4.  For translation extraction features, we took advantage of machine transliteration and machine translation models, and combined them with frequency  and distance information.                           Figure 4. Web search space strategy comparison  6 Discussion and Future Work  In this paper we demonstrated the feasibility of  the proposed approach by searching for the English  translation for a given Chinese key phrase, where  we use punctuations and Chinese words as the  boundary of candidate English translations. In the  future we plan to try more flexible translation candidate selection methods, and apply them to other  language pairs. We also would like to test our approach on more standard test sets, and compare the  performance with other systems.   Our approach works on short snippets for query  expansion and translation extraction, and the computation time is short. Therefore the search engine?s response time is the major factor of  computational efficiency.      "'), ('text2', b'b\'1 In t roduct ion   Nominal compound analysis is one of crucial  issues that have been continuously studied by  computational nd theoretical linguists. Many  linguists have dealt with nonlinal compounds  in view of semantic interpretation, and tried to  explain how nominal compounds are semanti-  This work was partially supported by a KOSEF\\\'s post-  doctoral fellowship grant.  cally interpreted (Levi, 1978; Selkirk, 1982). In  the field of natural anguage processing, various  computational models have been established for  syntactic analysis and semantic interpretation  of nominal compounds (Finin, 1980; McDon-  ald, 1982; Arens ct al. , 1987; Pustejovsky  et al , 1993; Kobayasi et al , 1994; Van-  derwerde, 1994; Lauer, 1995). Recently it has  been shown that noun phrase analysis is effec-  rive for the improvement of the application of  natural anguage processing such as information  retrieval (Zhai, 1997).  Parsing nominal compound is a basic step  for ~11 problems related to it. From a brack-  eting point of view, structural ambiguity is also  a main problem in nominal compomld analysis  like in other parsing problems. Re(:ent works  have shown that the corpus-b;~sed approach for  nominal compound analysis makes a good re-  sult to resolve the ambiguities (Fustcjovsky et  al. , 1993; Kobayasi et al , 1994; Lauer, 1995;  Zhai, 1997).  Lauer (1995) has compared two diffbrent  models of corpus-based approaches fbr nomi-  nal compound analysis. One was called as the  adjacency model which was inspired by (Puste-  jovsky et al , 1993), and the other was re-  ferred to as the dependency model which was  presented by Kobayasi ~t al. (1994) 2 and  Lauer (t995). Given a nominal compound of  three nouns n~\\\'-.2\\\'a:~, let A.s. t)e a metric used to  evaluate the association of two nouns. In the  adjacency model, if A.~(\\\',,l:\\\',J.2) > A.s(n2,n3),   then the structure is determined as ((\\\'hi \\\'n2) n3).  Otherwise, (\\\'nl (\\\',l,~ \\\'n:,)). On the other hand, in  2In their work, the structure is determined l)y com-  paring the multiplication of the ~ssociations between all  two nOuns, that is, by comImring A,s(\\\'..t, \\\'n2)A.s(n2, n3)  and AS(nl, n3) As (n2, \\\',l.:~). It m~tkes similar results to the  dependency model.  292  tim dClmn(h,,ncy model, the decision is det)en-  dent on the association strength of nt for \\\'rt2 and  \\\',,::. That is. the left branching tree ((at \\\'n2) ha)  is constructed it" A.s(nt,\\\'u2) > As(at ,ha) ,  and  I:he right branching tree (\\\'nL (n2 \\\'ha)) is made,  ~M,,,rwise. Lauer (1995) has claimed that the  ~h\\\',lmndency model makes intuitive sense and  i)r~)duces t)(,,tter results.  In this paper, we propose a new model tbr  ~)minal comt)ound analysis on the basis of  w()rd (:o-()(:cui\\\'ren(;(?s and grannnatical rela-  ti(mshil)s ilnmanent in nominal (:ompounds.  Tim grammatical relation can sometimes  ma,k(,, the (tisnmbiguationmore precise as  wo, ll as it gives a clue of the nonfinal in-  l.(Ul)r(\\\'Iation. For example, in the nominal  (:~nnl)ound "KYEONG JAENG (competition)  YUBALa(bringing about) CHEJE(system)"  whi(:h meallS system to bring about competition,  tim nominal conlpound "KYEONGJAENG  Cl-tEJE((:oml)etition system)" co-occurs much  more fl\\\'equently titan "KYEONGJAENG  YUBAL(bringing about competition)". How-  o.w;r, its structure is selected to be \\\\[\\\\[KYEONG-  .IAENG YUBAL\\\\] CHEJE\\\\]. Why it is analyzed  in such a way can be shown easily by trans-  li)rming the nominal compound to the clause.  Because "YUBAL(bringing about)" is the  predicatiw,, noun that derives the verb with the  1)redicative suffix attached, the modifying noun  phrase can be transformed to the corresponding  VP which has the meaning of "to bring about  competition" (Figure 1). The verb "YUBAL-  HA-NEUN(to bring about)" in VP takes the  "KYEONG,lAENG(competit ion)" as the ob-  .iect. The predicative noun "YUBAL(bringing  about)" also subcategorizes a noun phrase  "KYEONGJAENG(competit ion)" in the same  rammer as the verb. In the right syntactic  tree of Figure 1, it should be noted that the  object of a verb does not have the dependency  ,elation to the noun outside the maximal  1)rojection of its head, VP. Likewise, the object  "KYE()NGJAENG(competit ion)" does not  have a,ny dependency with the other noun  over the predicative noun "YUBAL(bringing  a,t)out)".  :WUBAL is a noun in Korean which means to cause  t,o bring about something  2 St ructure  o f  Nomina l  Compound  There is not any adjective derivation in Ko-  rean. Rather, a noun itself plays an adverbial  or adjective role ill a nominal compound, or  modifies other noun with possessive postposi-  tion attached. Table 1 shows various relations  occurred in nominal compounds.  As shown in the example, there is a rela-  tionship between two nouns which have de-  pendency relation in a nominal compound.  For instance, the first nominal compound  in the example expresses compound mean-  ing of individual nouns, i.e. the attribute  that a .file has. On the other hand, in  the example (c) of the example, the noun  "GAENYEOM(concept)" is the object of the  predicative noun "GUBUN(discrimination)". A  nominal compound, as such, often has the  similar structure to a simple sentence, e.g.  complement-predicate structure, as well as  representing compound meaning with several  nouns combined.  Many researchers have tried to explain con-  straints given in tile process of word combi-  nation and the principle of semantic compo-  sition. Levi (1978) has tried to find the se-  mantic constraints which govern the combina-  tion of each noun in a nominal compound.  Sproat (1985) has taken into consideration the  predicate-argument relation of nominals on the  basis of generative syntax. He explained that  the nominalization suffix nominalizes the syn-  tactic category ofa verb, but 0 role of the verb  is percolated into its parent node.  We claim that the nominalization is the phe-  nomenon occurred at the syntactic level, and  hence the syntactic relations hould be reflected  in nominal parsing. Namely, tbr accurate nomi-  nal compound parsing, we need syntactic knowl-  edge about nominal compound in addition to  lexical information about lexical selection. We  propose a nominal parsing model based on two  relations, which can be immediately applied to  nominal interpretation. We classi(y the syntac-  tic relations in a nominal compound as tbllows:  modifier-head re la t ion  One noun (adnomi-  nal, adjective) adds n certain meaning to  the other noun (head) producing a com-  pound meaning (1, 2 in Table 1).  complement-predicate r la t ion  One is the  293  NP NP  NP NP ," . . . .  - _ _ .  VP NP  _ ~ _  CHFF.JE ,\\\' ~ CHEJE  - . (system) . \\\' \\\' - -  - .  (system) - - . .   ," NP(obj) NP . -~  . . . . . .  ~4 P(obj\\\\] . . . .  V- " - :\\\'?2z-z.~  t /\\\' KYEONGJAENG YUBAL I t q0 KYEONGJAENG YUBAL ,  ~ (competition) (bringing about) / \\\' .  subj (competition) (bringing about) ,\\\'  Figure 1: Example shows that syntactic relations have influence on deternfining the structure of a  nominal compound  nominal compound meaning  PA\\\'IL(file) SOGSEONG(attr ibule)  GIBON(basis) GAENYEOM(concept)  GAENYEOM(concept)  GUBUN (discrimination)  DAETONGRYEONG(president)  DANGSEON(being elected)  GONGDONG (working together) BEONYEOG(translat ion)  file attr ibute  basic concept  discrimination of concept  being elected to president  to translate together  Table 1: Role of modifying noun in nominal compomM  complement (subject, object, adverb) of  the other noun (predicative noun) in a  nominalcompound (3, 4, 5 in Table 1).  When considering the complement-  predicate relation, we can figure out  some syntactic constraints imposed on  nonfinal compounds. For example,  in "PA\\\'.IL(file) SOGSEONG(attr ibute)  BYEONKYEONG (change)",  "SOGSEONG(attr ibute)" is the object of the  predicative noun "BYEONKYEONG (change)".  It can be expanded to a sentence like "X changes  the .file attribute". In other words, the syntactic  lewfls of two phrases "PA\\\'IL SOGSEONG(fi le  ~ttribute)" and "BYEONKYEONG(change)"  in the compound noun are different, where  one is NP and the other is VP. That the  syntactic levels (i.e. syntactic categories) of  nominal compounds are different means that  the different method is required for the proper  a,nalysis of their structures.  Next, a predicative noun does not subcate-  gorize more than two nominals with the same  granunatical cases. For instance, a predicative  norm in a nominal compound governs either a  subject or an object at most. The situation is w-~ry sinfilar to that occurred in a sentence. In  this paper, this is called one case per sentence,  which means that a predicative noun cannot  subcategorize two nouns of the same grammat-  ical cases when the relations of nominals can be  expanded to a sentence.  3 Acqu i r ing  Lex ica l  Knowledge  We collect lexical co-occurrence instances from  corpus in order to get knowledge tor nomi-  nal compound analysis. The text material is  composed of 40 million (:ojeols of Yonsei Lex-  icographical Center corpus a.mt KAIST corpus  (330M bytes). The Korean morphoh)gi(:al ana-  lyzer, the POS tagger and the partial parser are  used to obtain co-occurreu(:es.  In order to construct linguistic lexical  data tbr nominals, we first, extracted verb-  noun CO-OCcur rence  ( |a ta  f i \\\' on l  ( ; ( ) rpus  using  the partial parser. A noun is c(mnected  to a verb with a synta(:ti(: relation, and  the co-occurrences are re,1)rescnted t)y triples  (verb, nou\\\'n,, syntactic rda, t\\\'io\\\'H,). The postpo-  sitions are reposited in tit(,, syntactic relation  feld in order to represent the syntacti(: relations  which might o(:cur tmtween two nouns. Nom-  inal pairs with (:omplenmnt-predi(:ate relation  are derived fl\\\'om the data extracted.  Predicative nomls l)e(:()me vexbs with  the verbalization suffix such as \\\'-HA-\\\' at-  tached. For exampl(,,, the predicative noun  \\\'KEOMSAEK(retrieva.1)\\\' is verbalized to  \\\'KEOMSAEK-HA(retrieve)\\\'  1)y adding  the suffix \\\'-HA-\\\'. Theretbr(~, we (:an get  294  c~mq)lement-predicate relations by reducing  w;rl)s to predicative nouns with cutting, if  ;my, the verbalization suffix. Table 2 shows  s(Hne llOun-nouIl co-occurrence xamples of  ,omplement-predicate relation derived in that  way.  Second, co-occur rences  co lnposed  of  only two  1,orals (complete nominal compound) were ob-  rained. In Korean, complete nominal com-  IT(rends arc extracted in the tbllowing way. Let  us suplmse that N, NA,  NP  be the set of nouns,  the set of nouns with tile possessive postposi-  ,:ion, and the set of nouns with a postposition  ~xcept he possessive postposition, respectively.  ? For eojeols et,e2,e3, where el ? N U  NA,  e2 E NUNA,  e3 E NP ,  count (n2, ha),  where \\\'r~,2 and n3 are tile nouns that belong  to e~ and e:~ respectively.  The data could contain two relations e.g.  modifier-head relation and complement-head re-  lation. Therefbre, we manually divide them into  two classes by hand according to the relation.  Many erroneous pairs could be removed by the  ma,nual process. Furthermore, we manually as-  sign to each nominal pair syntactic relations  such as SUB J, OBJ and ADV since the syn-  ta(:tic relation does not explicitly appear from  Ira.its obtained in the second (Table 3), Actually,  there is it() immanent syntactic relationbetween  two nouns of modifier-head relation. On the  other hand, some syntactic relation such as case  marker and adverbial relation can be given to  two nouns with complement-predicate relation.  Some examples are given in Table 3. The data  of complement-head relation are merged with  those established with the partial parser, which  are complement-head co-occurrences. The rest  of the data have modifier-head co-occurrences.  Consequently, the complement-predicate co-  occurrence is represented with a triple {comp-  \\\',,o\\\'wn,, pred-noun, syn-rel) as shown in Table 2.  Syntactic relation is described with postposition  tbr case mark or ADV in Korean. The syntactic  relation is not given to the modifier-head co-  occurrence.  In the corpus based approach for natural an-  guage processing, we should take into consider-  ation the data sparseness problem because the  data do not contain whole phenomena of the  language in most cases. Ma~W researchers have  proposed conceptual asso(:iation to ba(:k off the  lexical association on the assumption that words  within a (;lass behave similarly (Resnik, 1993;  Kobayasi et al , 1994; Lauer, 1995). Namely,  word classes were stored instead of word co-  occurrences.  Here, we must note that predicates does  not act according to their semantic category.  Predicates tend to have wholly different case  frames ti\\\'om each other. Thus, we stored  individual predicative nouns and semantic  classes of their arguments instead of each  semantic lass tor two nouns: In effect, given  a word co-occurrence pair (\\\'nl,\\\'n2) and, if any,  a syntactic relation s, it is transfbrmed and  counted in the fbllowing way.  1. Let ci be the thesaurus class which ni belongs to.  2. I f  (nl ,n2) are a pair in eo-occurrences of  complement-predicate relation  3. Then  4. For each ci which nl belongs to,  5. Increase the \\\\]~\\\'equency of (ci, \\\'n2, s) with the count  of (~1, n~).  (Here, ,s is an immanent syntactic relation)  6. Else  7. For" each class ci and c i to which \\\'n~ and n2 belongs  respectively,  8. Increase the .#\\\'equency of (ci, cj) with the count of  (n~,,~)  Consequently, we built two knowledge sources  with different properties, so that we needed to  make the method to deal with them. In the next  section, we will explain the effective method of  analysis based on that different lexical knowl-  edge.  4 Nominal Compound Analysis  In order to make tile process efficient, the ana-  lyzer identifies the relations in a nominal com-  pound, if any, which can be the guideline of  phrase structuring, and then analyzes the struc-  tures based on the relations.  Figure 2 shows an example of the phrase  structure of a nominal compound to include the  complement-predicate relation. We showed that  the nominal compound with the complement-  predicate relationcan be expanded to a sim-  ple sentence which contains NPs and VP. This  means again that the nonfinal compound with  295  argument predicative noun syntactic relation  GAENYEOM(concept) YEONGU(study) OBJ   GYEONJEHAG(eeonomics) YEONGU(study) OBJ   GWAHAGJA(scientist) YEONGU(study) SUBJ  Table 2: Noun-noun co-occurrence xamples derived from lexical data of predicate YEONGU-  HA(research)  first noun second noun immanet syntactic relation (meaning)  DAMBAE(tobacco) GAGE(store)  CHARYANG (car) GAGYEOG(price)  GEUMSOG(meta l ) .  GAGONG(process) OBJ(process metal)  WANJEON(wholeness) GADONG(operation) ADV(operate wholly)  Table 3: Examples  the complement-predicate relation can be di-  vided into one or more phrasal units which we  (:all inside phruse.  The nonfihal compound in Figure 2 has three  inside phrases - NPsuBJ,  NPoBJ and V. Some  nonfinal compounds may not have any inside  phrase. Besides, the structure in each inside  phrase can be determined by the word co-  occurrence based method presented by Lauer  (1995) and. (Kobayasi et al , 1994), i.e. only  statistical association.  4.1  Assoc ia t ion  between nouns   Inside phrases can be detected based on the  association, since two nouns associated with  the complement-predicate relation indicate exis-  tence of an inside phrase. We distinguish the as-  sociation relation by discriminating knowledge  source. Thus  the associations are calculated in  a different way as follows. Here, ambi(n) is  the number of thesaurus classes in which n ap-  pears, and Nc\\\'p and NMH are the total number  of the complement-predicate nd the modifier-  head co-occurrences- respectively.  . Complement-Predicate  The association can be computed based  (m the complement-predicate r lations  obtained from complement-predicate co-  occurrence data. It measures the strength  of statistical association between a noun,  \\\'At, and a predicative noun, n.2, with a given  syntactii~ relation s which is the syntactic  relation like subject, object, adverb. Let ci  1)e categories to which nl belongs. Then,  the degree that nl is associated with n2 as  of two nouns  analyzed  .  the complement of n2 is defined as tbllows:  Assoccp (?t,1, n2)  -.~ 1 freq(ci, \\\'n2) (1) ?  i  Modifier-Head  The association of two nouns is estimated  by the co-occurrences wlfich were collected  for the modifier-head relation. In the sim-  ilar way to the above, let ci and qj be the  categories to which \\\'n, and \\\'n2 belongs re-  spectively. Then, the association degree of  nl and n2 is defined as tbllows:  ASSOCMH(ni,n2)-- 1 ?Z freq(ci,cj)  NMH . a\\\'m, bi(nl )ambi(n2)  (2)  The syntactic relation is deternfined by the  association. If\\\' the association between two  nouns can be computed by the t\\\'ornnfla 1,  the complement-t)redicate relation is given to  the nouns. If not, the relation of two nouns  is simply concluded with the modifier-head  relation. We canrecognize the syntactic  relation inside a nominal (:Oml)OmM by the  association involved. In order to distinguish  the associations in accordance with the rela-  tions, the association is expressed by a triple  (relation, (sy\\\'n-\\\'re, l, v.,l\\\'u,e.)}. Tim relation is  chosen with CP or MH a~:c:ording to the fi)rmula  used to estimate the a.ssocia.tion. If \\\'relation is  CP, the syn-\\\'rc, l has a,s its va.lue SUB J, OBJ,  ADV etc., which arc given by co-oc~:urrence  data acquired. ()therwise, (/) is assigned. Lastly,  the value is computed by the tbrnnfla. The  association is estimated in the tbllowing way,  296  I ! s  \\\\] _~- VP  / -  -t f~_-~ ~_   i NPsuBJ NPoB J V  I  NP  NPsuBa NPom NP v  ,,\\\'" SAYONGJA-YI", ,."\\\'FILE SOKSEONG",, ,\\\'" BYEONKYEONG\\\'"  ".. (of user) ./" "- .  (file) (attribute).." ".. (change) ..\\\'  Figure 2: Example of the phrase structure of a nominal compound  l,h(:r(:fl)re:  ff A.~,~o(:c.,p(, l,"~,\\\'2) > 0  As.s,,(:(.,,.,, , ..2) = (CP,(.W,,n-rel, Assoecp (n,, ,,.2)))  ( \\\'./,,\\\'i ( \\\',  .4.s.so(:(,,,,, ",\\\'2) = (MH,(?, ASSOeMH(nl, n2)))  If no co-occurrence data for a nominal  (:Oml)ound are fbund in both databases, the  modifier-head relations is assumed and the left  association is favored tbr unseen data. The  lm;ti-wence of left association is reasonable tbr  I)ra.cketing of nonfinal compounds since the left  associations occupy the bracketing patterns  lnuch more than the right associations as shown  in Ta,l)le 6.  4.2  Pars ing   Since the head always tbllows its complement in Korean, the ith noun in the nominal compound  consisting of n nouns has head candidates of  ,,,- i that it might be depend on, and the parser  selects the most probable one from them. The  parser determines the head of a complement by  a,n association degree of head candidates for the  complement.  The easiest way is to have the head candi-  date list sorted on the association, and select  most strongly associative one. In the process of  selection, the tbllowing constraints are imposed  if the relation of two nouns is complement-  predicate(CP). Given a nominal compound of  three nouns  (?~, 1., \\\'//,2, ha),  ? If (n2, ha) are related with CP and the syn-  tactic relation of (",2, ",:3) is the same as that  of (nl, ha), then "~,l is not dependent on n3.  This is called one case per sentence con-  straint.  If nl has an association with n2 by CP rela-  tion, it does not have dependency relation  with ha. See Figure 1  I f  n2 plays an adverbial role tbr ha, then n,  is not linked with rt,2.  Cross dependency is not allowed. It means  that dependent-head relations do not cross  each other.  As an example, giventhe nominal compound  "iDAEJUNG(public) ~MUNHWA(culture)  aBIPAN(criticism)", we can get the association  table as shown in Table 4. According to  the table, the first and second noun can be  linked with the modifier-head relation and  the association degree of 0.00021. The second  noun can depend on the third noun with  the complement-predicate relation, and the  association degree is 0.00018. Furthermore,  the argument is inihrred to the object of the  predicate, which can be easily recognized by  the co-occurrence data extracted.  The table is sorted on the association so that  the parser can easily search tbr the probable  candidate for head. In order to effectively de-  tect inside phrases and check the constraints,  the syntactic relation should be checked prior to  the comparison of the association value. That  is, the first key is the rdal: ion and the second,  associat ion value. Thus, CP > MH, and the  297  2 3  (MH,  (?, 0.00021)) (CP, (OBJ, 0.00014))  (CP, (OBJ,  0.00018))  Table 4: Association table(AT) for the example nominal compound "DAEJUNG MUNHWA BI-  PAN"  association values are compared in case of the  sanle rvlation value.  As a consequence, the association table is  actually implemented to the association list as  follows:  \\\\ [DAE JUNG (public)\\\\]- (3,OBJ, ( CP, O.O0014))  (2,?,(MH,0.00021))  \\\\ [MUNHWA (culture)\\\\]- (3,OBJ, (CP,0.00018))  From the list we know it is probable  that the noun "DAEJUNG(public)" is depen-  dent on "BIPAN(criticism)" with OBJ  rela-  tion. On the other hand, two words "DAE-  JUNG(public)" and "MUNHWA(culture)" are  tbund in modifier-head co-occurrences and thus  associated with the modifier-head relation.  Then, the parsing process can be defined as fol-  lows:  h, ead( n,: ) = \\\'at (3)  l = index( max (Assoc(ni, nj)))  j= i+ l,...,k  Here index returns the index of noun nl  whose association with ni is the maximum.  Namely, the parser tries to find the following  candidate tbr the head of each noun ni in a nom-  inal compound consisting of k nouns, and make  n link between them. If constraints are violated  while parsing, the next candidate of the list is  considered by the parser. According to the al-  gorithm, the given example is parsed as follows:  . There is only one candidate for  "MUNHWA". "MUNHWA(culture)" has  the dependency on "BIPAN(criticism)"  with object relation. The fact that there  is tim complement-predicate relation  lmtween two nouns indicates that those  are the elements of inside phrases, where  one belongs to NP and the other has the  property of VP. The inside phrases are  detected by the syntactic relation.  2. The most probable candidate of  "DAEJUNG(public)" is also "BI-  PAN(criticism)", but it violates one  case per sentence since the predicative  noun already took theobject. Thus,  another candidate is taken.  3. The next head candidate  "MUNHWA(culture)" is satisfactory  to the constraints as the modifier-head  relation, and "DAEJUNG(publ ic)" is  linked to "MUNHWA(culture)" with the  relation.  5 Exper imenta l  Resu l t s   For experiments, we collected 387 nominal com-  pounds fronl a million word corpus. Nominal  compounds conlposed of more. than tbur nouns  (a series of 5 nouns or more) are excluded be-  cause the number of them is too small to eval-  uate our system.  Some examples of analysis are shown in Table  5. In the table, the modifier-head relation is  represented with MH, and the complement-  predicate is described with OBJ  and SUBJ  that means object and subject respectively. No  depedency between nouns is marked with \\\'-\\\'  For instance, the modifier-head relation is as-  signed to "MUSOG SINANG" which have the  meaning of the religion o.f private society that  is traditional and s\\\'alJerstitio\\\'as. However, we  don\\\'t know about the semantic relation hidden  in the results analyzed. In addition, the nom-  inal compound "JISIK\\\'IN-YI(intellex:tual\\\'s)  CHAEK\\\' IM (responsibility) HOIPI(evasion)"  means that the intellectual evades h, is responsi-  bility. Actually, its structure is determined as  \\\\[JISIK\\\'IN-YI,s,,t~./ \\\\[CHAEK\\\'IMot~./ HOIPIv\\\\]\\\\]  which can be ext)anded to a, siml)le sentence.  Bracketed patterns of the example uonfinal  conlpounds are shown in %tble 6. According to  the table, the baseline a.ccm\\\'acy of the default  system is at least 73.6%. As shown in Table 7,  the precision fi)r nnalysis of nominal comt)ounds  298  nominal compounds(n1, n2, ha) structure R.(n,.,\\\'n,2) iI~(v,,,\\\'n:~) /~(\\\'n2,na)  MUSOG SIN\\\'ANG JEONTONG ((nl n2) n3) MH MH  (private society, religion,tradition)  DAEJUNG MUNHWA BIPAN ((nl n2) n3) MH OBJ  (public, culture, criticism)  FRANCE KEUNDAE MUNHAG (nl (n2 n3)) MH MH  (France, modern, literature)  .I\\\\[SIK\\\'IN-YI CHAEK\\\'IM HOIPI (nl (n2 n3)) SUB.I OBJ  (intelh;(:tual\\\'s, responsibility, evasion)  Ta.lfle 5: Examples of some nominal compound analyses, R(n,z, \\\',,~) is the, synta.ctic relation between  ", i  a,n(1%; identified  if- of n(mns in NP pattern fl\\\'eq  (nl-YI (n2 n3))  ((n>Y~ n2) n3)  ((nl n2) n3)  (nl (n2 ha))  (nl-YI (n2 (n3 n4)))  ((nl-YI (n2 n3)) n4)  (((nl-YI n2) n3) n4)  (nl-YI ((n2 n3) n4)  ((nl ng.) (ha n4))  (((nl n2) n3)n4)  (nl ((n2 n3) n4))  ((nl (n2 n3)) n4)  (nl (n2 (n3 n4)))  54  31  189  41  2  10  4  6  9  32  Ta,lfle 6: the patterns of nominal compound  s(;ru(;t;ures  ot\\\' the length three and four is about 88.3% and  66.3%. The result is fairly good in that nomi-  ~m.1 compounds of length three occur much more  t\\\'requently than those of length four. Overall  I~recision of analysis is about 84.2%.  In addition,we compared three different mod-  els to evaluate our system - default model by the  dominant pattern, dependency model presented  by Kobayasi et al (1994) and Lauer (1995),  a.nd our model. In the default analysis, nomi-  ha.1 compounds were bracketed by the dominant  pa,tterns hown in Table 6. For the dependency  model, we used the method presented by Lauer  (1995).  Table 8 shows the comparison of the results  produced by our algorithm and the other two  methods. Our system made a better result  in the disambiguation process. The results  show that the syntactic information in nomi-  hal phrases plays an important role in deciding  their structures.  However, there are still errors produced.  Some nouns has the word sense ambiguity, and  are used as both predict~tive noun and com-  mon noun. Because of the sense ambiguity,  some modifier-head relations are misrecognized  to complement-predicate. Other errors contain  the same kind of results as (Latter, 1995). To  overcome the errors, we think that semantic re-  lations immanent in two nouns are considered. 6 Conc lus ion   Many statistical parsers have not taken care of  analysis of nominal compounds. Furthermore,  many researches which dealt with nominal com-  pound parsing seemed not to have computa-  tional approaches tbr linguistic phenomenon i nominal compounds.  We proposed Korean nominM compound  analysis based on linguistic statstical knowl-  edge. Actually, immanent syntactic relations  like subject and object as well as structures  of nominal compounds arc identified using our  nominal compound analyzer and knowledge ac-  quisition method. Syntactic relations identi-  fied can be effectively used in semantic inter-  pretation of nominal compound. Moreover, the  parser was more accurate by using linguistic  knowledge such as structural information and  syntactic relation immanent in nouns.  It is expected that our parsing results in-  cluding identification of syntactic relations are  useful for the application system such as infor-  mation extraction because many nominal com-  pounds are contained in Korean document bod-  ies and titles, which often represent some events.  However, the system still has some difficul-  299  # of nominal compounds # of success I precision  3 nouns 315 278 88.3  4 nouns 72 48 \\\\[ 66.3  total 387 326 84.2  Table 7: Overall results of nominal compound analysis  total  # of success precision  3 nouns   precision  4 nouns   precision  (1) 285 73.6 77.1 58.3  (2) 315 81.4 85.4 63.9  (3) 326 84.2 88.3 66.3  Table 8: Results of nominal compound analysis (1) default analysis by pattern (2) results using the  dependency model (3) results using our algorithm  ties, which caused erroneous results. In the fu-  ture work, we feel it is necessary that lexical  I)arameters be transformed into conceptual pa-  rameters with large size of semantic knowledge,  and filrther studies on linguistic properties of  nominals be made. \'')])
INFO:tensorflow:Finished getting dataset.
I0712 13:01:23.647305 140680526767936 input_pipeline.py:91] Finished getting dataset.
I0712 13:01:23.647524 140680526767936 input_pipeline.py:94] Using char-level/byte dataset..
I0712 13:01:23.771517 140680526767936 train.py:106] Vocab Size: 257
I0712 13:05:06.017722 140680526767936 checkpoints.py:242] Found no checkpoint directory at trained_models/matching/longformer
I0712 13:05:06.722120 140680526767936 train_utils.py:370] Starting training
I0712 13:05:06.722313 140680526767936 train_utils.py:371] ====================
2022-07-12 13:08:45.199360: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:62] Constant folding an instruction is taking > 1s:

  compare.2251 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-07-12 13:08:45.335122: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:131] The operation took 1.135925025s
Constant folding an instruction is taking > 1s:

  compare.2251 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
I0712 13:09:11.666092 140680526767936 train_utils.py:377] train in step: 0
I0712 13:09:11.694744 140680526767936 train_utils.py:377] train in step: 1
I0712 13:09:11.915429 140680526767936 train_utils.py:377] train in step: 2
I0712 13:09:12.267442 140680526767936 train_utils.py:377] train in step: 3
I0712 13:09:12.618506 140680526767936 train_utils.py:377] train in step: 4
I0712 13:09:12.968683 140680526767936 train_utils.py:377] train in step: 5
I0712 13:09:13.322250 140680526767936 train_utils.py:377] train in step: 6
I0712 13:09:13.677310 140680526767936 train_utils.py:377] train in step: 7
I0712 13:09:14.020400 140680526767936 train_utils.py:377] train in step: 8
I0712 13:09:14.371422 140680526767936 train_utils.py:377] train in step: 9
I0712 13:09:14.721787 140680526767936 train_utils.py:377] train in step: 10
I0712 13:09:15.072290 140680526767936 train_utils.py:377] train in step: 11
I0712 13:09:15.428019 140680526767936 train_utils.py:377] train in step: 12
I0712 13:09:15.777445 140680526767936 train_utils.py:377] train in step: 13
I0712 13:09:16.125048 140680526767936 train_utils.py:377] train in step: 14
I0712 13:09:16.476816 140680526767936 train_utils.py:377] train in step: 15
I0712 13:09:16.829052 140680526767936 train_utils.py:377] train in step: 16
I0712 13:09:17.179908 140680526767936 train_utils.py:377] train in step: 17
I0712 13:09:17.532470 140680526767936 train_utils.py:377] train in step: 18
I0712 13:09:17.886313 140680526767936 train_utils.py:377] train in step: 19
I0712 13:09:18.235003 140680526767936 train_utils.py:377] train in step: 20
I0712 13:09:18.584774 140680526767936 train_utils.py:377] train in step: 21
I0712 13:09:18.935411 140680526767936 train_utils.py:377] train in step: 22
I0712 13:09:19.286525 140680526767936 train_utils.py:377] train in step: 23
I0712 13:09:19.637414 140680526767936 train_utils.py:377] train in step: 24
I0712 13:09:19.991128 140680526767936 train_utils.py:377] train in step: 25
I0712 13:09:20.344555 140680526767936 train_utils.py:377] train in step: 26
I0712 13:09:20.690210 140680526767936 train_utils.py:377] train in step: 27
I0712 13:09:21.041713 140680526767936 train_utils.py:377] train in step: 28
I0712 13:09:21.392279 140680526767936 train_utils.py:377] train in step: 29
I0712 13:09:21.744239 140680526767936 train_utils.py:377] train in step: 30
I0712 13:09:22.094376 140680526767936 train_utils.py:377] train in step: 31
I0712 13:09:22.447444 140680526767936 train_utils.py:377] train in step: 32
I0712 13:09:22.799632 140680526767936 train_utils.py:377] train in step: 33
I0712 13:09:23.150340 140680526767936 train_utils.py:377] train in step: 34
I0712 13:09:23.498764 140680526767936 train_utils.py:377] train in step: 35
I0712 13:09:23.849315 140680526767936 train_utils.py:377] train in step: 36
I0712 13:09:24.202801 140680526767936 train_utils.py:377] train in step: 37
I0712 13:09:24.553565 140680526767936 train_utils.py:377] train in step: 38
I0712 13:09:24.903790 140680526767936 train_utils.py:377] train in step: 39
I0712 13:09:25.254178 140680526767936 train_utils.py:377] train in step: 40
I0712 13:09:25.605278 140680526767936 train_utils.py:377] train in step: 41
I0712 13:09:25.958349 140680526767936 train_utils.py:377] train in step: 42
I0712 13:09:26.305769 140680526767936 train_utils.py:377] train in step: 43
I0712 13:09:26.658540 140680526767936 train_utils.py:377] train in step: 44
I0712 13:09:27.008085 140680526767936 train_utils.py:377] train in step: 45
I0712 13:09:27.359934 140680526767936 train_utils.py:377] train in step: 46
I0712 13:09:27.710670 140680526767936 train_utils.py:377] train in step: 47
I0712 13:09:28.062306 140680526767936 train_utils.py:377] train in step: 48
I0712 13:09:28.413812 140680526767936 train_utils.py:377] train in step: 49
I0712 13:09:28.765020 140680526767936 train_utils.py:377] train in step: 50
I0712 13:09:29.115297 140680526767936 train_utils.py:377] train in step: 51
I0712 13:09:29.465737 140680526767936 train_utils.py:377] train in step: 52
I0712 13:09:29.818207 140680526767936 train_utils.py:377] train in step: 53
I0712 13:09:30.167727 140680526767936 train_utils.py:377] train in step: 54
I0712 13:09:30.520418 140680526767936 train_utils.py:377] train in step: 55
I0712 13:09:30.869230 140680526767936 train_utils.py:377] train in step: 56
I0712 13:09:31.223504 140680526767936 train_utils.py:377] train in step: 57
I0712 13:09:31.570725 140680526767936 train_utils.py:377] train in step: 58
I0712 13:09:31.926083 140680526767936 train_utils.py:377] train in step: 59
I0712 13:09:32.273058 140680526767936 train_utils.py:377] train in step: 60
I0712 13:09:32.624260 140680526767936 train_utils.py:377] train in step: 61
I0712 13:09:32.975144 140680526767936 train_utils.py:377] train in step: 62
I0712 13:09:33.326578 140680526767936 train_utils.py:377] train in step: 63
I0712 13:09:33.677516 140680526767936 train_utils.py:377] train in step: 64
I0712 13:09:34.027939 140680526767936 train_utils.py:377] train in step: 65
I0712 13:09:34.379004 140680526767936 train_utils.py:377] train in step: 66
I0712 13:09:34.729157 140680526767936 train_utils.py:377] train in step: 67
I0712 13:09:35.080625 140680526767936 train_utils.py:377] train in step: 68
I0712 13:09:35.432422 140680526767936 train_utils.py:377] train in step: 69
I0712 13:09:35.782743 140680526767936 train_utils.py:377] train in step: 70
I0712 13:09:36.135936 140680526767936 train_utils.py:377] train in step: 71
I0712 13:09:36.488105 140680526767936 train_utils.py:377] train in step: 72
I0712 13:09:36.836627 140680526767936 train_utils.py:377] train in step: 73
I0712 13:09:37.187760 140680526767936 train_utils.py:377] train in step: 74
I0712 13:09:37.538266 140680526767936 train_utils.py:377] train in step: 75
I0712 13:09:37.889515 140680526767936 train_utils.py:377] train in step: 76
I0712 13:09:38.240664 140680526767936 train_utils.py:377] train in step: 77
I0712 13:09:38.591838 140680526767936 train_utils.py:377] train in step: 78
I0712 13:09:38.941874 140680526767936 train_utils.py:377] train in step: 79
I0712 13:09:39.294348 140680526767936 train_utils.py:377] train in step: 80
I0712 13:09:39.645087 140680526767936 train_utils.py:377] train in step: 81
I0712 13:09:39.995995 140680526767936 train_utils.py:377] train in step: 82
I0712 13:09:40.346651 140680526767936 train_utils.py:377] train in step: 83
I0712 13:09:40.697945 140680526767936 train_utils.py:377] train in step: 84
I0712 13:09:41.051480 140680526767936 train_utils.py:377] train in step: 85
I0712 13:09:41.399824 140680526767936 train_utils.py:377] train in step: 86
I0712 13:09:41.753411 140680526767936 train_utils.py:377] train in step: 87
I0712 13:09:42.102034 140680526767936 train_utils.py:377] train in step: 88
I0712 13:09:42.455311 140680526767936 train_utils.py:377] train in step: 89
I0712 13:09:42.803827 140680526767936 train_utils.py:377] train in step: 90
I0712 13:09:43.154054 140680526767936 train_utils.py:377] train in step: 91
I0712 13:09:43.506257 140680526767936 train_utils.py:377] train in step: 92
I0712 13:09:43.856853 140680526767936 train_utils.py:377] train in step: 93
I0712 13:09:44.212576 140680526767936 train_utils.py:377] train in step: 94
I0712 13:09:44.563901 140680526767936 train_utils.py:377] train in step: 95
I0712 13:09:44.915262 140680526767936 train_utils.py:377] train in step: 96
I0712 13:09:45.266422 140680526767936 train_utils.py:377] train in step: 97
I0712 13:09:45.617749 140680526767936 train_utils.py:377] train in step: 98
I0712 13:09:45.968819 140680526767936 train_utils.py:377] train in step: 99
I0712 13:09:46.319667 140680526767936 train_utils.py:377] train in step: 100
I0712 13:09:46.673798 140680526767936 train_utils.py:377] train in step: 101
I0712 13:09:47.022913 140680526767936 train_utils.py:377] train in step: 102
I0712 13:09:47.375090 140680526767936 train_utils.py:377] train in step: 103
I0712 13:09:47.727352 140680526767936 train_utils.py:377] train in step: 104
I0712 13:09:48.081657 140680526767936 train_utils.py:377] train in step: 105
I0712 13:09:48.429918 140680526767936 train_utils.py:377] train in step: 106
I0712 13:09:48.782252 140680526767936 train_utils.py:377] train in step: 107
I0712 13:09:49.137045 140680526767936 train_utils.py:377] train in step: 108
I0712 13:09:49.484649 140680526767936 train_utils.py:377] train in step: 109
I0712 13:09:49.837338 140680526767936 train_utils.py:377] train in step: 110
I0712 13:09:50.189026 140680526767936 train_utils.py:377] train in step: 111
I0712 13:09:50.540599 140680526767936 train_utils.py:377] train in step: 112
I0712 13:09:50.899870 140680526767936 train_utils.py:377] train in step: 113
I0712 13:09:51.252634 140680526767936 train_utils.py:377] train in step: 114
I0712 13:09:51.606278 140680526767936 train_utils.py:377] train in step: 115
I0712 13:09:51.956953 140680526767936 train_utils.py:377] train in step: 116
I0712 13:09:52.309767 140680526767936 train_utils.py:377] train in step: 117
I0712 13:09:52.663003 140680526767936 train_utils.py:377] train in step: 118
I0712 13:09:53.019744 140680526767936 train_utils.py:377] train in step: 119
I0712 13:09:53.378689 140680526767936 train_utils.py:377] train in step: 120
I0712 13:09:53.741780 140680526767936 train_utils.py:377] train in step: 121
I0712 13:09:54.100674 140680526767936 train_utils.py:377] train in step: 122
I0712 13:09:54.460959 140680526767936 train_utils.py:377] train in step: 123
I0712 13:09:54.817946 140680526767936 train_utils.py:377] train in step: 124
I0712 13:09:55.176066 140680526767936 train_utils.py:377] train in step: 125
I0712 13:09:55.534726 140680526767936 train_utils.py:377] train in step: 126
I0712 13:09:55.896211 140680526767936 train_utils.py:377] train in step: 127
I0712 13:09:56.252840 140680526767936 train_utils.py:377] train in step: 128
I0712 13:09:56.610660 140680526767936 train_utils.py:377] train in step: 129
I0712 13:09:56.967578 140680526767936 train_utils.py:377] train in step: 130
I0712 13:09:57.330271 140680526767936 train_utils.py:377] train in step: 131
I0712 13:09:57.686746 140680526767936 train_utils.py:377] train in step: 132
I0712 13:09:58.042796 140680526767936 train_utils.py:377] train in step: 133
I0712 13:09:58.397178 140680526767936 train_utils.py:377] train in step: 134
I0712 13:09:58.755899 140680526767936 train_utils.py:377] train in step: 135
I0712 13:09:59.114726 140680526767936 train_utils.py:377] train in step: 136
I0712 13:09:59.473702 140680526767936 train_utils.py:377] train in step: 137
I0712 13:09:59.832350 140680526767936 train_utils.py:377] train in step: 138
I0712 13:10:00.190848 140680526767936 train_utils.py:377] train in step: 139
I0712 13:10:00.550496 140680526767936 train_utils.py:377] train in step: 140
I0712 13:10:00.909662 140680526767936 train_utils.py:377] train in step: 141
I0712 13:10:01.267380 140680526767936 train_utils.py:377] train in step: 142
I0712 13:10:01.626180 140680526767936 train_utils.py:377] train in step: 143
I0712 13:10:01.984782 140680526767936 train_utils.py:377] train in step: 144
I0712 13:10:02.344480 140680526767936 train_utils.py:377] train in step: 145
I0712 13:10:02.704422 140680526767936 train_utils.py:377] train in step: 146
I0712 13:10:03.066387 140680526767936 train_utils.py:377] train in step: 147
I0712 13:10:03.423726 140680526767936 train_utils.py:377] train in step: 148
I0712 13:10:03.781340 140680526767936 train_utils.py:377] train in step: 149
I0712 13:10:04.144442 140680526767936 train_utils.py:377] train in step: 150
I0712 13:10:04.500764 140680526767936 train_utils.py:377] train in step: 151
I0712 13:10:04.860709 140680526767936 train_utils.py:377] train in step: 152
I0712 13:10:05.219479 140680526767936 train_utils.py:377] train in step: 153
I0712 13:10:05.578671 140680526767936 train_utils.py:377] train in step: 154
I0712 13:10:05.938650 140680526767936 train_utils.py:377] train in step: 155
I0712 13:10:06.303364 140680526767936 train_utils.py:377] train in step: 156
I0712 13:10:06.661800 140680526767936 train_utils.py:377] train in step: 157
I0712 13:10:07.023671 140680526767936 train_utils.py:377] train in step: 158
I0712 13:10:07.383205 140680526767936 train_utils.py:377] train in step: 159
I0712 13:10:07.754573 140680526767936 train_utils.py:377] train in step: 160
I0712 13:10:08.111700 140680526767936 train_utils.py:377] train in step: 161
I0712 13:10:08.471016 140680526767936 train_utils.py:377] train in step: 162
I0712 13:10:08.828642 140680526767936 train_utils.py:377] train in step: 163
I0712 13:10:09.187561 140680526767936 train_utils.py:377] train in step: 164
I0712 13:10:09.545866 140680526767936 train_utils.py:377] train in step: 165
I0712 13:10:09.904881 140680526767936 train_utils.py:377] train in step: 166
I0712 13:10:10.264015 140680526767936 train_utils.py:377] train in step: 167
I0712 13:10:10.632655 140680526767936 train_utils.py:377] train in step: 168
I0712 13:10:10.992523 140680526767936 train_utils.py:377] train in step: 169
I0712 13:10:11.357786 140680526767936 train_utils.py:377] train in step: 170
I0712 13:10:11.715402 140680526767936 train_utils.py:377] train in step: 171
I0712 13:10:12.075579 140680526767936 train_utils.py:377] train in step: 172
I0712 13:10:12.435084 140680526767936 train_utils.py:377] train in step: 173
I0712 13:10:12.795921 140680526767936 train_utils.py:377] train in step: 174
I0712 13:10:13.152049 140680526767936 train_utils.py:377] train in step: 175
I0712 13:10:13.510923 140680526767936 train_utils.py:377] train in step: 176
I0712 13:10:13.873238 140680526767936 train_utils.py:377] train in step: 177
I0712 13:10:14.231504 140680526767936 train_utils.py:377] train in step: 178
I0712 13:10:14.591895 140680526767936 train_utils.py:377] train in step: 179
I0712 13:10:14.954386 140680526767936 train_utils.py:377] train in step: 180
I0712 13:10:15.311063 140680526767936 train_utils.py:377] train in step: 181
I0712 13:10:15.671901 140680526767936 train_utils.py:377] train in step: 182
I0712 13:10:16.028907 140680526767936 train_utils.py:377] train in step: 183
I0712 13:10:16.390825 140680526767936 train_utils.py:377] train in step: 184
I0712 13:10:16.750259 140680526767936 train_utils.py:377] train in step: 185
I0712 13:10:17.108766 140680526767936 train_utils.py:377] train in step: 186
I0712 13:10:17.467608 140680526767936 train_utils.py:377] train in step: 187
I0712 13:10:17.827188 140680526767936 train_utils.py:377] train in step: 188
I0712 13:10:18.188547 140680526767936 train_utils.py:377] train in step: 189
I0712 13:10:18.543984 140680526767936 train_utils.py:377] train in step: 190
I0712 13:10:18.903055 140680526767936 train_utils.py:377] train in step: 191
I0712 13:10:19.262153 140680526767936 train_utils.py:377] train in step: 192
I0712 13:10:19.620878 140680526767936 train_utils.py:377] train in step: 193
I0712 13:10:19.979832 140680526767936 train_utils.py:377] train in step: 194
I0712 13:10:20.340869 140680526767936 train_utils.py:377] train in step: 195
I0712 13:10:20.696402 140680526767936 train_utils.py:377] train in step: 196
I0712 13:10:21.055944 140680526767936 train_utils.py:377] train in step: 197
I0712 13:10:21.416514 140680526767936 train_utils.py:377] train in step: 198
I0712 13:10:21.776438 140680526767936 train_utils.py:377] train in step: 199
I0712 13:10:22.134450 140680526767936 train_utils.py:377] train in step: 200
I0712 13:10:23.195286 140680526767936 train_utils.py:396] train in step: 200, loss: 0.7337999939918518, acc: 0.5024999976158142
I0712 13:14:06.179326 140680526767936 train_utils.py:411] eval in step: 200, loss: 0.9278, acc: 0.4250
I0712 13:14:06.183244 140680526767936 train_utils.py:421] Testing...
I0712 13:14:14.413068 140680526767936 train_utils.py:424] test in step: 200, loss: 0.8272, acc: 0.5150
I0712 13:14:14.452494 140680526767936 train_utils.py:377] train in step: 201
I0712 13:14:14.668727 140680526767936 train_utils.py:377] train in step: 202
I0712 13:14:15.019671 140680526767936 train_utils.py:377] train in step: 203
I0712 13:14:15.371260 140680526767936 train_utils.py:377] train in step: 204
I0712 13:14:15.722953 140680526767936 train_utils.py:377] train in step: 205
I0712 13:14:16.076004 140680526767936 train_utils.py:377] train in step: 206
I0712 13:14:16.427553 140680526767936 train_utils.py:377] train in step: 207
I0712 13:14:16.779208 140680526767936 train_utils.py:377] train in step: 208
I0712 13:14:17.130749 140680526767936 train_utils.py:377] train in step: 209
I0712 13:14:17.484282 140680526767936 train_utils.py:377] train in step: 210
I0712 13:14:17.835313 140680526767936 train_utils.py:377] train in step: 211
I0712 13:14:18.187964 140680526767936 train_utils.py:377] train in step: 212
I0712 13:14:18.542323 140680526767936 train_utils.py:377] train in step: 213
I0712 13:14:18.893634 140680526767936 train_utils.py:377] train in step: 214
I0712 13:14:19.245011 140680526767936 train_utils.py:377] train in step: 215
I0712 13:14:19.596384 140680526767936 train_utils.py:377] train in step: 216
I0712 13:14:19.950407 140680526767936 train_utils.py:377] train in step: 217
I0712 13:14:20.301166 140680526767936 train_utils.py:377] train in step: 218
I0712 13:14:20.651745 140680526767936 train_utils.py:377] train in step: 219
I0712 13:14:21.003370 140680526767936 train_utils.py:377] train in step: 220
I0712 13:14:21.354608 140680526767936 train_utils.py:377] train in step: 221
I0712 13:14:21.706653 140680526767936 train_utils.py:377] train in step: 222
I0712 13:14:22.057863 140680526767936 train_utils.py:377] train in step: 223
I0712 13:14:22.411184 140680526767936 train_utils.py:377] train in step: 224
I0712 13:14:22.763033 140680526767936 train_utils.py:377] train in step: 225
I0712 13:14:23.115915 140680526767936 train_utils.py:377] train in step: 226
I0712 13:14:23.466530 140680526767936 train_utils.py:377] train in step: 227
I0712 13:14:23.820328 140680526767936 train_utils.py:377] train in step: 228
I0712 13:14:24.169831 140680526767936 train_utils.py:377] train in step: 229
I0712 13:14:24.520750 140680526767936 train_utils.py:377] train in step: 230
I0712 13:14:24.872001 140680526767936 train_utils.py:377] train in step: 231
I0712 13:14:25.222920 140680526767936 train_utils.py:377] train in step: 232
I0712 13:14:25.574331 140680526767936 train_utils.py:377] train in step: 233
I0712 13:14:25.926348 140680526767936 train_utils.py:377] train in step: 234
I0712 13:14:26.278475 140680526767936 train_utils.py:377] train in step: 235
I0712 13:14:26.629434 140680526767936 train_utils.py:377] train in step: 236
I0712 13:14:26.983624 140680526767936 train_utils.py:377] train in step: 237
I0712 13:14:27.335398 140680526767936 train_utils.py:377] train in step: 238
I0712 13:14:27.688318 140680526767936 train_utils.py:377] train in step: 239
I0712 13:14:28.039766 140680526767936 train_utils.py:377] train in step: 240
I0712 13:14:28.391874 140680526767936 train_utils.py:377] train in step: 241
I0712 13:14:28.743154 140680526767936 train_utils.py:377] train in step: 242
I0712 13:14:29.099827 140680526767936 train_utils.py:377] train in step: 243
I0712 13:14:29.450410 140680526767936 train_utils.py:377] train in step: 244
I0712 13:14:29.802023 140680526767936 train_utils.py:377] train in step: 245
I0712 13:14:30.154576 140680526767936 train_utils.py:377] train in step: 246
I0712 13:14:30.505677 140680526767936 train_utils.py:377] train in step: 247
I0712 13:14:30.857446 140680526767936 train_utils.py:377] train in step: 248
I0712 13:14:31.209050 140680526767936 train_utils.py:377] train in step: 249
I0712 13:14:31.561512 140680526767936 train_utils.py:377] train in step: 250
I0712 13:14:31.912308 140680526767936 train_utils.py:377] train in step: 251
I0712 13:14:32.265417 140680526767936 train_utils.py:377] train in step: 252
I0712 13:14:32.617640 140680526767936 train_utils.py:377] train in step: 253
I0712 13:14:32.969117 140680526767936 train_utils.py:377] train in step: 254
I0712 13:14:33.322052 140680526767936 train_utils.py:377] train in step: 255
I0712 13:14:33.672817 140680526767936 train_utils.py:377] train in step: 256
I0712 13:14:34.027961 140680526767936 train_utils.py:377] train in step: 257
I0712 13:14:34.375401 140680526767936 train_utils.py:377] train in step: 258
I0712 13:14:34.727610 140680526767936 train_utils.py:377] train in step: 259
I0712 13:14:35.078895 140680526767936 train_utils.py:377] train in step: 260
I0712 13:14:35.433231 140680526767936 train_utils.py:377] train in step: 261
I0712 13:14:35.782026 140680526767936 train_utils.py:377] train in step: 262
I0712 13:14:36.135897 140680526767936 train_utils.py:377] train in step: 263
I0712 13:14:36.487974 140680526767936 train_utils.py:377] train in step: 264
I0712 13:14:36.842816 140680526767936 train_utils.py:377] train in step: 265
I0712 13:14:37.194472 140680526767936 train_utils.py:377] train in step: 266
I0712 13:14:37.555825 140680526767936 train_utils.py:377] train in step: 267
I0712 13:14:37.907471 140680526767936 train_utils.py:377] train in step: 268
I0712 13:14:38.263197 140680526767936 train_utils.py:377] train in step: 269
I0712 13:14:38.614168 140680526767936 train_utils.py:377] train in step: 270
I0712 13:14:38.967756 140680526767936 train_utils.py:377] train in step: 271
I0712 13:14:39.318266 140680526767936 train_utils.py:377] train in step: 272
I0712 13:14:39.671293 140680526767936 train_utils.py:377] train in step: 273
I0712 13:14:40.031264 140680526767936 train_utils.py:377] train in step: 274
I0712 13:14:40.382908 140680526767936 train_utils.py:377] train in step: 275
I0712 13:14:40.734967 140680526767936 train_utils.py:377] train in step: 276
I0712 13:14:41.090460 140680526767936 train_utils.py:377] train in step: 277
I0712 13:14:41.451216 140680526767936 train_utils.py:377] train in step: 278
I0712 13:14:41.813813 140680526767936 train_utils.py:377] train in step: 279
I0712 13:14:42.170064 140680526767936 train_utils.py:377] train in step: 280
I0712 13:14:42.539267 140680526767936 train_utils.py:377] train in step: 281
I0712 13:14:42.897989 140680526767936 train_utils.py:377] train in step: 282
I0712 13:14:43.256533 140680526767936 train_utils.py:377] train in step: 283
I0712 13:14:43.615240 140680526767936 train_utils.py:377] train in step: 284
I0712 13:14:43.974383 140680526767936 train_utils.py:377] train in step: 285
I0712 13:14:44.332910 140680526767936 train_utils.py:377] train in step: 286
I0712 13:14:44.692280 140680526767936 train_utils.py:377] train in step: 287
I0712 13:14:45.051006 140680526767936 train_utils.py:377] train in step: 288
I0712 13:14:45.413050 140680526767936 train_utils.py:377] train in step: 289
I0712 13:14:45.771498 140680526767936 train_utils.py:377] train in step: 290
I0712 13:14:46.129577 140680526767936 train_utils.py:377] train in step: 291
I0712 13:14:46.487936 140680526767936 train_utils.py:377] train in step: 292
I0712 13:14:46.847622 140680526767936 train_utils.py:377] train in step: 293
I0712 13:14:47.210178 140680526767936 train_utils.py:377] train in step: 294
I0712 13:14:47.595059 140680526767936 train_utils.py:377] train in step: 295
I0712 13:14:47.953998 140680526767936 train_utils.py:377] train in step: 296
I0712 13:14:48.319514 140680526767936 train_utils.py:377] train in step: 297
I0712 13:14:48.679251 140680526767936 train_utils.py:377] train in step: 298
I0712 13:14:49.039103 140680526767936 train_utils.py:377] train in step: 299
I0712 13:14:49.398737 140680526767936 train_utils.py:377] train in step: 300
I0712 13:14:49.758162 140680526767936 train_utils.py:377] train in step: 301
I0712 13:14:50.117861 140680526767936 train_utils.py:377] train in step: 302
I0712 13:14:50.476711 140680526767936 train_utils.py:377] train in step: 303
I0712 13:14:50.836059 140680526767936 train_utils.py:377] train in step: 304
I0712 13:14:51.195083 140680526767936 train_utils.py:377] train in step: 305
I0712 13:14:51.554356 140680526767936 train_utils.py:377] train in step: 306
I0712 13:14:51.912889 140680526767936 train_utils.py:377] train in step: 307
I0712 13:14:52.271535 140680526767936 train_utils.py:377] train in step: 308
I0712 13:14:52.631110 140680526767936 train_utils.py:377] train in step: 309
I0712 13:14:52.989663 140680526767936 train_utils.py:377] train in step: 310
I0712 13:14:53.348842 140680526767936 train_utils.py:377] train in step: 311
I0712 13:14:53.709060 140680526767936 train_utils.py:377] train in step: 312
I0712 13:14:54.067487 140680526767936 train_utils.py:377] train in step: 313
I0712 13:14:54.426411 140680526767936 train_utils.py:377] train in step: 314
I0712 13:14:54.785994 140680526767936 train_utils.py:377] train in step: 315
I0712 13:14:55.146329 140680526767936 train_utils.py:377] train in step: 316
I0712 13:14:55.505223 140680526767936 train_utils.py:377] train in step: 317
I0712 13:14:55.863906 140680526767936 train_utils.py:377] train in step: 318
I0712 13:14:56.224316 140680526767936 train_utils.py:377] train in step: 319
I0712 13:14:56.592015 140680526767936 train_utils.py:377] train in step: 320
I0712 13:14:56.952035 140680526767936 train_utils.py:377] train in step: 321
I0712 13:14:57.310385 140680526767936 train_utils.py:377] train in step: 322
I0712 13:14:57.673202 140680526767936 train_utils.py:377] train in step: 323
I0712 13:14:58.030062 140680526767936 train_utils.py:377] train in step: 324
I0712 13:14:58.388494 140680526767936 train_utils.py:377] train in step: 325
I0712 13:14:58.746752 140680526767936 train_utils.py:377] train in step: 326
I0712 13:14:59.107482 140680526767936 train_utils.py:377] train in step: 327
I0712 13:14:59.466242 140680526767936 train_utils.py:377] train in step: 328
I0712 13:14:59.829802 140680526767936 train_utils.py:377] train in step: 329
I0712 13:15:00.183731 140680526767936 train_utils.py:377] train in step: 330
I0712 13:15:00.543228 140680526767936 train_utils.py:377] train in step: 331
I0712 13:15:00.903534 140680526767936 train_utils.py:377] train in step: 332
I0712 13:15:01.262575 140680526767936 train_utils.py:377] train in step: 333
I0712 13:15:01.621605 140680526767936 train_utils.py:377] train in step: 334
I0712 13:15:01.983315 140680526767936 train_utils.py:377] train in step: 335
I0712 13:15:02.343171 140680526767936 train_utils.py:377] train in step: 336
I0712 13:15:02.703240 140680526767936 train_utils.py:377] train in step: 337
I0712 13:15:03.062973 140680526767936 train_utils.py:377] train in step: 338
I0712 13:15:03.421699 140680526767936 train_utils.py:377] train in step: 339
I0712 13:15:03.782630 140680526767936 train_utils.py:377] train in step: 340
I0712 13:15:04.142580 140680526767936 train_utils.py:377] train in step: 341
I0712 13:15:04.502048 140680526767936 train_utils.py:377] train in step: 342
I0712 13:15:04.861217 140680526767936 train_utils.py:377] train in step: 343
I0712 13:15:05.228368 140680526767936 train_utils.py:377] train in step: 344
I0712 13:15:05.599015 140680526767936 train_utils.py:377] train in step: 345
I0712 13:15:05.958872 140680526767936 train_utils.py:377] train in step: 346
I0712 13:15:06.319052 140680526767936 train_utils.py:377] train in step: 347
I0712 13:15:06.680928 140680526767936 train_utils.py:377] train in step: 348
I0712 13:15:07.037261 140680526767936 train_utils.py:377] train in step: 349
I0712 13:15:07.396739 140680526767936 train_utils.py:377] train in step: 350
I0712 13:15:07.755453 140680526767936 train_utils.py:377] train in step: 351
I0712 13:15:08.116589 140680526767936 train_utils.py:377] train in step: 352
I0712 13:15:08.476253 140680526767936 train_utils.py:377] train in step: 353
I0712 13:15:08.847926 140680526767936 train_utils.py:377] train in step: 354
I0712 13:15:09.207888 140680526767936 train_utils.py:377] train in step: 355
I0712 13:15:09.575412 140680526767936 train_utils.py:377] train in step: 356
I0712 13:15:09.934690 140680526767936 train_utils.py:377] train in step: 357
I0712 13:15:10.295034 140680526767936 train_utils.py:377] train in step: 358
I0712 13:15:10.653210 140680526767936 train_utils.py:377] train in step: 359
I0712 13:15:11.013144 140680526767936 train_utils.py:377] train in step: 360
I0712 13:15:11.372684 140680526767936 train_utils.py:377] train in step: 361
I0712 13:15:11.731519 140680526767936 train_utils.py:377] train in step: 362
I0712 13:15:12.090897 140680526767936 train_utils.py:377] train in step: 363
I0712 13:15:12.450257 140680526767936 train_utils.py:377] train in step: 364
I0712 13:15:12.809520 140680526767936 train_utils.py:377] train in step: 365
I0712 13:15:13.178473 140680526767936 train_utils.py:377] train in step: 366
I0712 13:15:13.537065 140680526767936 train_utils.py:377] train in step: 367
I0712 13:15:13.896299 140680526767936 train_utils.py:377] train in step: 368
I0712 13:15:14.255710 140680526767936 train_utils.py:377] train in step: 369
I0712 13:15:14.618402 140680526767936 train_utils.py:377] train in step: 370
I0712 13:15:14.977930 140680526767936 train_utils.py:377] train in step: 371
I0712 13:15:15.338176 140680526767936 train_utils.py:377] train in step: 372
I0712 13:15:15.703160 140680526767936 train_utils.py:377] train in step: 373
I0712 13:15:16.066892 140680526767936 train_utils.py:377] train in step: 374
I0712 13:15:16.425504 140680526767936 train_utils.py:377] train in step: 375
I0712 13:15:16.784888 140680526767936 train_utils.py:377] train in step: 376
I0712 13:15:17.144620 140680526767936 train_utils.py:377] train in step: 377
I0712 13:15:17.508983 140680526767936 train_utils.py:377] train in step: 378
I0712 13:15:17.867436 140680526767936 train_utils.py:377] train in step: 379
I0712 13:15:18.231995 140680526767936 train_utils.py:377] train in step: 380
I0712 13:15:18.589574 140680526767936 train_utils.py:377] train in step: 381
I0712 13:15:18.949867 140680526767936 train_utils.py:377] train in step: 382
I0712 13:15:19.310908 140680526767936 train_utils.py:377] train in step: 383
I0712 13:15:19.678267 140680526767936 train_utils.py:377] train in step: 384
I0712 13:15:20.038162 140680526767936 train_utils.py:377] train in step: 385
I0712 13:15:20.403230 140680526767936 train_utils.py:377] train in step: 386
I0712 13:15:20.762951 140680526767936 train_utils.py:377] train in step: 387
I0712 13:15:21.122277 140680526767936 train_utils.py:377] train in step: 388
I0712 13:15:21.481567 140680526767936 train_utils.py:377] train in step: 389
I0712 13:15:21.841066 140680526767936 train_utils.py:377] train in step: 390
I0712 13:15:22.201119 140680526767936 train_utils.py:377] train in step: 391
I0712 13:15:22.561489 140680526767936 train_utils.py:377] train in step: 392
I0712 13:15:22.921277 140680526767936 train_utils.py:377] train in step: 393
I0712 13:15:23.280780 140680526767936 train_utils.py:377] train in step: 394
I0712 13:15:23.644073 140680526767936 train_utils.py:377] train in step: 395
I0712 13:15:24.001543 140680526767936 train_utils.py:377] train in step: 396
I0712 13:15:24.361489 140680526767936 train_utils.py:377] train in step: 397
I0712 13:15:24.721933 140680526767936 train_utils.py:377] train in step: 398
I0712 13:15:25.081566 140680526767936 train_utils.py:377] train in step: 399
I0712 13:15:25.441572 140680526767936 train_utils.py:377] train in step: 400
I0712 13:15:26.103058 140680526767936 train_utils.py:396] train in step: 400, loss: 0.7398999929428101, acc: 0.49619999527931213
I0712 13:15:34.526214 140680526767936 train_utils.py:411] eval in step: 400, loss: 0.7795, acc: 0.4800
I0712 13:15:34.530866 140680526767936 train_utils.py:421] Testing...
I0712 13:15:43.145794 140680526767936 train_utils.py:424] test in step: 400, loss: 0.7491, acc: 0.5200
I0712 13:15:43.180226 140680526767936 train_utils.py:377] train in step: 401
I0712 13:15:43.402297 140680526767936 train_utils.py:377] train in step: 402
I0712 13:15:43.760819 140680526767936 train_utils.py:377] train in step: 403
I0712 13:15:44.120908 140680526767936 train_utils.py:377] train in step: 404
I0712 13:15:44.480878 140680526767936 train_utils.py:377] train in step: 405
I0712 13:15:44.839927 140680526767936 train_utils.py:377] train in step: 406
I0712 13:15:45.199553 140680526767936 train_utils.py:377] train in step: 407
I0712 13:15:45.558948 140680526767936 train_utils.py:377] train in step: 408
I0712 13:15:45.919037 140680526767936 train_utils.py:377] train in step: 409
I0712 13:15:46.277648 140680526767936 train_utils.py:377] train in step: 410
I0712 13:15:46.636692 140680526767936 train_utils.py:377] train in step: 411
I0712 13:15:46.996766 140680526767936 train_utils.py:377] train in step: 412
I0712 13:15:47.355479 140680526767936 train_utils.py:377] train in step: 413
I0712 13:15:47.715967 140680526767936 train_utils.py:377] train in step: 414
I0712 13:15:48.074696 140680526767936 train_utils.py:377] train in step: 415
I0712 13:15:48.433733 140680526767936 train_utils.py:377] train in step: 416
I0712 13:15:48.793184 140680526767936 train_utils.py:377] train in step: 417
I0712 13:15:49.152360 140680526767936 train_utils.py:377] train in step: 418
I0712 13:15:49.510621 140680526767936 train_utils.py:377] train in step: 419
I0712 13:15:49.871088 140680526767936 train_utils.py:377] train in step: 420
I0712 13:15:50.231031 140680526767936 train_utils.py:377] train in step: 421
I0712 13:15:50.591047 140680526767936 train_utils.py:377] train in step: 422
I0712 13:15:50.950555 140680526767936 train_utils.py:377] train in step: 423
I0712 13:15:51.311627 140680526767936 train_utils.py:377] train in step: 424
I0712 13:15:51.670769 140680526767936 train_utils.py:377] train in step: 425
I0712 13:15:52.030330 140680526767936 train_utils.py:377] train in step: 426
I0712 13:15:52.389894 140680526767936 train_utils.py:377] train in step: 427
I0712 13:15:52.748541 140680526767936 train_utils.py:377] train in step: 428
I0712 13:15:53.113154 140680526767936 train_utils.py:377] train in step: 429
I0712 13:15:53.471237 140680526767936 train_utils.py:377] train in step: 430
I0712 13:15:53.829864 140680526767936 train_utils.py:377] train in step: 431
I0712 13:15:54.190157 140680526767936 train_utils.py:377] train in step: 432
I0712 13:15:54.548637 140680526767936 train_utils.py:377] train in step: 433
I0712 13:15:54.909117 140680526767936 train_utils.py:377] train in step: 434
I0712 13:15:55.275449 140680526767936 train_utils.py:377] train in step: 435
I0712 13:15:55.633888 140680526767936 train_utils.py:377] train in step: 436
I0712 13:15:55.997439 140680526767936 train_utils.py:377] train in step: 437
I0712 13:15:56.356822 140680526767936 train_utils.py:377] train in step: 438
I0712 13:15:56.719115 140680526767936 train_utils.py:377] train in step: 439
I0712 13:15:57.078213 140680526767936 train_utils.py:377] train in step: 440
I0712 13:15:57.437779 140680526767936 train_utils.py:377] train in step: 441
I0712 13:15:57.796427 140680526767936 train_utils.py:377] train in step: 442
I0712 13:15:58.156136 140680526767936 train_utils.py:377] train in step: 443
I0712 13:15:58.515887 140680526767936 train_utils.py:377] train in step: 444
I0712 13:15:58.885361 140680526767936 train_utils.py:377] train in step: 445
I0712 13:15:59.243324 140680526767936 train_utils.py:377] train in step: 446
I0712 13:15:59.614521 140680526767936 train_utils.py:377] train in step: 447
I0712 13:15:59.973443 140680526767936 train_utils.py:377] train in step: 448
I0712 13:16:00.332531 140680526767936 train_utils.py:377] train in step: 449
I0712 13:16:00.691466 140680526767936 train_utils.py:377] train in step: 450
I0712 13:16:01.051101 140680526767936 train_utils.py:377] train in step: 451
I0712 13:16:01.409345 140680526767936 train_utils.py:377] train in step: 452
I0712 13:16:01.767971 140680526767936 train_utils.py:377] train in step: 453
I0712 13:16:02.128644 140680526767936 train_utils.py:377] train in step: 454
I0712 13:16:02.487341 140680526767936 train_utils.py:377] train in step: 455
I0712 13:16:02.845305 140680526767936 train_utils.py:377] train in step: 456
I0712 13:16:03.207332 140680526767936 train_utils.py:377] train in step: 457
I0712 13:16:03.564719 140680526767936 train_utils.py:377] train in step: 458
I0712 13:16:03.923323 140680526767936 train_utils.py:377] train in step: 459
I0712 13:16:04.281693 140680526767936 train_utils.py:377] train in step: 460
I0712 13:16:04.643332 140680526767936 train_utils.py:377] train in step: 461
I0712 13:16:05.006912 140680526767936 train_utils.py:377] train in step: 462
I0712 13:16:05.365243 140680526767936 train_utils.py:377] train in step: 463
I0712 13:16:05.724914 140680526767936 train_utils.py:377] train in step: 464
I0712 13:16:06.084529 140680526767936 train_utils.py:377] train in step: 465
I0712 13:16:06.444558 140680526767936 train_utils.py:377] train in step: 466
I0712 13:16:06.803201 140680526767936 train_utils.py:377] train in step: 467
I0712 13:16:07.170301 140680526767936 train_utils.py:377] train in step: 468
I0712 13:16:07.529745 140680526767936 train_utils.py:377] train in step: 469
I0712 13:16:07.888010 140680526767936 train_utils.py:377] train in step: 470
I0712 13:16:08.247990 140680526767936 train_utils.py:377] train in step: 471
I0712 13:16:08.606166 140680526767936 train_utils.py:377] train in step: 472
I0712 13:16:08.966417 140680526767936 train_utils.py:377] train in step: 473
I0712 13:16:09.327898 140680526767936 train_utils.py:377] train in step: 474
I0712 13:16:09.691100 140680526767936 train_utils.py:377] train in step: 475
I0712 13:16:10.048208 140680526767936 train_utils.py:377] train in step: 476
I0712 13:16:10.407801 140680526767936 train_utils.py:377] train in step: 477
I0712 13:16:10.767108 140680526767936 train_utils.py:377] train in step: 478
I0712 13:16:11.125621 140680526767936 train_utils.py:377] train in step: 479
I0712 13:16:11.484333 140680526767936 train_utils.py:377] train in step: 480
I0712 13:16:11.846233 140680526767936 train_utils.py:377] train in step: 481
I0712 13:16:12.201136 140680526767936 train_utils.py:377] train in step: 482
I0712 13:16:12.563190 140680526767936 train_utils.py:377] train in step: 483
I0712 13:16:12.920635 140680526767936 train_utils.py:377] train in step: 484
I0712 13:16:13.278945 140680526767936 train_utils.py:377] train in step: 485
I0712 13:16:13.641620 140680526767936 train_utils.py:377] train in step: 486
I0712 13:16:14.002522 140680526767936 train_utils.py:377] train in step: 487
I0712 13:16:14.365480 140680526767936 train_utils.py:377] train in step: 488
I0712 13:16:14.726038 140680526767936 train_utils.py:377] train in step: 489
I0712 13:16:15.086131 140680526767936 train_utils.py:377] train in step: 490
I0712 13:16:15.444879 140680526767936 train_utils.py:377] train in step: 491
I0712 13:16:15.807320 140680526767936 train_utils.py:377] train in step: 492
I0712 13:16:16.167345 140680526767936 train_utils.py:377] train in step: 493
I0712 13:16:16.534957 140680526767936 train_utils.py:377] train in step: 494
I0712 13:16:16.895319 140680526767936 train_utils.py:377] train in step: 495
I0712 13:16:17.262378 140680526767936 train_utils.py:377] train in step: 496
I0712 13:16:17.624559 140680526767936 train_utils.py:377] train in step: 497
I0712 13:16:17.983301 140680526767936 train_utils.py:377] train in step: 498
I0712 13:16:18.343336 140680526767936 train_utils.py:377] train in step: 499
I0712 13:16:18.702435 140680526767936 train_utils.py:377] train in step: 500
I0712 13:16:19.062365 140680526767936 train_utils.py:377] train in step: 501
I0712 13:16:19.420980 140680526767936 train_utils.py:377] train in step: 502
I0712 13:16:19.779487 140680526767936 train_utils.py:377] train in step: 503
I0712 13:16:20.137964 140680526767936 train_utils.py:377] train in step: 504
I0712 13:16:20.497995 140680526767936 train_utils.py:377] train in step: 505
I0712 13:16:20.857535 140680526767936 train_utils.py:377] train in step: 506
I0712 13:16:21.218186 140680526767936 train_utils.py:377] train in step: 507
I0712 13:16:21.576257 140680526767936 train_utils.py:377] train in step: 508
I0712 13:16:21.935545 140680526767936 train_utils.py:377] train in step: 509
I0712 13:16:22.294520 140680526767936 train_utils.py:377] train in step: 510
I0712 13:16:22.654238 140680526767936 train_utils.py:377] train in step: 511
I0712 13:16:23.013487 140680526767936 train_utils.py:377] train in step: 512
I0712 13:16:23.372921 140680526767936 train_utils.py:377] train in step: 513
I0712 13:16:23.731346 140680526767936 train_utils.py:377] train in step: 514
I0712 13:16:24.090503 140680526767936 train_utils.py:377] train in step: 515
I0712 13:16:24.451158 140680526767936 train_utils.py:377] train in step: 516
I0712 13:16:24.810837 140680526767936 train_utils.py:377] train in step: 517
I0712 13:16:25.170988 140680526767936 train_utils.py:377] train in step: 518
I0712 13:16:25.534600 140680526767936 train_utils.py:377] train in step: 519
I0712 13:16:25.894916 140680526767936 train_utils.py:377] train in step: 520
I0712 13:16:26.254938 140680526767936 train_utils.py:377] train in step: 521
I0712 13:16:26.614461 140680526767936 train_utils.py:377] train in step: 522
I0712 13:16:26.974242 140680526767936 train_utils.py:377] train in step: 523
I0712 13:16:27.334203 140680526767936 train_utils.py:377] train in step: 524
I0712 13:16:27.694766 140680526767936 train_utils.py:377] train in step: 525
I0712 13:16:28.053721 140680526767936 train_utils.py:377] train in step: 526
I0712 13:16:28.418930 140680526767936 train_utils.py:377] train in step: 527
I0712 13:16:28.777691 140680526767936 train_utils.py:377] train in step: 528
I0712 13:16:29.136932 140680526767936 train_utils.py:377] train in step: 529
I0712 13:16:29.495961 140680526767936 train_utils.py:377] train in step: 530
I0712 13:16:29.857574 140680526767936 train_utils.py:377] train in step: 531
I0712 13:16:30.215814 140680526767936 train_utils.py:377] train in step: 532
I0712 13:16:30.574886 140680526767936 train_utils.py:377] train in step: 533
I0712 13:16:30.934106 140680526767936 train_utils.py:377] train in step: 534
I0712 13:16:31.294722 140680526767936 train_utils.py:377] train in step: 535
I0712 13:16:31.655443 140680526767936 train_utils.py:377] train in step: 536
I0712 13:16:32.013435 140680526767936 train_utils.py:377] train in step: 537
I0712 13:16:32.373440 140680526767936 train_utils.py:377] train in step: 538
I0712 13:16:32.732547 140680526767936 train_utils.py:377] train in step: 539
I0712 13:16:33.091569 140680526767936 train_utils.py:377] train in step: 540
I0712 13:16:33.451583 140680526767936 train_utils.py:377] train in step: 541
I0712 13:16:33.809951 140680526767936 train_utils.py:377] train in step: 542
I0712 13:16:34.172657 140680526767936 train_utils.py:377] train in step: 543
I0712 13:16:34.534241 140680526767936 train_utils.py:377] train in step: 544
I0712 13:16:34.893085 140680526767936 train_utils.py:377] train in step: 545
I0712 13:16:35.251439 140680526767936 train_utils.py:377] train in step: 546
I0712 13:16:35.610280 140680526767936 train_utils.py:377] train in step: 547
I0712 13:16:35.969846 140680526767936 train_utils.py:377] train in step: 548
I0712 13:16:36.328774 140680526767936 train_utils.py:377] train in step: 549
I0712 13:16:36.688406 140680526767936 train_utils.py:377] train in step: 550
I0712 13:16:37.046723 140680526767936 train_utils.py:377] train in step: 551
I0712 13:16:37.406576 140680526767936 train_utils.py:377] train in step: 552
I0712 13:16:37.763153 140680526767936 train_utils.py:377] train in step: 553
I0712 13:16:38.123157 140680526767936 train_utils.py:377] train in step: 554
I0712 13:16:38.482088 140680526767936 train_utils.py:377] train in step: 555
I0712 13:16:38.842437 140680526767936 train_utils.py:377] train in step: 556
I0712 13:16:39.201957 140680526767936 train_utils.py:377] train in step: 557
I0712 13:16:39.561149 140680526767936 train_utils.py:377] train in step: 558
I0712 13:16:39.924879 140680526767936 train_utils.py:377] train in step: 559
I0712 13:16:40.282089 140680526767936 train_utils.py:377] train in step: 560
I0712 13:16:40.643793 140680526767936 train_utils.py:377] train in step: 561
I0712 13:16:41.004363 140680526767936 train_utils.py:377] train in step: 562
I0712 13:16:41.364121 140680526767936 train_utils.py:377] train in step: 563
I0712 13:16:41.728451 140680526767936 train_utils.py:377] train in step: 564
I0712 13:16:42.086231 140680526767936 train_utils.py:377] train in step: 565
I0712 13:16:42.461796 140680526767936 train_utils.py:377] train in step: 566
I0712 13:16:42.819932 140680526767936 train_utils.py:377] train in step: 567
I0712 13:16:43.182532 140680526767936 train_utils.py:377] train in step: 568
I0712 13:16:43.540908 140680526767936 train_utils.py:377] train in step: 569
I0712 13:16:43.908488 140680526767936 train_utils.py:377] train in step: 570
I0712 13:16:44.267207 140680526767936 train_utils.py:377] train in step: 571
I0712 13:16:44.625439 140680526767936 train_utils.py:377] train in step: 572
I0712 13:16:44.985091 140680526767936 train_utils.py:377] train in step: 573
I0712 13:16:45.342773 140680526767936 train_utils.py:377] train in step: 574
I0712 13:16:45.702632 140680526767936 train_utils.py:377] train in step: 575
I0712 13:16:46.062296 140680526767936 train_utils.py:377] train in step: 576
I0712 13:16:46.420468 140680526767936 train_utils.py:377] train in step: 577
I0712 13:16:46.779753 140680526767936 train_utils.py:377] train in step: 578
I0712 13:16:47.140535 140680526767936 train_utils.py:377] train in step: 579
I0712 13:16:47.498067 140680526767936 train_utils.py:377] train in step: 580
I0712 13:16:47.857261 140680526767936 train_utils.py:377] train in step: 581
I0712 13:16:48.221834 140680526767936 train_utils.py:377] train in step: 582
I0712 13:16:48.581658 140680526767936 train_utils.py:377] train in step: 583
I0712 13:16:48.941445 140680526767936 train_utils.py:377] train in step: 584
I0712 13:16:49.300548 140680526767936 train_utils.py:377] train in step: 585
I0712 13:16:49.659191 140680526767936 train_utils.py:377] train in step: 586
I0712 13:16:50.018434 140680526767936 train_utils.py:377] train in step: 587
I0712 13:16:50.378388 140680526767936 train_utils.py:377] train in step: 588
I0712 13:16:50.738485 140680526767936 train_utils.py:377] train in step: 589
I0712 13:16:51.099484 140680526767936 train_utils.py:377] train in step: 590
I0712 13:16:51.459609 140680526767936 train_utils.py:377] train in step: 591
I0712 13:16:51.819529 140680526767936 train_utils.py:377] train in step: 592
I0712 13:16:52.191679 140680526767936 train_utils.py:377] train in step: 593
I0712 13:16:52.545076 140680526767936 train_utils.py:377] train in step: 594
I0712 13:16:52.904130 140680526767936 train_utils.py:377] train in step: 595
I0712 13:16:53.263395 140680526767936 train_utils.py:377] train in step: 596
I0712 13:16:53.623076 140680526767936 train_utils.py:377] train in step: 597
I0712 13:16:53.982086 140680526767936 train_utils.py:377] train in step: 598
I0712 13:16:54.341681 140680526767936 train_utils.py:377] train in step: 599
I0712 13:16:54.700476 140680526767936 train_utils.py:377] train in step: 600
I0712 13:16:55.197102 140680526767936 train_utils.py:396] train in step: 600, loss: 0.7119999527931213, acc: 0.4861999750137329
I0712 13:17:03.719764 140680526767936 train_utils.py:411] eval in step: 600, loss: 0.7140, acc: 0.5100
I0712 13:17:03.724218 140680526767936 train_utils.py:421] Testing...
I0712 13:17:12.200239 140680526767936 train_utils.py:424] test in step: 600, loss: 0.7256, acc: 0.4850
I0712 13:17:12.232337 140680526767936 train_utils.py:377] train in step: 601
I0712 13:17:12.455531 140680526767936 train_utils.py:377] train in step: 602
I0712 13:17:12.826900 140680526767936 train_utils.py:377] train in step: 603
I0712 13:17:13.185277 140680526767936 train_utils.py:377] train in step: 604
I0712 13:17:13.547374 140680526767936 train_utils.py:377] train in step: 605
I0712 13:17:13.906248 140680526767936 train_utils.py:377] train in step: 606
I0712 13:17:14.267442 140680526767936 train_utils.py:377] train in step: 607
I0712 13:17:14.627018 140680526767936 train_utils.py:377] train in step: 608
I0712 13:17:14.991698 140680526767936 train_utils.py:377] train in step: 609
I0712 13:17:15.349528 140680526767936 train_utils.py:377] train in step: 610
I0712 13:17:15.709025 140680526767936 train_utils.py:377] train in step: 611
I0712 13:17:16.069833 140680526767936 train_utils.py:377] train in step: 612
I0712 13:17:16.428852 140680526767936 train_utils.py:377] train in step: 613
I0712 13:17:16.787737 140680526767936 train_utils.py:377] train in step: 614
I0712 13:17:17.146780 140680526767936 train_utils.py:377] train in step: 615
I0712 13:17:17.507056 140680526767936 train_utils.py:377] train in step: 616
I0712 13:17:17.866197 140680526767936 train_utils.py:377] train in step: 617
I0712 13:17:18.227156 140680526767936 train_utils.py:377] train in step: 618
I0712 13:17:18.584394 140680526767936 train_utils.py:377] train in step: 619
I0712 13:17:18.945031 140680526767936 train_utils.py:377] train in step: 620
I0712 13:17:19.306850 140680526767936 train_utils.py:377] train in step: 621
I0712 13:17:19.664732 140680526767936 train_utils.py:377] train in step: 622
I0712 13:17:20.022879 140680526767936 train_utils.py:377] train in step: 623
I0712 13:17:20.382991 140680526767936 train_utils.py:377] train in step: 624
I0712 13:17:20.741131 140680526767936 train_utils.py:377] train in step: 625
I0712 13:17:21.101752 140680526767936 train_utils.py:377] train in step: 626
I0712 13:17:21.459914 140680526767936 train_utils.py:377] train in step: 627
I0712 13:17:21.820713 140680526767936 train_utils.py:377] train in step: 628
I0712 13:17:22.179265 140680526767936 train_utils.py:377] train in step: 629
I0712 13:17:22.545932 140680526767936 train_utils.py:377] train in step: 630
I0712 13:17:22.906922 140680526767936 train_utils.py:377] train in step: 631
I0712 13:17:23.273847 140680526767936 train_utils.py:377] train in step: 632
I0712 13:17:23.632735 140680526767936 train_utils.py:377] train in step: 633
I0712 13:17:23.995350 140680526767936 train_utils.py:377] train in step: 634
I0712 13:17:24.354058 140680526767936 train_utils.py:377] train in step: 635
I0712 13:17:24.711981 140680526767936 train_utils.py:377] train in step: 636
I0712 13:17:25.071726 140680526767936 train_utils.py:377] train in step: 637
I0712 13:17:25.429812 140680526767936 train_utils.py:377] train in step: 638
I0712 13:17:25.790669 140680526767936 train_utils.py:377] train in step: 639
I0712 13:17:26.148508 140680526767936 train_utils.py:377] train in step: 640
I0712 13:17:26.508513 140680526767936 train_utils.py:377] train in step: 641
I0712 13:17:26.867283 140680526767936 train_utils.py:377] train in step: 642
I0712 13:17:27.226557 140680526767936 train_utils.py:377] train in step: 643
I0712 13:17:27.586823 140680526767936 train_utils.py:377] train in step: 644
I0712 13:17:27.946676 140680526767936 train_utils.py:377] train in step: 645
I0712 13:17:28.311176 140680526767936 train_utils.py:377] train in step: 646
I0712 13:17:28.664946 140680526767936 train_utils.py:377] train in step: 647
I0712 13:17:29.028910 140680526767936 train_utils.py:377] train in step: 648
I0712 13:17:29.384219 140680526767936 train_utils.py:377] train in step: 649
I0712 13:17:29.743443 140680526767936 train_utils.py:377] train in step: 650
I0712 13:17:30.102707 140680526767936 train_utils.py:377] train in step: 651
I0712 13:17:30.462701 140680526767936 train_utils.py:377] train in step: 652
I0712 13:17:30.822551 140680526767936 train_utils.py:377] train in step: 653
I0712 13:17:31.182933 140680526767936 train_utils.py:377] train in step: 654
I0712 13:17:31.542253 140680526767936 train_utils.py:377] train in step: 655
I0712 13:17:31.901823 140680526767936 train_utils.py:377] train in step: 656
I0712 13:17:32.261895 140680526767936 train_utils.py:377] train in step: 657
I0712 13:17:32.621272 140680526767936 train_utils.py:377] train in step: 658
I0712 13:17:32.980802 140680526767936 train_utils.py:377] train in step: 659
I0712 13:17:33.342070 140680526767936 train_utils.py:377] train in step: 660
I0712 13:17:33.702928 140680526767936 train_utils.py:377] train in step: 661
I0712 13:17:34.062864 140680526767936 train_utils.py:377] train in step: 662
I0712 13:17:34.435640 140680526767936 train_utils.py:377] train in step: 663
I0712 13:17:34.794850 140680526767936 train_utils.py:377] train in step: 664
I0712 13:17:35.155198 140680526767936 train_utils.py:377] train in step: 665
I0712 13:17:35.514127 140680526767936 train_utils.py:377] train in step: 666
I0712 13:17:35.873517 140680526767936 train_utils.py:377] train in step: 667
I0712 13:17:36.233167 140680526767936 train_utils.py:377] train in step: 668
I0712 13:17:36.593339 140680526767936 train_utils.py:377] train in step: 669
I0712 13:17:36.952740 140680526767936 train_utils.py:377] train in step: 670
I0712 13:17:37.324298 140680526767936 train_utils.py:377] train in step: 671
I0712 13:17:37.681672 140680526767936 train_utils.py:377] train in step: 672
I0712 13:17:38.049574 140680526767936 train_utils.py:377] train in step: 673
I0712 13:17:38.405349 140680526767936 train_utils.py:377] train in step: 674
I0712 13:17:38.765442 140680526767936 train_utils.py:377] train in step: 675
I0712 13:17:39.126455 140680526767936 train_utils.py:377] train in step: 676
I0712 13:17:39.502743 140680526767936 train_utils.py:377] train in step: 677
I0712 13:17:39.861808 140680526767936 train_utils.py:377] train in step: 678
I0712 13:17:40.224541 140680526767936 train_utils.py:377] train in step: 679
I0712 13:17:40.589742 140680526767936 train_utils.py:377] train in step: 680
I0712 13:17:41.013067 140680526767936 train_utils.py:377] train in step: 681
I0712 13:17:41.381948 140680526767936 train_utils.py:377] train in step: 682
I0712 13:17:41.739825 140680526767936 train_utils.py:377] train in step: 683
I0712 13:17:42.097945 140680526767936 train_utils.py:377] train in step: 684
I0712 13:17:42.458123 140680526767936 train_utils.py:377] train in step: 685
I0712 13:17:42.817451 140680526767936 train_utils.py:377] train in step: 686
I0712 13:17:43.177594 140680526767936 train_utils.py:377] train in step: 687
I0712 13:17:43.537609 140680526767936 train_utils.py:377] train in step: 688
I0712 13:17:43.897304 140680526767936 train_utils.py:377] train in step: 689
I0712 13:17:44.256661 140680526767936 train_utils.py:377] train in step: 690
I0712 13:17:44.616420 140680526767936 train_utils.py:377] train in step: 691
I0712 13:17:44.975640 140680526767936 train_utils.py:377] train in step: 692
I0712 13:17:45.335957 140680526767936 train_utils.py:377] train in step: 693
I0712 13:17:45.695825 140680526767936 train_utils.py:377] train in step: 694
I0712 13:17:46.055822 140680526767936 train_utils.py:377] train in step: 695
I0712 13:17:46.417567 140680526767936 train_utils.py:377] train in step: 696
I0712 13:17:46.777520 140680526767936 train_utils.py:377] train in step: 697
I0712 13:17:47.136479 140680526767936 train_utils.py:377] train in step: 698
I0712 13:17:47.500583 140680526767936 train_utils.py:377] train in step: 699
I0712 13:17:47.858070 140680526767936 train_utils.py:377] train in step: 700
I0712 13:17:48.222018 140680526767936 train_utils.py:377] train in step: 701
I0712 13:17:48.579590 140680526767936 train_utils.py:377] train in step: 702
I0712 13:17:48.939139 140680526767936 train_utils.py:377] train in step: 703
I0712 13:17:49.357618 140680526767936 train_utils.py:377] train in step: 704
I0712 13:17:49.727378 140680526767936 train_utils.py:377] train in step: 705
I0712 13:17:50.086653 140680526767936 train_utils.py:377] train in step: 706
I0712 13:17:50.446030 140680526767936 train_utils.py:377] train in step: 707
I0712 13:17:50.806252 140680526767936 train_utils.py:377] train in step: 708
I0712 13:17:51.202087 140680526767936 train_utils.py:377] train in step: 709
I0712 13:17:51.605340 140680526767936 train_utils.py:377] train in step: 710
I0712 13:17:51.968227 140680526767936 train_utils.py:377] train in step: 711
I0712 13:17:52.327717 140680526767936 train_utils.py:377] train in step: 712
I0712 13:17:52.687548 140680526767936 train_utils.py:377] train in step: 713
I0712 13:17:53.047365 140680526767936 train_utils.py:377] train in step: 714
I0712 13:17:53.422204 140680526767936 train_utils.py:377] train in step: 715
I0712 13:17:53.790439 140680526767936 train_utils.py:377] train in step: 716
I0712 13:17:54.150589 140680526767936 train_utils.py:377] train in step: 717
I0712 13:17:54.510516 140680526767936 train_utils.py:377] train in step: 718
I0712 13:17:54.880351 140680526767936 train_utils.py:377] train in step: 719
I0712 13:17:55.241828 140680526767936 train_utils.py:377] train in step: 720
I0712 13:17:55.601203 140680526767936 train_utils.py:377] train in step: 721
I0712 13:17:55.960633 140680526767936 train_utils.py:377] train in step: 722
I0712 13:17:56.320562 140680526767936 train_utils.py:377] train in step: 723
I0712 13:17:56.679821 140680526767936 train_utils.py:377] train in step: 724
I0712 13:17:57.039945 140680526767936 train_utils.py:377] train in step: 725
I0712 13:17:57.400352 140680526767936 train_utils.py:377] train in step: 726
I0712 13:17:57.760421 140680526767936 train_utils.py:377] train in step: 727
I0712 13:17:58.118641 140680526767936 train_utils.py:377] train in step: 728
I0712 13:17:58.478523 140680526767936 train_utils.py:377] train in step: 729
I0712 13:17:58.839003 140680526767936 train_utils.py:377] train in step: 730
I0712 13:17:59.200330 140680526767936 train_utils.py:377] train in step: 731
I0712 13:17:59.561639 140680526767936 train_utils.py:377] train in step: 732
I0712 13:17:59.925899 140680526767936 train_utils.py:377] train in step: 733
I0712 13:18:00.290948 140680526767936 train_utils.py:377] train in step: 734
I0712 13:18:00.651204 140680526767936 train_utils.py:377] train in step: 735
I0712 13:18:01.010665 140680526767936 train_utils.py:377] train in step: 736
I0712 13:18:01.455915 140680526767936 train_utils.py:377] train in step: 737
I0712 13:18:01.834577 140680526767936 train_utils.py:377] train in step: 738
I0712 13:18:02.195946 140680526767936 train_utils.py:377] train in step: 739
I0712 13:18:02.562515 140680526767936 train_utils.py:377] train in step: 740
I0712 13:18:02.959515 140680526767936 train_utils.py:377] train in step: 741
I0712 13:18:03.372091 140680526767936 train_utils.py:377] train in step: 742
I0712 13:18:03.808834 140680526767936 train_utils.py:377] train in step: 743
I0712 13:18:04.217583 140680526767936 train_utils.py:377] train in step: 744
I0712 13:18:04.577534 140680526767936 train_utils.py:377] train in step: 745
I0712 13:18:04.937196 140680526767936 train_utils.py:377] train in step: 746
I0712 13:18:05.298650 140680526767936 train_utils.py:377] train in step: 747
I0712 13:18:05.668641 140680526767936 train_utils.py:377] train in step: 748
I0712 13:18:06.049880 140680526767936 train_utils.py:377] train in step: 749
I0712 13:18:06.419373 140680526767936 train_utils.py:377] train in step: 750
I0712 13:18:06.779517 140680526767936 train_utils.py:377] train in step: 751
I0712 13:18:07.153480 140680526767936 train_utils.py:377] train in step: 752
I0712 13:18:07.541652 140680526767936 train_utils.py:377] train in step: 753
I0712 13:18:07.918359 140680526767936 train_utils.py:377] train in step: 754
I0712 13:18:08.295643 140680526767936 train_utils.py:377] train in step: 755
I0712 13:18:08.665481 140680526767936 train_utils.py:377] train in step: 756
I0712 13:18:09.025732 140680526767936 train_utils.py:377] train in step: 757
I0712 13:18:09.399908 140680526767936 train_utils.py:377] train in step: 758
I0712 13:18:09.763848 140680526767936 train_utils.py:377] train in step: 759
I0712 13:18:10.179413 140680526767936 train_utils.py:377] train in step: 760
I0712 13:18:10.603710 140680526767936 train_utils.py:377] train in step: 761
I0712 13:18:10.974629 140680526767936 train_utils.py:377] train in step: 762
I0712 13:18:11.341171 140680526767936 train_utils.py:377] train in step: 763
I0712 13:18:11.734416 140680526767936 train_utils.py:377] train in step: 764
I0712 13:18:12.291733 140680526767936 train_utils.py:377] train in step: 765
I0712 13:18:12.690444 140680526767936 train_utils.py:377] train in step: 766
I0712 13:18:13.058994 140680526767936 train_utils.py:377] train in step: 767
I0712 13:18:13.416334 140680526767936 train_utils.py:377] train in step: 768
I0712 13:18:13.776415 140680526767936 train_utils.py:377] train in step: 769
I0712 13:18:14.136515 140680526767936 train_utils.py:377] train in step: 770
I0712 13:18:14.530623 140680526767936 train_utils.py:377] train in step: 771
I0712 13:18:14.905795 140680526767936 train_utils.py:377] train in step: 772
I0712 13:18:15.271411 140680526767936 train_utils.py:377] train in step: 773
I0712 13:18:15.633032 140680526767936 train_utils.py:377] train in step: 774
I0712 13:18:15.997313 140680526767936 train_utils.py:377] train in step: 775
I0712 13:18:16.360929 140680526767936 train_utils.py:377] train in step: 776
I0712 13:18:16.755443 140680526767936 train_utils.py:377] train in step: 777
I0712 13:18:17.151246 140680526767936 train_utils.py:377] train in step: 778
I0712 13:18:17.533643 140680526767936 train_utils.py:377] train in step: 779
I0712 13:18:17.896981 140680526767936 train_utils.py:377] train in step: 780
I0712 13:18:18.260687 140680526767936 train_utils.py:377] train in step: 781
I0712 13:18:18.632616 140680526767936 train_utils.py:377] train in step: 782
I0712 13:18:18.998149 140680526767936 train_utils.py:377] train in step: 783
I0712 13:18:19.394914 140680526767936 train_utils.py:377] train in step: 784
I0712 13:18:19.845709 140680526767936 train_utils.py:377] train in step: 785
I0712 13:18:20.211568 140680526767936 train_utils.py:377] train in step: 786
I0712 13:18:20.596227 140680526767936 train_utils.py:377] train in step: 787
I0712 13:18:20.981261 140680526767936 train_utils.py:377] train in step: 788
I0712 13:18:21.433941 140680526767936 train_utils.py:377] train in step: 789
I0712 13:18:21.871740 140680526767936 train_utils.py:377] train in step: 790
I0712 13:18:22.322077 140680526767936 train_utils.py:377] train in step: 791
I0712 13:18:22.727342 140680526767936 train_utils.py:377] train in step: 792
I0712 13:18:23.115177 140680526767936 train_utils.py:377] train in step: 793
I0712 13:18:23.476765 140680526767936 train_utils.py:377] train in step: 794
I0712 13:18:23.836749 140680526767936 train_utils.py:377] train in step: 795
I0712 13:18:24.201294 140680526767936 train_utils.py:377] train in step: 796
I0712 13:18:24.569634 140680526767936 train_utils.py:377] train in step: 797
I0712 13:18:24.994915 140680526767936 train_utils.py:377] train in step: 798
I0712 13:18:25.376575 140680526767936 train_utils.py:377] train in step: 799
I0712 13:18:25.744072 140680526767936 train_utils.py:377] train in step: 800
I0712 13:18:26.239068 140680526767936 train_utils.py:396] train in step: 800, loss: 0.7077999711036682, acc: 0.5
I0712 13:18:34.621674 140680526767936 train_utils.py:411] eval in step: 800, loss: 0.7033, acc: 0.5000
I0712 13:18:34.625106 140680526767936 train_utils.py:421] Testing...
I0712 13:18:43.086017 140680526767936 train_utils.py:424] test in step: 800, loss: 0.7105, acc: 0.4750
I0712 13:18:43.127409 140680526767936 train_utils.py:377] train in step: 801
I0712 13:18:43.346432 140680526767936 train_utils.py:377] train in step: 802
I0712 13:18:43.710504 140680526767936 train_utils.py:377] train in step: 803
I0712 13:18:44.069069 140680526767936 train_utils.py:377] train in step: 804
I0712 13:18:44.428613 140680526767936 train_utils.py:377] train in step: 805
I0712 13:18:44.792505 140680526767936 train_utils.py:377] train in step: 806
I0712 13:18:45.148077 140680526767936 train_utils.py:377] train in step: 807
I0712 13:18:45.509480 140680526767936 train_utils.py:377] train in step: 808
I0712 13:18:45.867337 140680526767936 train_utils.py:377] train in step: 809
I0712 13:18:46.227020 140680526767936 train_utils.py:377] train in step: 810
I0712 13:18:46.585791 140680526767936 train_utils.py:377] train in step: 811
I0712 13:18:46.945779 140680526767936 train_utils.py:377] train in step: 812
I0712 13:18:47.306838 140680526767936 train_utils.py:377] train in step: 813
I0712 13:18:47.666102 140680526767936 train_utils.py:377] train in step: 814
I0712 13:18:48.025228 140680526767936 train_utils.py:377] train in step: 815
I0712 13:18:48.394645 140680526767936 train_utils.py:377] train in step: 816
I0712 13:18:48.754252 140680526767936 train_utils.py:377] train in step: 817
I0712 13:18:49.113024 140680526767936 train_utils.py:377] train in step: 818
I0712 13:18:49.479122 140680526767936 train_utils.py:377] train in step: 819
I0712 13:18:49.859797 140680526767936 train_utils.py:377] train in step: 820
I0712 13:18:50.227973 140680526767936 train_utils.py:377] train in step: 821
I0712 13:18:50.590099 140680526767936 train_utils.py:377] train in step: 822
I0712 13:18:50.952138 140680526767936 train_utils.py:377] train in step: 823
I0712 13:18:51.326387 140680526767936 train_utils.py:377] train in step: 824
I0712 13:18:51.687803 140680526767936 train_utils.py:377] train in step: 825
I0712 13:18:52.059580 140680526767936 train_utils.py:377] train in step: 826
I0712 13:18:52.419311 140680526767936 train_utils.py:377] train in step: 827
I0712 13:18:52.784249 140680526767936 train_utils.py:377] train in step: 828
I0712 13:18:53.144584 140680526767936 train_utils.py:377] train in step: 829
I0712 13:18:53.589508 140680526767936 train_utils.py:377] train in step: 830
I0712 13:18:54.058799 140680526767936 train_utils.py:377] train in step: 831
I0712 13:18:54.431447 140680526767936 train_utils.py:377] train in step: 832
I0712 13:18:54.795348 140680526767936 train_utils.py:377] train in step: 833
I0712 13:18:55.161475 140680526767936 train_utils.py:377] train in step: 834
I0712 13:18:55.522961 140680526767936 train_utils.py:377] train in step: 835
I0712 13:18:55.884199 140680526767936 train_utils.py:377] train in step: 836
I0712 13:18:56.244805 140680526767936 train_utils.py:377] train in step: 837
I0712 13:18:56.605122 140680526767936 train_utils.py:377] train in step: 838
I0712 13:18:56.964315 140680526767936 train_utils.py:377] train in step: 839
I0712 13:18:57.334006 140680526767936 train_utils.py:377] train in step: 840
I0712 13:18:57.702049 140680526767936 train_utils.py:377] train in step: 841
I0712 13:18:58.061678 140680526767936 train_utils.py:377] train in step: 842
I0712 13:18:58.428627 140680526767936 train_utils.py:377] train in step: 843
I0712 13:18:58.787233 140680526767936 train_utils.py:377] train in step: 844
I0712 13:18:59.162545 140680526767936 train_utils.py:377] train in step: 845
I0712 13:18:59.539802 140680526767936 train_utils.py:377] train in step: 846
I0712 13:18:59.909050 140680526767936 train_utils.py:377] train in step: 847
I0712 13:19:00.271575 140680526767936 train_utils.py:377] train in step: 848
I0712 13:19:00.638911 140680526767936 train_utils.py:377] train in step: 849
I0712 13:19:01.002673 140680526767936 train_utils.py:377] train in step: 850
I0712 13:19:01.390835 140680526767936 train_utils.py:377] train in step: 851
I0712 13:19:01.773649 140680526767936 train_utils.py:377] train in step: 852
I0712 13:19:02.156585 140680526767936 train_utils.py:377] train in step: 853
I0712 13:19:03.004278 140680526767936 train_utils.py:377] train in step: 854
I0712 13:19:04.490827 140680526767936 train_utils.py:377] train in step: 855
I0712 13:19:05.429269 140680526767936 train_utils.py:377] train in step: 856
I0712 13:19:06.367917 140680526767936 train_utils.py:377] train in step: 857
I0712 13:19:07.307403 140680526767936 train_utils.py:377] train in step: 858
I0712 13:19:08.245038 140680526767936 train_utils.py:377] train in step: 859
I0712 13:19:09.194411 140680526767936 train_utils.py:377] train in step: 860
I0712 13:19:10.133762 140680526767936 train_utils.py:377] train in step: 861
I0712 13:19:11.077358 140680526767936 train_utils.py:377] train in step: 862
I0712 13:19:12.014277 140680526767936 train_utils.py:377] train in step: 863
I0712 13:19:12.953543 140680526767936 train_utils.py:377] train in step: 864
I0712 13:19:13.888173 140680526767936 train_utils.py:377] train in step: 865
I0712 13:19:14.826627 140680526767936 train_utils.py:377] train in step: 866
I0712 13:19:15.763432 140680526767936 train_utils.py:377] train in step: 867
I0712 13:19:16.702624 140680526767936 train_utils.py:377] train in step: 868
I0712 13:19:17.641547 140680526767936 train_utils.py:377] train in step: 869
I0712 13:19:18.578852 140680526767936 train_utils.py:377] train in step: 870
I0712 13:19:19.386980 140680526767936 train_utils.py:377] train in step: 871
I0712 13:19:19.775674 140680526767936 train_utils.py:377] train in step: 872
I0712 13:19:20.135627 140680526767936 train_utils.py:377] train in step: 873
I0712 13:19:20.494188 140680526767936 train_utils.py:377] train in step: 874
I0712 13:19:20.854758 140680526767936 train_utils.py:377] train in step: 875
I0712 13:19:21.213411 140680526767936 train_utils.py:377] train in step: 876
I0712 13:19:21.574473 140680526767936 train_utils.py:377] train in step: 877
I0712 13:19:21.936076 140680526767936 train_utils.py:377] train in step: 878
I0712 13:19:22.295741 140680526767936 train_utils.py:377] train in step: 879
I0712 13:19:22.654855 140680526767936 train_utils.py:377] train in step: 880
I0712 13:19:23.013489 140680526767936 train_utils.py:377] train in step: 881
I0712 13:19:23.372784 140680526767936 train_utils.py:377] train in step: 882
I0712 13:19:23.732681 140680526767936 train_utils.py:377] train in step: 883
I0712 13:19:24.092315 140680526767936 train_utils.py:377] train in step: 884
I0712 13:19:24.451229 140680526767936 train_utils.py:377] train in step: 885
I0712 13:19:24.815639 140680526767936 train_utils.py:377] train in step: 886
I0712 13:19:25.171597 140680526767936 train_utils.py:377] train in step: 887
I0712 13:19:25.531203 140680526767936 train_utils.py:377] train in step: 888
I0712 13:19:25.892926 140680526767936 train_utils.py:377] train in step: 889
I0712 13:19:26.251787 140680526767936 train_utils.py:377] train in step: 890
I0712 13:19:26.611429 140680526767936 train_utils.py:377] train in step: 891
I0712 13:19:26.970898 140680526767936 train_utils.py:377] train in step: 892
I0712 13:19:27.329378 140680526767936 train_utils.py:377] train in step: 893
I0712 13:19:27.699156 140680526767936 train_utils.py:377] train in step: 894
I0712 13:19:28.058513 140680526767936 train_utils.py:377] train in step: 895
I0712 13:19:28.423542 140680526767936 train_utils.py:377] train in step: 896
I0712 13:19:28.783365 140680526767936 train_utils.py:377] train in step: 897
I0712 13:19:29.142704 140680526767936 train_utils.py:377] train in step: 898
I0712 13:19:29.502734 140680526767936 train_utils.py:377] train in step: 899
I0712 13:19:29.862947 140680526767936 train_utils.py:377] train in step: 900
I0712 13:19:30.222516 140680526767936 train_utils.py:377] train in step: 901
I0712 13:19:30.582159 140680526767936 train_utils.py:377] train in step: 902
I0712 13:19:30.942399 140680526767936 train_utils.py:377] train in step: 903
I0712 13:19:31.302157 140680526767936 train_utils.py:377] train in step: 904
I0712 13:19:31.668017 140680526767936 train_utils.py:377] train in step: 905
I0712 13:19:32.025544 140680526767936 train_utils.py:377] train in step: 906
I0712 13:19:32.383019 140680526767936 train_utils.py:377] train in step: 907
I0712 13:19:32.745024 140680526767936 train_utils.py:377] train in step: 908
I0712 13:19:33.103739 140680526767936 train_utils.py:377] train in step: 909
I0712 13:19:33.462618 140680526767936 train_utils.py:377] train in step: 910
I0712 13:19:33.822412 140680526767936 train_utils.py:377] train in step: 911
I0712 13:19:34.182436 140680526767936 train_utils.py:377] train in step: 912
I0712 13:19:34.542554 140680526767936 train_utils.py:377] train in step: 913
I0712 13:19:34.901480 140680526767936 train_utils.py:377] train in step: 914
I0712 13:19:35.262100 140680526767936 train_utils.py:377] train in step: 915
I0712 13:19:35.621644 140680526767936 train_utils.py:377] train in step: 916
I0712 13:19:35.991160 140680526767936 train_utils.py:377] train in step: 917
I0712 13:19:36.350496 140680526767936 train_utils.py:377] train in step: 918
I0712 13:19:36.709227 140680526767936 train_utils.py:377] train in step: 919
I0712 13:19:37.069065 140680526767936 train_utils.py:377] train in step: 920
I0712 13:19:37.428060 140680526767936 train_utils.py:377] train in step: 921
I0712 13:19:37.788306 140680526767936 train_utils.py:377] train in step: 922
I0712 13:19:38.147316 140680526767936 train_utils.py:377] train in step: 923
I0712 13:19:38.507070 140680526767936 train_utils.py:377] train in step: 924
I0712 13:19:38.874572 140680526767936 train_utils.py:377] train in step: 925
I0712 13:19:39.233716 140680526767936 train_utils.py:377] train in step: 926
I0712 13:19:39.593462 140680526767936 train_utils.py:377] train in step: 927
I0712 13:19:39.953246 140680526767936 train_utils.py:377] train in step: 928
I0712 13:19:40.313282 140680526767936 train_utils.py:377] train in step: 929
I0712 13:19:40.673232 140680526767936 train_utils.py:377] train in step: 930
I0712 13:19:41.032056 140680526767936 train_utils.py:377] train in step: 931
I0712 13:19:41.391303 140680526767936 train_utils.py:377] train in step: 932
I0712 13:19:41.751127 140680526767936 train_utils.py:377] train in step: 933
I0712 13:19:42.112881 140680526767936 train_utils.py:377] train in step: 934
I0712 13:19:42.470841 140680526767936 train_utils.py:377] train in step: 935
I0712 13:19:42.831361 140680526767936 train_utils.py:377] train in step: 936
I0712 13:19:43.205706 140680526767936 train_utils.py:377] train in step: 937
I0712 13:19:43.572264 140680526767936 train_utils.py:377] train in step: 938
I0712 13:19:43.937720 140680526767936 train_utils.py:377] train in step: 939
I0712 13:19:44.297331 140680526767936 train_utils.py:377] train in step: 940
I0712 13:19:44.656570 140680526767936 train_utils.py:377] train in step: 941
I0712 13:19:45.016119 140680526767936 train_utils.py:377] train in step: 942
I0712 13:19:45.378239 140680526767936 train_utils.py:377] train in step: 943
I0712 13:19:45.742406 140680526767936 train_utils.py:377] train in step: 944
I0712 13:19:46.103341 140680526767936 train_utils.py:377] train in step: 945
I0712 13:19:46.464677 140680526767936 train_utils.py:377] train in step: 946
I0712 13:19:46.826630 140680526767936 train_utils.py:377] train in step: 947
I0712 13:19:47.191010 140680526767936 train_utils.py:377] train in step: 948
I0712 13:19:47.565069 140680526767936 train_utils.py:377] train in step: 949
I0712 13:19:47.936049 140680526767936 train_utils.py:377] train in step: 950
I0712 13:19:48.312817 140680526767936 train_utils.py:377] train in step: 951
I0712 13:19:48.694344 140680526767936 train_utils.py:377] train in step: 952
I0712 13:19:49.059725 140680526767936 train_utils.py:377] train in step: 953
I0712 13:19:49.426581 140680526767936 train_utils.py:377] train in step: 954
I0712 13:19:49.782746 140680526767936 train_utils.py:377] train in step: 955
I0712 13:19:50.150012 140680526767936 train_utils.py:377] train in step: 956
I0712 13:19:50.534353 140680526767936 train_utils.py:377] train in step: 957
I0712 13:19:50.902586 140680526767936 train_utils.py:377] train in step: 958
I0712 13:19:51.283777 140680526767936 train_utils.py:377] train in step: 959
I0712 13:19:51.655166 140680526767936 train_utils.py:377] train in step: 960
I0712 13:19:52.023616 140680526767936 train_utils.py:377] train in step: 961
I0712 13:19:52.438545 140680526767936 train_utils.py:377] train in step: 962
I0712 13:19:52.830812 140680526767936 train_utils.py:377] train in step: 963
I0712 13:19:53.193452 140680526767936 train_utils.py:377] train in step: 964
I0712 13:19:53.556035 140680526767936 train_utils.py:377] train in step: 965
I0712 13:19:53.937917 140680526767936 train_utils.py:377] train in step: 966
I0712 13:19:54.378779 140680526767936 train_utils.py:377] train in step: 967
I0712 13:19:54.796968 140680526767936 train_utils.py:377] train in step: 968
I0712 13:19:55.170996 140680526767936 train_utils.py:377] train in step: 969
I0712 13:19:55.536785 140680526767936 train_utils.py:377] train in step: 970
I0712 13:19:55.902331 140680526767936 train_utils.py:377] train in step: 971
I0712 13:19:56.295607 140680526767936 train_utils.py:377] train in step: 972
I0712 13:19:56.761390 140680526767936 train_utils.py:377] train in step: 973
I0712 13:19:57.190570 140680526767936 train_utils.py:377] train in step: 974
I0712 13:19:57.559489 140680526767936 train_utils.py:377] train in step: 975
I0712 13:19:57.961659 140680526767936 train_utils.py:377] train in step: 976
I0712 13:19:58.386050 140680526767936 train_utils.py:377] train in step: 977
I0712 13:19:58.926371 140680526767936 train_utils.py:377] train in step: 978
I0712 13:19:59.320879 140680526767936 train_utils.py:377] train in step: 979
I0712 13:19:59.687244 140680526767936 train_utils.py:377] train in step: 980
I0712 13:20:00.048933 140680526767936 train_utils.py:377] train in step: 981
I0712 13:20:00.415942 140680526767936 train_utils.py:377] train in step: 982
I0712 13:20:00.782375 140680526767936 train_utils.py:377] train in step: 983
I0712 13:20:01.165971 140680526767936 train_utils.py:377] train in step: 984
I0712 13:20:01.577565 140680526767936 train_utils.py:377] train in step: 985
I0712 13:20:01.936870 140680526767936 train_utils.py:377] train in step: 986
I0712 13:20:02.346632 140680526767936 train_utils.py:377] train in step: 987
I0712 13:20:02.783622 140680526767936 train_utils.py:377] train in step: 988
I0712 13:20:03.179866 140680526767936 train_utils.py:377] train in step: 989
I0712 13:20:03.588820 140680526767936 train_utils.py:377] train in step: 990
I0712 13:20:03.996172 140680526767936 train_utils.py:377] train in step: 991
I0712 13:20:04.360159 140680526767936 train_utils.py:377] train in step: 992
I0712 13:20:04.728724 140680526767936 train_utils.py:377] train in step: 993
I0712 13:20:05.158262 140680526767936 train_utils.py:377] train in step: 994
I0712 13:20:05.626161 140680526767936 train_utils.py:377] train in step: 995
I0712 13:20:06.028260 140680526767936 train_utils.py:377] train in step: 996
I0712 13:20:06.408786 140680526767936 train_utils.py:377] train in step: 997
I0712 13:20:06.840113 140680526767936 train_utils.py:377] train in step: 998
I0712 13:20:07.239783 140680526767936 train_utils.py:377] train in step: 999
I0712 13:20:07.623353 140680526767936 train_utils.py:377] train in step: 1000
I0712 13:20:07.629539 140680526767936 checkpoints.py:120] Saving checkpoint at step: 1000
I0712 13:20:08.199070 140680526767936 checkpoints.py:149] Saved checkpoint at trained_models/matching/longformer/checkpoint_1000
I0712 13:20:08.240102 140680526767936 train_utils.py:396] train in step: 1000, loss: 0.6955999732017517, acc: 0.5087000131607056
I0712 13:20:16.726063 140680526767936 train_utils.py:411] eval in step: 1000, loss: 0.7494, acc: 0.4800
I0712 13:20:16.731632 140680526767936 train_utils.py:421] Testing...
I0712 13:20:25.147902 140680526767936 train_utils.py:424] test in step: 1000, loss: 0.7194, acc: 0.5300
