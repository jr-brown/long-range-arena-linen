2022-07-12 13:25:54.635829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:25:54.636126: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:25:54.636310: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2022-07-12 13:25:54.636511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2022-07-12 13:25:54.636687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:25:54.636885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:25:54.637078: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2022-07-12 13:25:54.637144: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0712 13:25:54.637733 140563755112256 train.py:67] ===========Config Dict============
I0712 13:25:54.638090 140563755112256 train.py:68] available_devices:
- 0
- 1
- 2
- 3
base_type: dual_encoder
batch_size: 4
checkpoint_freq: 10000
data_dir: google_datasets/doc_retrieval/tsv_data/
emb_dim: 128
eval_frequency: 200
factors: constant * linear_warmup * rsqrt_decay
learning_rate: 0.05
max_eval_target_length: 200
max_length: 4000
max_predict_token_length: 50
max_target_length: 200
mlp_dim: 512
model_type: bigbird
num_eval_steps: 50
num_heads: 4
num_layers: 4
num_train_steps: 1001
pooling_mode: CLS
prompt: ''
qkv_dim: 128
random_seed: 0
restore_checkpoints: true
sampling_temperature: 0.6
sampling_top_k: 20
save_checkpoints: true
task_name: aan_pairs
tokenizer: char
trial: 0
vocab_file_path: google_datasets/doc_retrieval/aan/
warmup: 1000
weight_decay: 0.1

I0712 13:25:54.651539 140563755112256 xla_bridge.py:340] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0712 13:25:55.624370 140563755112256 xla_bridge.py:340] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0712 13:25:55.624970 140563755112256 xla_bridge.py:340] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0712 13:25:55.625206 140563755112256 train.py:80] GPU devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)]
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
I0712 13:25:55.625349 140563755112256 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
I0712 13:25:55.690685 140563755112256 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 13:25:55.722849 140563755112256 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 13:25:58.150571 140563755112256 input_pipeline.py:60] Data sample: OrderedDict([('label', 0.0), ('id1', b'W11-1607'), ('id2', b'W02-1713'), ('text1', b"b'1 Introduction Sentence fusion is the process by which content from two or more original sentences is transformed into a single output sentence. It is usually studied in the context of multidocument summarization, since fusing similar sentences can avoid repetition of material which is shared by more than one input. However, human editors and summarizers do not restrict themselves to combining sentences which share most of their content. This paper extends previous work on fusion to the case in which the input sentences are drawn from the same document and express fundamentally different content, while still remaining related enough to make fusion sensible1. 1Unfortunately, we cannot release our corpus due to licensing agreements. Our system is available at https:// Our data comes from a corpus of news articles for which we have un-edited and edited versions. We search this corpus for sentences which were fused (or separated) by the editor; these constitute naturally occurring data for our system. One example from our dataset consists of input sentences (1) and (2) and output (3). We show corresponding regions of the input and output in boldface. (1) The bodies showed signs of torture. (2) They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in the southern state of Guerrero, state police said. (3) The bodies of the men, which showed signs of torture, were left on the side of a highway in Chilpancingo, which is about an hour north of the tourist resort of Acapulco, state police told Reuters. While the two original sentences are linked by a common topic and reference to a shared entity, they are not paraphrases of one another. This could create a problem for traditional fusion systems which first find an alignment between similar dependency graphs, then extract a shared structure. While our system has the same basic framework of alignment and extraction, it performs the two jointly, as parts of a global optimization task. This makes it robust to uncertainty about the hidden correspondences between the sentences. We use structured online learning to find parameters for the system, allowing it to bitbucket.org/melsner/sentencefusion. 54 discover good ways to piece together input sentences by examining examples from our corpus. Sentence fusion is a common strategy in humanauthored summaries of single documents? 36% of sentences in the summaries investigated by Jing and McKeown (1999) contain content from multiple sentences in the original document. This suggests that a method to fuse dissimilar sentences could be useful for single-document summarization. Our dataset is evidence that editing also involves fusing sentences, and thus that models of this task could contribute to systems for automatic editing. In the remainder of the paper, we first give an overview of related work (Section 2). We next describe our dataset and preprocessing in more detail (Section 3), describe the optimization we perform (Section 4), and explain how we learn parameters for it (Section 5). Finally, we discuss our experimental evaluation and give results (Section 6). 2 Related work Previous work on sentence fusion examines the taskin the context of multidocument summarization, targeting groups of sentences with mostly redundant content. The pioneering work on fusion is Barzilay and McKeown (2005), which introduces the framework used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output. Our work most closely follows Filippova and Strube (2008), which proposes using Integer Linear Programming (ILP) for extraction of an output dependency tree. ILP allows specification of grammaticality constraints in terms of dependency relationships (Clarke and Lapata, 2008), as opposed to previous fusion methods (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005) which used language modeling to extract their output. In their ILP, Filippova and Strube (2008) optimize a function based on syntactic importance scores learned from a corpus of general text. While similar methods have been used for the related task of sentence compression, improvements can be obtained using supervised learning (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2009) if a suitable corpus of compressed sentences can be obtained. This paper is the first we know of to adopt the supervised strategy for sentence fusion. For supervised learning to be effective, it is necessary to find or produce example data. Previous work does produce some examples written by humans, though these are used during evaluation, not for learning (a large corpus of fusions (McKeown et al., 2010) was recently compiled as a first step toward a supervised fusion system). However, they elicit these examples by asking experimental subjects to fuse selected input sentences? the choice of which sentences to fuse is made by the system, not the subjects. In contrast, our dataset consists of sentences humans actually chose to fuse as part of a practical writing task. Moreover, our sentences have disparate content, while previous work focuses on sentences whose content mostly overlaps. Input sentences with differing content present a challenge to the models used in previous work. All these models use deterministic node alignment heuristics to merge the input dependency graphs. Filippova and Strube (2008) align all content words with the same lemma and part of speech; Barzilay and McKeown (2005) and Marsi and Krahmer (2005) use syntactic methods based on tree similarity. Neither method is likely to work well for our data. Lexical methods over-align, since there are many potential points of correspondence between our sentences, only some of which should be merged? ?the Doha trade round? and ?U.S. trade representative? share a word, but probably ought to remain separate regardless. Syntactic methods, on the other hand, are unlikely to find any alignments since the input sentences are not paraphrases and have very different trees. Our system selects the set of nodes to merge during ILP optimization, allowing it to choose correspondences that lead to a sensible overall solution. 3 Data and preprocessing Our sentence fusion examples are drawn from a corpus of 516 pre- and post-editing articles from the Thomson-Reuters newswire, collected over a period of three months in 2008. We use asimple greedy method based on bigram count overlaps to align the sentences of each original article to sentences in the edited version, allowing us to find fused sentences. 55 Since these sentences are relatively rare, we use both merges (where the editor fused two input sentences) and splits (where the editor splits an input sentence into multiple outputs) as examples for our system. In the case of a split, we take the edited sentences as input for our method and attempt to produce the original through fusion2. This is suboptimal, since the editor?s decision to split the sentences probably means the fused version is too long, but is required in this small dataset to avoid sparsity. Out of a total of 9007 sentences in the corpus, our bigram method finds that 175 were split and 132 were merged, for a total of 307. We take 92 examples for testing and 189 for training3. Following previous work (Barzilay and McKeown, 2005), we adopt a labeled dependency format for our system?s input. To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the selftrained Charniak parser (McClosky et al, 2006). We then convert to dependencies and apply rules to simplify and label the graph. An example dependency graph is shown in Figure 1. We augment the dependency tree by adding a potential dependency labeled ?relative clause? between each subject and its verb. This allows our system to transform main clauses, like ?the bodies showed signs of torture?, into NPs like ?the bodies, which showed signs of torture?, a common paraphrase strategy in our dataset. We also add correspondences between the two sentences to the graph, marking nodes which the system might decide to merge while fusing the two sentences. We introduce correspondence arcs between pairs of probable synonyms4. We also annotate pronoun coreference by hand and create a correspondence between each pronoun and the heads of all coreferent NPs. The example sentence has only a single correspondence arc (?they? and ?bodies?) be2In a few cases, this creates two examples which share a sentence, since the editor sometimes splits content off from one sentence and merges it into another. 3We originally had 100 testing and 207 training examples, but found 26 of our examples were spurious, caused by faulty sentence segmentation. 4Words with the same part of speech whose similarity is greater than 3.0 according to the information-theoretic WordNet based similarity measure of Resnik (1995), using the implementation of (Pedersen et al, 2004). cause input sentence (1) is extremely short, but most sentences have more. bodies showed signs torture said left were they side highway chilpancingo policestate north hour resort acapulco root root rel sbj obj pp of rel sbj pp by pp of pp in pp about pp of pp of thean aux obj sbj rel merge? Figure 1: The labeled dependency graph for sentences (1) and (2). Dashed lines show a correspondence arc (?bodies? and ?they?) and potential relative clauses between subjects and VPs. 3.1 Retained information Sentence fusion can be thought of as a two-partprocess: first, the editor decides which information from the input sentences to retain, and then they generate a sentence incorporating it. In this paper, we focus on the generation stage. To avoid having to perform content selection5 , we provide our system with the true information selected by the editor. To do this, we align the input sentences with the output by repeatedly finding the longest common substring (LCS) until a substring containing a matching content word can no longer be found. (The LCS is computed by a dynamic program similar to that for edit distance, but unlike edit distance, repeated LCS can handle reordering.) We provide our system with the boundaries of the retained regions as part of the input. For the example above, these are the regions of sentences (1) and (2) marked in boldface. Although this helps the system select the correct information, generating a grammatical and easy-to-read fused sentence is still non-trivial (see examples in section 7). 4 Fusion via optimization Like Filippova and Strube (2008), we model our fusion task as a constrained optimization problem, which we solve using Integer Linear Programming (ILP). For each dependency from word w to head 5As pointed out by Daume III and Marcu (2004) and Krahmer et al (2008), content selection is not only difficult, but also somewhat ill-defined without discourse context information. 56 h in the input sentences, we have a binary variable xh,w, which is 1 if the dependency is retained in the output and 0 otherwise. However, unlike Filippova and Strube (2008), we do not know the points of correspondence between the inputs, only a set of possible points. Therefore, we also introduce 0-1 integer variables ms,t for each correspondence arc, which indicate whether word s in one sentence should be merged with word t in another. If the words are merged, they form a link between the two sentences, and only one of the pair appears in the output. Each dependency x, each word w, and each merger m have an associated weight value v, which is assigned based on its features and the learned parameters of our system (explained in Section 5). Our objective function (4) sums these weight values for the structures we retain: max ? h,w vh,w ? vw ? xh,w + ? s,t vs,t ?ms,t (4) We use structural constraints to require the output to form a single connected tree. (In the following equations, W denotes the set of words, X denotes the set of dependencies and M denotes the potential correspondence pairs.) Constraint (5) requires a word to have at most one parent and (6) allows it to be merged with at most one other word. (7) and (8) require each merged node to have a single parent: ?w ? W, ? h xh,w ? 1 (5) ?w ? W, ? t ms,t ? 1 (6) ?s, t ? M, ms,t ? ? h xh,s + ? h xh,t (7) ?s, t ? M, ms,t + ? h xh,s + ? h xh,t ? 2 (8) (9) forces the output to beconnected by ensuring that if a node has children, it either has a parent or is merged. ?w ? W, ? c xc,w? |W | ? h xh,w ? |W | ? u mu,w ? 0 (9) Certain choices of nodes to merge or dependencies to follow can create a cycle, so we also introduce a rank variable rw ? R for each word and constrain each word (except the root) to have a higher rank than its parent (10). Merged nodes must have equal ranks (11). ?w,h ? X,|X|xh,w + rh ? rw ? |X| ? 1 (10) ?s,t ? M,|X|ms,t + rs ? rt ? |X| (11) We also apply syntactic constraints to make sure we supply all the required arguments for each word we select. We hand-write rules to prevent the system from pruning determiners, auxiliary verbs, subjects, objects, verbal particles and the word ?not? unless their head word is also pruned or it can find a replacement argument of the same type. We learn probabilities for prepositional and subclause arguments using the estimation method described in Filippova and Strube (2008), which counts how often the argument appears with the head word in a large corpus. While they use these probabilities in the objective function, we threshold them and supply constraints to make sure all argument types with probability > 10% appear if the head is chosen. Word merging makes it more difficult to write constraints for required arguments, because a word s might be merged with some other word t which is attached to the correct argument type (for instance, if s and t are both verbs and they are merged, only one of them must be attached to a subject). This condition is modeled by the expression ms,t ?xt,a, where a is a argument word of the appropriate type. This expression is non-linear and cannot appear directly in a constraint, but we can introduce an auxiliary variable gs,t,A which summarizes it for a set of potential arguments A, while retaining a polynomial-sized program: ?s,t ? M, ? a?A xa,s+ ? a?A xa,t + |W |ms,t ? |W + 1|gs,t,A ? 0 (12) (13) then requires a word s to be connected to an argument in set A, either via a link or directly: 57 ?h xs,h ? 2 ? t:{s,t?M} gs,t,A ? 2 ? a?A xa,s ? 0 (13) The resulting resulting ILP is usually solvable within a second using CPLEX (Ilog, Inc., 2003). 4.1 Linearization The output of the ILP is a dependency tree, not an ordered sentence. We determine the final ordering mostly according to the original word order of the input. In the case of a merged node, however, we must also interleave modifiers of the merged heads, which are not ordered with respect to one another. We use a simple heuristic, trying to place dependencies with the same arc label next to one another; this can cause errors. We must also introduce conjunctions between arguments of the same syntactic type; our system always inserts ?and?. Finally, we choose a realization for the dummyrelative pronoun THAT using a trigram language model (Stolcke, 2002). A more sophisticated approach (Filippova and Strube, 2009) might lead to better results. 5 Learning The solution which the system finds depends on the weights v which we provide for each dependency, word and merger. We set the weights based on a dot product of features ? and parameters ?, which we learn from data using a supervised structured technique (Collins, 2002). To do so, we define a loss function L(s, s?) ? R which measures how poor solution s is when the true solution is s?. For each of our training examples, we compute the oracle solution, the best solution accessible to our system, by minimizing the loss. Finally, we use the structured averaged perceptron update rule to push our system?s parameters away from bad solutions and towards the oracle solutions for each example. Our loss function is designed to measure the highlevel similarity between two dependency trees containing some aligned regions. (For our system, these are the regions found by LCS alignment of the input strings with the output.) For two sentences to be similar, they should have similar links between the regions. Specifically, we define the paths P (s,C) in a tree s with a set of regions C as the set of word pairs w,w? where w is in one region, w? is in another, and the dependency path between w and w? lies entirely outside all the regions. An example is given in figure 2. left on the side of a highway...were bodies showedof the men, which signs of torture state police told Reuters root Figure 2: Paths between retained regions in sentence (3). Boxes indicate the retained regions. Our loss (equation 14) is defined as the number of paths in s and s? which do not match, plus a penalty K1 for keeping extra words, minus a bonus K2 for retaining words inside aligned regions: L(s,s?;C,K) = |(P (s,C) ? P (s?, C)) \\\\ (P (s,C) ? P (s?, C))| +K1|w ? s \\\\ C| ?K2|w ? s ? C| (14) To compute the oracle s?, we must minimize this loss function with respect to the human-authored reference sentence r over the space S of fused dependency trees our system can produce. s? = argmins?S L(s, r) (15) We perform the minimization by again using ILP, keeping the constraints from the original program but setting the objective to minimize the loss. This cannot be done directly, since the existence of a path from s to t must be modeled as a product of x variables for the dependencies forming the path. However, we can again introduce a polynomial number of auxiliary variables to solve the problem. We introduce a 0-1 variable qsh,w for each path start word s and dependency h,w, indicating whether the dependency from h to w is retained and forms part of a path from s. Likewise, we create variables qsw for each word and qsu,v for mergers6. Using these variables, we can state the loss function linearly as (16), 6The q variablesare constrained to have the appropriate values in the same way as (12) constrains g. We will print the specific equations in a technical report when this work is published. 58 where P (r, C) is the set of paths extracted from the reference solution. min ? s,t qsh,t ? 2 ? s,t?P (r,C) qsh,t (16) The oracle fused sentence for the example (1) and (2) is (17). The reference has a path from bodies to showed, so the oracle includes one as well. To do so, follows a relative clause arc, which was not in the original dependency tree but was created as an alternative by our syntactic analysis. (At this stage of processing, we show the dummy relative pronoun as THAT.) It creates a path from left to bodies by choosing to merge the pronoun they with its antecedent. Other options, such as linking the two original sentences with ?and?, are penalized because they would create erroneous paths? since there is no direct path between root and showed, the oracle should not make showed the head of its own clause. (17) the bodies THAT showed signs of torture were left on the side of a highway in Chilpancingo about an hour north of the tourist resort of Acapulco state police said The features which represent each merger, word and dependency are listed in Table 1. We use the first letters of POS tags (in the Penn Treebank encoding) to capture coarse groupings such as all nouns and all verbs. For mergers, we use two measures of semantic similarity, one based on Roget?s Thesaurus (Jarmasz and Szpakowicz, 2003) and another based on WordNet (Resnik, 1995). As previously stated, we hand-annotate the corpus with true pronoun coreference relationships (about 30% of sentences contain a coreferent pronoun). Finally, we provide the LCS retained region boundaries as explained above. Once we have defined the feature representation and the loss function, and can calculate the oracle for each datapoint, we can easily apply any structured online learning algorithm to optimize the parameters. We adopt the averaged perceptron, applied to structured learning by (Collins, 2002). For each example, we extract a current solution st by solving the ILP (with weights v dependent on our parameters ?), then perform an update to ? which forces the system away from st and towards the oracle solution s?. The update at each timestep t (18) depends on the loss, the global feature vectors ?, and COMPONENT FEATURES MERGER SAME WORD SAME POS TAGS SAME FIRST LETTER OF THE POS TAGS POS TAG IF WORD IS SAME COREFERENT PRONOUN SAME DEPENDENCY ARC LABEL TO PARENT ROGET?S SIMILARITY WORDNET SIMILARITY FIRST LETTER OF BOTH POS TAGS WORD POS TAG AND ITS FIRST LETTER WORD IS PART OF RETAINED CHUNK IN EDITOR?S FUSION DEPENDENCY POS TAGS OF THE PARENT AND CHILD FIRST LETTER OF THE POS TAGS TYPE OF THE DEPENDENCY DEPENDENCY IS AN INSERTED RELATIVE CLAUSE ARC PARENT IS RETAINED IN EDITOR?S SENTENCE CHILD IS RETAINED IN EDITOR?S SENTENCE Table 1: List of Features. a learning rate parameter ?. (Note thatthe update leaves the parameters unchanged if the loss relative to the oracle is 0, or if the two solutions cannot be distinguished in terms of their feature vectors.) ?t+1 = ?t + ?(L(st, r)?L(s?, r))(?(s?)??(st)) (18) We do 100 passes over the training data, with ? decaying exponentially toward 0. At the end of each pass over the data, we set ?? to the average of all the ?t for that pass (Freund and Schapire, 1999). Finally, at the end of training, we select the committee of 10 ?? which achieved lowest overall loss and average them to derive our final weights (Elsas et al, 2008). Since the loss function is nonsmooth, loss does not decrease on every pass, but it declines overall as the algorithm proceeds. 6 Evaluation Evaluating sentence fusion is a notoriously difficult task (Filippova and Strube, 2008; Daume III and Marcu, 2004) with no accepted quantitative metrics, so we have to depend on human judges for evaluation. We compare sentences produced by our system to three alternatives: the editor?s fused sentence, a readability upper-bound and a baseline formed by splicing the input sentences together by inserting the word ?and? between each one. The readability upper 59 bound is the output of parsing and linearization on the editor?s original sentence (Filippova and Strube, 2008); it is designed to measure the loss in grammaticality due to our preprocessing. Native English speakers rated the fused sentences with respect to readability and content on a scale of 1 to 5 (we give a scoring rubric based on (Nomoto, 2009)). 12 judges participated in the study, for a total of 1062 evaluations7 . Each judge saw the each pair of inputs with the retained regions boldfaced, plus a single fusion drawn randomly from among the four systems. Results are displayed in Table 2. System Readability Content Editor 4.55 4.56 Readability UB 3.97 4.27 ?And?-splice 3.65 3.80 Our System 3.12 3.83 Table 2: Results of human evaluation. 7 Discussion Readability scores indicate that the judges prefer human-authored sentences, then the readability upper bound, then ?and?-splicing and finally our system. This ordering is unsuprising considering that our system is abstractive and can make grammatical errors, while the remaining systems are all based on grammatical human-authored text. The gap of .58 between human sentences and the readability upper bound represents loss due to poor linearization; this accounts for over half the gap between our system and human performance. For content, the human-authored sentences slightly outperform the readability upper bound? this indicates that poor linearization has some effect on content as well as readability. Our system is slightly better than ?and?-splicing. The distribution of scores is shown in Table 3. The system gets more scores of 5 (perfect), but it occasionally fails drastically and receives a very low score; ?and?-splicing shows less variance. Both metrics show that, while our system does not achieve human performance, it does not lag behind 7One judge completed only the first 50 evaluations; the rest did all 92. 1 2 3 4 5 Total ?And?-splice 3 43 60 57 103 266System 24 24 39 58 115 260 Table 3: Number of times each Content score was assigned by human judges. by that much. It performs quite well on some relatively hard sentences and gets easy fusions right most of the time. For instance, the output on our example sentence is (19), matching the oracle (17). (19) The bodies who showed signs of torture were left on the side of a highway in Chilpancingo about an hour north of the tourist resort of Acapulco state police said. In some cases, the system output corresponds to the ?and?-splice baseline, but in many cases, the ?and?-splice baseline adds extraneous content. While the average length of a human-authored fusion is 34 words, the average splice is 49 words long. Plainly, editors often prefer to produce compact fusions rather than splices. Our own system?s output has an average length of 33 words per sentence, showing that it has properly learned to trim away extraneous information from the input. We instructed participants to penalize the content score when fused sentences lost important information or added extra details. Our integration of node alignment into our solution procedure helps the system to find good correspondences between the inputs. For inputs (20) and (21), the system was allowed to match ?company? to ?unit?, but could also match ?terrorism? to ?administration? or to ?lawsuit?. Our system correctly merges ?company? and ?unit?, but not the other two pairs, to form our output (22); the editor makes the same decision in their fused sentence (23). (20) The suit claims the company helped fly terrorism suspects abroad to secret prisons. (21) Holder?s review was disclosed the same day as Justice Department lawyers repeated a Bush administration state-secret claim in a lawsuit against a Boeing Co unit. 60 (22) Review was disclosed the same day as Justice Department lawyers repeated a Bush administration claim in a lawsuit against a Boeing Co unit that helped fly terrorism suspects abroad to secret prisons. (23) The review was disclosed the same day that Justice Department lawyers repeated Bush administration claims of state secrets in a lawsuit against a Boeing Co <BA.N> unit claiming it helped fly terrorism suspects abroad to secret prisons. In many cases, even when the result is awkward or ungrammatical, the ILP system makes reasonable choices of mergers and dependencies to retain. For inputs (24) and (25), the system (26) decides ?Secretary-General? belongs as a modifier on ?de Mello?, which is in fact the choice made by the editor (27). In order to add the relative clause, the editor paraphrased ?de Mello?s death? as ?de Mello was killed?. Our system, without this paraphrase option, is forced to produce the improper phrase ?de Mello?s death who?; a wider array of paraphrase options might lead to better results. This example also demonstrates that the system does not simply keep the LCS-aligned retained regions and throw away everything else, since the result would be ungrammatical. Here it links the selected content by also choosing to keep ?could have been?, ?an account? and ?death?. (24) Barker mixes an account ofVieira de Mello?s death with scenes from his career, which included working in countries such as Mozambique, Cyprus, Cambodia, Bangladesh, and the former Yugoslavia. (25) Had he lived, he could have been a future U.N. Secretary-General. (26) Barker mixes an account of Vieira de Mello?s death who could been a future U.N. secretary-general with scenes from career which included working in countries as such Mozambique Cyprus Cambodia and Bangladesh (27) Barker recounted the day Vieira de Mello, a Brazilian who was widely tipped as a future U.N. Secretary-General, was killed and mixes in the story of the 55-year-old?s career, which included working in countries such as Mozambique, Cyprus, Cambodia, Bangladesh, and Yugoslavia. Many of our errors are due to our simplistic linearization. For instance, we produce a sentence beginning ?Biden a veteran Democratic senator from Delaware that Vice president-elect and Joe...?, where a correct linearization of the output tree would have begun ?Vice President-elect Joe Biden, a veteran Democratic senator from Delaware that...?. Some errors also occur during the ILP tree extraction process. In (28), the system fails to mark the arguments of ?took? and ?position? as required, leading to their omission, which makes the output ungrammatical. (28) The White House that took when Israel invaded Lebanon in 2006 showed no signs of preparing to call for restraint by Israel and the stance echoed of the position.8 Conclusion We present a supervised method for learning to fuse disparate sentences. To the best of our knowledge, it is the first attempt at supervised learning for this task. We apply our method to naturally occurring sentences from editing data. Despite using text generation, our system is comparable to a nonabstractive baseline. Our technique is general enough to apply to conventional fusion of similar sentences as well? all that is needed is a suitable training dataset. We hope to make use of the new corpus of McKeown et al (2010) for this purpose. We are also interested in evaluating our approach on the fused sentences in abstractive single-document summaries. The performance of our readability upper bound suggests we could improve our results using better tree linearization techniques and parsing. Although we show results for our system using handannotated pronoun coreference, it should be possible to use automatic coreference resolution instead. Paraphrase rules would help our system replicate some output structures it is currently unable to match (for instance, it cannot convert between the copular ?X is Y? and appositive ?X, a Y? constructions). Currently, the system has just one such 61 rule, which converts main clauses to relatives. Others could potentially be learned from a corpus, as in (Cohn and Lapata, 2009). Finally, in this study, we deliberately avoid investigating the way editors choose which sentences to fuse and what content from each of them to retain. This is a challenging discourse problem that deserves further study.Acknowledgements We are very grateful to Alan Elsner, Howard Goller and Thomas Kim at Thomson-Reuters for giving us access to this dataset. We thank Eugene Charniak for his supervision, our colleagues in BLLIP for their comments, Kapil Thadani and Kathy McKeown for discussing the project with us, and our human evaluators for completing a task which turned out to be extremely tedious. Part of this work was funded by a Google Fellowship in Natural Language Processing.'"), ('text2', b'b\'1 Introduction In this paper we present XtraGen, a recently developed software system for the flexible, real-time generation of natural language that can be easily integrated into real-world, industrial application environments through its open XML- and Java-based implementation and interfaces. Our motivation for developing a completely new generation system started when we made the same observation as stated in the following quote: There are thousands, if not millions, of application programs in everyday use that automatically generate texts; but probably fewer than ten of these programs use the linguistic and knowledge-based techniques that have been studied by the natural language generation community. (Reiter, 1995) The goal of our company is to develop state-ofthe-art software and hence we wanted to change the portrayed situation at least in our environment for the applications we create. Therefore we started to experiment with XSL (World Wide Web Consortium, 2001) to generate natural language as suggested in (Cawsey, 2000) and (Wilcock, 2001). But we figured out fairly soon that XSL did not satisfy our needs and desires because with this mechanism ? we were not able to appropriately handle the issue of morphology, ? we could not parameterize the generation process at the desired level and ? we had no possibility to generate alternatives or recover from dead ends during generation since XSL is lacking a backtracking mechanism. Therefore we decided to develop our own natural language generation system that incorporates on the one hand many ideas found in XSL but on the other hand tries to give a solution the above described problems. 2 Generation Grammars 2.1 Formalism The grammar formalism conceived for the XtraGen system has been developed from an applicationoriented point of view. This means that from our standpoint real-world applications hardly ever require a full and complete linguistic coverage which is striven for by linguistically motivated generation systems. Therefore our formalism is based on extended templates that allow the inclusion of predefined and dynamically retrieved text, constraintbased inflection and a context-free selection mechanism. The development of this formalism was strongly influenced by the ideas found in the (Lispbased) formalism of the TG/2 system (Busemann, 1996; Wein, 1996) and the YAG system (Channarukul, 1999). A template has the overall form as depicted in the Backus-Naur Form in figure 2.1. Each part of the template will be elaborated below. 2.2 Conditions Conditions describe the exact circumstances under which a certain template can be applied and its actions executed. There are two distinct basic types <template id="String" category="String"> <conditions> Condition* </conditions> <parameters> Parameter* </parameters> <actions> Action+ </actions> <constraints> Constraint* </constraints> </template> Figure 1: Overview of a XtraGen template in Backus-Naur Form of conditions: Simple-Conditions and ComplexConditions. They in turn are the supertypes for more specific conditions: Simple-Condition They form the actual tests that are applied to the input structure. A set of commonly used conditions is already predefined such as ones that test for equality or that test whether certain information is existent in the input structure. If there is a need for some very specific conditional testing that cannot be realized with the existing meansa developer is free to implement and add its own conditional types. Complex-Condition This type of condition makes it possible to combine several conditions into a more complex one. Three predefined Complex-Conditions exist: the AndCondition, the Or-Condition and the NotCondition. Additional Complex-Conditions can also be added by providing an implementation for them. 2.3 Parameterization Parameterization is an easy and flexible means to guide and control the generation process with regard to different linguistic preferences such as matters of style or rhetorical structures. Parameterization works by introducing a preference mechanism that provides the possibility of dynamically sorting the application of templates according to a given set of parameters. <conditions> <or> <and> <condition type="equal"> <get path="/recall"/> <value>95</value> </condition> <condition type="less"> <get path="/accuracy"/> <value>90</value> </condition> </and> <not> <condition type="exist"> <get path="/exception"/> </condition> </not> </or> </conditions> Figure 2: Example of some complex interleaved conditions The way parametrization works in our system is a two-step process: Adding of parameters to templates During the design of a generation grammar the writer adds one or more parameters to some templates as in the example in figure 3. Here the upper template is intended to be used during the generation of text targeted at experts and the lower one in case text is to be produced for novices (level is expert in one template and novice in the other). Both of the templates are preferably used when a low verbosity level is desired (verbosity is low in both cases). Setting of the parameters at runtime At runtime the parameters corresponding to the ones defined in the grammar are set to the desired values. To continue our example, we now set the value of the parameter level to expert (see figure 4) and hence the template in the upper box would be selected. The particularity of our system is that parameters can be assigned a weight and thus a priority. In our example we might want to give a higher priority to the parameter level than to the parameter verbosity as shown in figure 5 This now sorts the application of templates in a way that they are first sorted according to their level of verbosity and the result is further sorted according to the level of expertise. <template id="explainExpert" category="explain"> <parameters> <parameter name="level" value="expert"> <parameter name="verbosity" value="low"> </parameters> ... </template> <template id="explainNovice" category="explain"> <parameters> <parameter name="level" value="novice"> <parameter name="verbosity" value="low"> </parameters> ... </template> Figure 3: Example of using parameters on the level of generation grammars generator.addParameter("level", "expert"); Figure 4: Example of using parameters on the level of programming code generator.addParameter("level", "novice", 0.75); generator.addParameter("verbosity", "low", 0.5); Figure 5: Example of using parameters with a weight specified on the level of programming code 2.4 Actions In the case that all conditions of a given template have been tested successfully (see section 2.2) the actions contained in the actions-part of the template are executed. There are four different types of actions that can appear: String-Action, Getter-Action, InflectionAction and Selection-Action. The actual purpose of each of them is quite different but all of them return a result string when executed successfully. String-Action This typeof action simply returns a statically specified string as a result ? a socalled canned text. Getter-Action With a Getter-Action it is possible to directly access and retrieve data from the entered input structure. The syntax used for specifying the path to the data conforms to the syntax of XPath (World Wide Web Consortium, 1999). There is no additional processing done on the returned data. <get path="/values/startTime"/> Inflection-Action This action inflects a stem according to the defined morphological constraints and returns the result. The stem can be stated statically in the grammar as in case (a) or can be dynamically retrieved from the input structure as in case (b). The needed morphological constraints are furnished by the constraints-part of the template to which the given label provides a link (confer to section 2.5 below for details). (a) <inflect stem="bring" label="X0"/> (b) <inflect path="/action" label="X0"/> Selection-Action The Selection-Action can actually be seen as the most important of the actions since it accounts for the context-free backbone of the system. It allows to select another template directly via a specified identifier as in case (a) or via a given category as in case (b). In the second case several templates might have the given category and hence backtracking might be invoked at his point. (see section 5.1) (a) <select id="top"/> (b) <select category="top" optional="true"/> Selections can also be declared optional as in (b) which means that in case the selection of the template fails no backtracking is invoked and simply an empty string is returned. 2.5 Constraints and Morphology The treatment of morphology is naturally one of the major issues in the context of a complete natural language generation especially when working with morphologically rich languages such as German, Russian or Finnish. Therefore we took great care to design and develop a morphological subsystem that is powerful and flexible yet easy to understand and use. The actual realization of the component is based on a constraint-based inheritance algorithm that follows the example of PATR-II (Shieber et al, 1989). In the (overly simplified) example in figure 6 one can get a glimpse on how the morphology works: There are two Selection-Actions, the first one labelled X0, the second one labelled X1. The given constraint now tells that the attribute number of X0 is the same as the attribute number of X1 and sets it dynamically to the value retrieved by the GetterAction. <template ...> <actions> <select category="determiner" label="X0"/> <select category="noun" label="X1"/> </actions> <constraint> <place label="X0" attribute="number"/> <place label="X1" attribute="number"/> <get path="/categoryNumber"/> </constraint> </template> Figure 6: Example of using constraints 2.6 Compilation In order to be able to work with a generation grammar the generation engine requires the grammar (and its templates) to exist in the form of a Java object. But since the original format of the grammar is plain XML this format must be transformed through a compilation process into the internally needed representation. Our system is capable to perform such a compilation in two different ways: Just-in-time Compilation With this technique the required templates are compiled from their XML source into their correspondingJava objects at runtime of the generation engine, i.e. during the actual generation process. This type of compilation is advised only for smaller grammars or during the development and testing of a new grammar since the constant interleaving of compilation on the one hand and the actual generation process on the other leads to some quite noticeable overhead. This overhead is naturally not acceptable when XtraGen is used in real-time applications. Pre-Compilation This type of compilation allows to compile the whole grammar before its actual deployment during the generation process. The pre-compilation of grammars can improve the performance of the generator-engine tremendously and is therefore to be preferred in most situations. (The pre-compilation of generation grammars is very similar to the Translets approach in XSL (Apache XML Project, 2002) where XSL stylesheets are compiled in advance into Java objects.) 3 Input In contrast to other generation systems that require their input to adhere to some particular (and most of the time proprietary) encoding format the core engine of our system only demands its input to be a valid XML structure. The actual restrictions on the input are imposed only at the level of the generation grammars in terms of their access to the input (see section 2.4 on Getter-Actions and Inflection-Actions). This can obviously lead to a severe drawback: In case that either the generation grammar or the input structure changes heavily there might emerge a complete mismatch between the XPath specified in the actions and the actual structure of the input. Under circumstances when it is not feasible to change either the input structure or the grammar, we propose to introduce an additional mapping layer between input and generator that is based on a XSL stylesheet and that dynamically maps the input in the way that is needed by the grammar. 4 Editor We have stressed in the sections before that we believe our formalism to be powerful yet very straightforward to implement and use. But when developing larger grammars for real-world applications it becomes quite a demanding, non-trivial task to keep track of all the templates and especially of the relations between them (e.g. relations on the level of morphological constraints) Common XML editors are of no help at this point since they cannot show such relations at all. Therefore the development of egram, a Javabased graphical editor for generation grammars is on its way at the site of our cooperation partner DFKI (German Research Center for Artificial Intelligence). After the completion of its development this tool will allow to comfortably edit all aspects of generation grammars and templates. Among many other things the editor will be able to depict the whole generation grammar and process in a graphical tree format in which dependencies between templates are shown in an intuitive way. 5 Implementation The realization and implementation of the XtraGen system is based entirely on the two cornerstones Java and XML. XML was chosen because it has become the defacto standard language in many (if not most) scenarios where information transfer takes place. This in turn is caused byits unique capabilities to encode information in a way that is easy to read, process, and generate (even for human beings as in the case of our formalism). Java was chosen because it provides many mechanism to bolster the productivity of a programmer during the development of new software with such things as an extensive programming interface or automatic memory management for example. An additional advantage of Java is the availability of many free and readily usable open-source packages that provide a host of diverse functionalities. The most important ones in our project were the different XML packages and in particular the XML parser Xerces or the XSLT engine Xalan (Apache XML Project, 2002). 5.1 Backtracking During the generation process it is possible that two different templates are applicable at the same time (i.e. they have the same category and all of their conditions are satisfied). Now if one of the templates is selected this leads to one of two different results: ? The application of the template was successful which means that all of its actions could be successfully executed and a result was returned. ? The application of the template failed because the execution of one or more of its actions was not successful. But the failure of a template described above does not mean that there exist no solution at all. Therefore we backtrack to the point where the unsuccessful template was selected and apply another template. This procedure is repeated until there are no more templates at this backtrack point. The underlying implementation of the backtracking mechanism is quite elaborated since it has to take several important issues into account, the most important ones are: Performance issues We implemented several different mechanisms that help to tremendously enhance the performance during the backtracking phase such as the memorization of partial solution. Constraint issues We had to take great care of the constraint inheritance mechanism during the backtracking implementation so that an invocation of backtracking does not lead to a misguided percolation of constraints and hence a corrupted morphology. 5.2 Programming Interface So far we have talked about the deployment of XtraGen only on the level of generation grammars and their XML-based formalism. Now we turn to the description of the tasks that have to be undertaken on the level of programming code to make the system run. The following shows the individual steps that are be taken to generate some output with XtraGen: Creating a new generator-engine The very first thing to do in order to get the whole system running is to create a new Generator object which represents the core generation engine: Generator generator = new Generator(); By doing so one implicitly creates objects for the internal subcomponents such as the already mentioned morphological component and puts them under the control of the generation engine. Setting the start-category or -id The generation engine needs to know which template it should start from. This is done by specifying either a start-category as in case (a) or a start-identifier as in case (b). (a) generator.setStartCategory( String category); (b) generator.setStartId(Stringid); Setting the grammar Without a generation grammar it would naturally be impossible to generate any output at all. There are two different possibilities to pass a grammar to the generation engine: (a) generator.setGrammarDocument( Document grammar); (b) generator.setGrammar( Grammar grammar); In the first case a Document object (World Wide Web Consortium, 2000) that contains the grammar in parsed XML-format is passed, in the second case a pre-compiled Grammar object is passed. (see section 2.6) Setting the input In addition to the grammar the generation engine needs an input structure to generate from. This can be set as follows: generator.setInputDocument( Document input); Again, the parameter passed is a Document object that contains the input in parsed XML format. The input can be reassigned between two calls to the generation engine. Setting parameters In subsection 2.3 we talked about the use of parameters to control and guide the generation process. The way parameterization works is explained in detail there. To set parameters at runtime one has to add the following methods: (a) generator.addParameter( String name, String value); (b) generator.addParameter( String name, String value, double weight); This step is only needed if parameterization is desired. Otherwise these methods can be omitted and parameterization is turned automatically off. Run the generation process To now actually start the generation process and get some output, one of the following calls can be used: (a) String result = generator.generate(); (b) Document result = generator.generateDocument(); The difference between the two calls is that in case (a) a simple String containing the result is returned whereas in the case (b) a Document object is passed back. 6 Evaluation At the end of a software development phase any newly created system must proof in an evaluation phase whether it reaches its predefined goals. (Mellish and Dale, 1998) This is especially true in an industrial context with commercial applications as in our case. The context for the evaluation of XtraGen was provided by X-Booster (Beauregard et al, 2002) which is another project at our site that was concurrently developed with our system. This system is an optimized implementation of Slipper (Cohen and Singer, 1999), a binary classifier that induces rules via boosting and combines a set of those classifiers to solve multi-class problems. It was the goal to successfully integrate XtraGen into this system. The motivation behind this is based on the fact that common classification systems are quite nontransparent in regard to their inner workings. Therefore it becomes rather difficult to understand how and why certain classification decisions are made by those systems. We departed at exactly this point: XtraGen was to be used to automatically generate texts that explain the learning phase of the X-Booster and hence make the classification more transparent. As an additional ?gadget? we wanted to create the explanations in all the languages that are spoken at our company site: English, German, French, Italian, Russian, Bulgarian and Turkish. 6.1 Integration Tasks As the very first task of the integration procedure we needed to answer the question what to actually output to the user and in which exact formatthis output should be. We decided on producing a description of the complete learning phase with two kinds of output texts: One targeted at experts and one for novice users. The format chosen for the final output was HTML. Now we needed to adapt the code of X-Booster slightly to make the meta data about the learning phase accessible from the outside. To do so we wrote some small methods that simply returned in XML format the meta data which were only stored in internal variables up to this point. The next step was to add to X-Booster?s own code the code for calling the generation engine and for transforming the result into the final output-format. This was done as described in section 5.2. Finally (and most importantly) the generation grammars for the different languages were developed. This happened in the way that we first set-up a prototypical grammar for English which we tested extensively. Then in a second step the grammars for the other languages were modelled according to this exemplary one. For this we worked together with different native-speaking colleagues that translated the original English grammar into their language. 6.2 Sample Template and Output Figure 7 below shows a small portion of the output returned after running X-Booster together with the integrated XtraGen on a given training set. This result was produced by using the English generation grammar and the parameters set for producing texts targeted at novice users. The number of documents is 37, divided into 2 different categories. The results have been produced using 3 fold-cross-validation which means that the data-set is divided into 1/3 test-set and 2/3 training-set. The learner is trained on the training-data only and evaluated on the test-set which has not been presented before. We repeat this process 3 times on non-intersecting testdata. The overall result is then computed finally as the average of all performed tests. The average accuracy reaches 100.0% which is achieved by applying 5 rules. Figure 7: Sample output from X-Booster Figure 8 shows parts of the template that produced the second paragraph of this sample output. The template is complete in the sense that all different aspects of a template are exposed and it is only simplified in the respect that similar parts of the template are left out. <template id="foldNumberNovice" category="foldNumber"> <conditions> <and> <condition type="exists"/> <get path="/foldNumber"/> </condition> ... </and> </conditions> <parameters> <parameter name="level" value="novice"/> ... </parameters> <actions> ... We repeat this process <get path="/foldNumber"/> <inflect stem="time" label="X0"/> ... </actions> <constraints> <constraint> <place label="X0" attribute="number"/> <get path="/foldNumber"/> </constraint> ... </constraints> </template> Figure 8: Sample template from X-Booster 7 Outlook and Future Work Because the above described evaluation proofed to be quite successful it was decided to further deploy XtraGen at our site and to integrate it into new products and projects. One of the first of these projects is Mietta-II (Multilingual Information Environment for Travel and Tourism Applications), an European Commission funded industrial project with the goal of developing a comprehensive web search and information portal that specializes on the tourism sector. (Additional application scenarios are already envisagedfor a possible later stage of the project.) In this environment we will apply natural language generation to produce texts and messages for various types of media such as dynamically generated web pages, paper leaflets, hand-held devices and in particular cellular phones. For the last of those media we are exploring the possibility of producing voice-enabled output with a dedicated voice server that is based on VoiceXML (World Wide Web Consortium, 2002) or JSML (Sun Microsystems, 1999). We are experimenting at the moment with the different possible outputs and on how these outputs can be encoded in a generation grammar enriched with VoiceXML or JSML tags.8 Conclusion In this contribution we presented XtraGen, a realtime, template-based natural language generation system designed for real-world applications and based on standard XML- and Java-technologies. We described in detail the different aspects of its generation grammars with an emphasis on their formalism. Then we covered architectural and implementational issues of the system. After depicting the evaluation done for XtraGen we concluded with presenting a new project where the system is used and where new ideas are experimented with.Acknowledgements We are grateful to Sven Schmeier, Huiyan Huang and Oliver Plaehn for their collaboration on XBooster and the Mietta-II project and especially to Stephan Busemann for many fruitful discussions on TG/2. This work was carried out in the Mietta-II project funded by the European Commission under the Fifth Framework Programme IST-2000-30161.\'')])
INFO:tensorflow:Finished getting dataset.
I0712 13:25:58.208436 140563755112256 input_pipeline.py:91] Finished getting dataset.
I0712 13:25:58.208628 140563755112256 input_pipeline.py:94] Using char-level/byte dataset..
I0712 13:25:58.327786 140563755112256 train.py:106] Vocab Size: 257
I0712 13:26:15.943259 140563755112256 checkpoints.py:242] Found no checkpoint directory at trained_models/matching/bigbird
I0712 13:26:16.616879 140563755112256 train_utils.py:370] Starting training
I0712 13:26:16.617063 140563755112256 train_utils.py:371] ====================
I0712 13:27:12.283980 140563755112256 train_utils.py:377] train in step: 0
I0712 13:27:12.319594 140563755112256 train_utils.py:377] train in step: 1
I0712 13:27:12.382498 140563755112256 train_utils.py:377] train in step: 2
I0712 13:27:12.449487 140563755112256 train_utils.py:377] train in step: 3
I0712 13:27:12.516159 140563755112256 train_utils.py:377] train in step: 4
I0712 13:27:12.591423 140563755112256 train_utils.py:377] train in step: 5
I0712 13:27:12.653617 140563755112256 train_utils.py:377] train in step: 6
I0712 13:27:12.722882 140563755112256 train_utils.py:377] train in step: 7
I0712 13:27:12.785213 140563755112256 train_utils.py:377] train in step: 8
I0712 13:27:12.848614 140563755112256 train_utils.py:377] train in step: 9
I0712 13:27:12.912837 140563755112256 train_utils.py:377] train in step: 10
I0712 13:27:12.976807 140563755112256 train_utils.py:377] train in step: 11
I0712 13:27:13.041204 140563755112256 train_utils.py:377] train in step: 12
I0712 13:27:13.105262 140563755112256 train_utils.py:377] train in step: 13
I0712 13:27:13.170403 140563755112256 train_utils.py:377] train in step: 14
I0712 13:27:13.234314 140563755112256 train_utils.py:377] train in step: 15
I0712 13:27:13.298223 140563755112256 train_utils.py:377] train in step: 16
I0712 13:27:13.363118 140563755112256 train_utils.py:377] train in step: 17
I0712 13:27:13.427847 140563755112256 train_utils.py:377] train in step: 18
I0712 13:27:13.492004 140563755112256 train_utils.py:377] train in step: 19
I0712 13:27:13.556201 140563755112256 train_utils.py:377] train in step: 20
I0712 13:27:13.626030 140563755112256 train_utils.py:377] train in step: 21
I0712 13:27:13.689032 140563755112256 train_utils.py:377] train in step: 22
I0712 13:27:13.753078 140563755112256 train_utils.py:377] train in step: 23
I0712 13:27:13.817759 140563755112256 train_utils.py:377] train in step: 24
I0712 13:27:13.881656 140563755112256 train_utils.py:377] train in step: 25
I0712 13:27:13.950546 140563755112256 train_utils.py:377] train in step: 26
I0712 13:27:14.012609 140563755112256 train_utils.py:377] train in step: 27
I0712 13:27:14.077077 140563755112256 train_utils.py:377] train in step: 28
I0712 13:27:14.141637 140563755112256 train_utils.py:377] train in step: 29
I0712 13:27:14.208709 140563755112256 train_utils.py:377] train in step: 30
I0712 13:27:14.274669 140563755112256 train_utils.py:377] train in step: 31
I0712 13:27:14.337891 140563755112256 train_utils.py:377] train in step: 32
I0712 13:27:14.402050 140563755112256 train_utils.py:377] train in step: 33
I0712 13:27:14.469340 140563755112256 train_utils.py:377] train in step: 34
I0712 13:27:14.533293 140563755112256 train_utils.py:377] train in step: 35
I0712 13:27:14.597705 140563755112256 train_utils.py:377] train in step: 36
I0712 13:27:14.671467 140563755112256 train_utils.py:377] train in step: 37
I0712 13:27:14.736892 140563755112256 train_utils.py:377] train in step: 38
I0712 13:27:14.799968 140563755112256 train_utils.py:377] train in step: 39
I0712 13:27:14.862969 140563755112256 train_utils.py:377] train in step: 40
I0712 13:27:14.928317 140563755112256 train_utils.py:377] train in step: 41
I0712 13:27:14.990959 140563755112256 train_utils.py:377] train in step: 42
I0712 13:27:15.054758 140563755112256 train_utils.py:377] train in step: 43
I0712 13:27:15.118997 140563755112256 train_utils.py:377] train in step: 44
I0712 13:27:15.183071 140563755112256 train_utils.py:377] train in step: 45
I0712 13:27:15.247053 140563755112256 train_utils.py:377] train in step: 46
I0712 13:27:15.316381 140563755112256 train_utils.py:377] train in step: 47
I0712 13:27:15.379897 140563755112256 train_utils.py:377] train in step: 48
I0712 13:27:15.444087 140563755112256 train_utils.py:377] train in step: 49
I0712 13:27:15.508710 140563755112256 train_utils.py:377] train in step: 50
I0712 13:27:15.575018 140563755112256 train_utils.py:377] train in step: 51
I0712 13:27:15.638436 140563755112256 train_utils.py:377] train in step: 52
I0712 13:27:15.703238 140563755112256 train_utils.py:377] train in step: 53
I0712 13:27:15.767975 140563755112256 train_utils.py:377] train in step: 54
I0712 13:27:15.836109 140563755112256 train_utils.py:377] train in step: 55
I0712 13:27:15.899320 140563755112256 train_utils.py:377] train in step: 56
I0712 13:27:15.965275 140563755112256 train_utils.py:377] train in step: 57
I0712 13:27:16.030094 140563755112256 train_utils.py:377] train in step: 58
I0712 13:27:16.095336 140563755112256 train_utils.py:377] train in step: 59
I0712 13:27:16.160414 140563755112256 train_utils.py:377] train in step: 60
I0712 13:27:16.225540 140563755112256 train_utils.py:377] train in step: 61
I0712 13:27:16.291544 140563755112256 train_utils.py:377] train in step: 62
I0712 13:27:16.357651 140563755112256 train_utils.py:377] train in step: 63
I0712 13:27:16.423374 140563755112256 train_utils.py:377] train in step: 64
I0712 13:27:16.489644 140563755112256 train_utils.py:377] train in step: 65
I0712 13:27:16.559713 140563755112256 train_utils.py:377] train in step: 66
I0712 13:27:16.624139 140563755112256 train_utils.py:377] train in step: 67
I0712 13:27:16.690738 140563755112256 train_utils.py:377] train in step: 68
I0712 13:27:16.759271 140563755112256 train_utils.py:377] train in step: 69
I0712 13:27:16.823132 140563755112256 train_utils.py:377] train in step: 70
I0712 13:27:16.890598 140563755112256 train_utils.py:377] train in step: 71
I0712 13:27:16.956292 140563755112256 train_utils.py:377] train in step: 72
I0712 13:27:17.023995 140563755112256 train_utils.py:377] train in step: 73
I0712 13:27:17.092527 140563755112256 train_utils.py:377] train in step: 74
I0712 13:27:17.156130 140563755112256 train_utils.py:377] train in step: 75
I0712 13:27:17.225579 140563755112256 train_utils.py:377] train in step: 76
I0712 13:27:17.291015 140563755112256 train_utils.py:377] train in step: 77
I0712 13:27:17.356901 140563755112256 train_utils.py:377] train in step: 78
I0712 13:27:17.425518 140563755112256 train_utils.py:377] train in step: 79
I0712 13:27:17.491275 140563755112256 train_utils.py:377] train in step: 80
I0712 13:27:17.558274 140563755112256 train_utils.py:377] train in step: 81
I0712 13:27:17.624068 140563755112256 train_utils.py:377] train in step: 82
I0712 13:27:17.690514 140563755112256 train_utils.py:377] train in step: 83
I0712 13:27:17.757073 140563755112256 train_utils.py:377] train in step: 84
I0712 13:27:17.825665 140563755112256 train_utils.py:377] train in step: 85
I0712 13:27:17.894326 140563755112256 train_utils.py:377] train in step: 86
I0712 13:27:17.960519 140563755112256 train_utils.py:377] train in step: 87
I0712 13:27:18.027525 140563755112256 train_utils.py:377] train in step: 88
I0712 13:27:18.093610 140563755112256 train_utils.py:377] train in step: 89
I0712 13:27:18.160179 140563755112256 train_utils.py:377] train in step: 90
I0712 13:27:18.226489 140563755112256 train_utils.py:377] train in step: 91
I0712 13:27:18.292908 140563755112256 train_utils.py:377] train in step: 92
I0712 13:27:18.359426 140563755112256 train_utils.py:377] train in step: 93
I0712 13:27:18.426194 140563755112256 train_utils.py:377] train in step: 94
I0712 13:27:18.492713 140563755112256 train_utils.py:377] train in step: 95
I0712 13:27:18.561417 140563755112256 train_utils.py:377] train in step: 96
I0712 13:27:18.627761 140563755112256 train_utils.py:377] train in step: 97
I0712 13:27:18.694265 140563755112256 train_utils.py:377] train in step: 98
I0712 13:27:18.760585 140563755112256 train_utils.py:377] train in step: 99
I0712 13:27:18.827155 140563755112256 train_utils.py:377] train in step: 100
I0712 13:27:18.893780 140563755112256 train_utils.py:377] train in step: 101
I0712 13:27:18.960190 140563755112256 train_utils.py:377] train in step: 102
I0712 13:27:19.028398 140563755112256 train_utils.py:377] train in step: 103
I0712 13:27:19.095693 140563755112256 train_utils.py:377] train in step: 104
I0712 13:27:19.161693 140563755112256 train_utils.py:377] train in step: 105
I0712 13:27:19.239148 140563755112256 train_utils.py:377] train in step: 106
I0712 13:27:19.306715 140563755112256 train_utils.py:377] train in step: 107
I0712 13:27:19.373774 140563755112256 train_utils.py:377] train in step: 108
I0712 13:27:19.437114 140563755112256 train_utils.py:377] train in step: 109
I0712 13:27:19.503404 140563755112256 train_utils.py:377] train in step: 110
I0712 13:27:19.571800 140563755112256 train_utils.py:377] train in step: 111
I0712 13:27:19.634626 140563755112256 train_utils.py:377] train in step: 112
I0712 13:27:19.700694 140563755112256 train_utils.py:377] train in step: 113
I0712 13:27:19.767417 140563755112256 train_utils.py:377] train in step: 114
I0712 13:27:19.834108 140563755112256 train_utils.py:377] train in step: 115
I0712 13:27:19.900462 140563755112256 train_utils.py:377] train in step: 116
I0712 13:27:19.968582 140563755112256 train_utils.py:377] train in step: 117
I0712 13:27:20.037114 140563755112256 train_utils.py:377] train in step: 118
I0712 13:27:20.100566 140563755112256 train_utils.py:377] train in step: 119
I0712 13:27:20.167090 140563755112256 train_utils.py:377] train in step: 120
I0712 13:27:20.234024 140563755112256 train_utils.py:377] train in step: 121
I0712 13:27:20.301383 140563755112256 train_utils.py:377] train in step: 122
I0712 13:27:20.367122 140563755112256 train_utils.py:377] train in step: 123
I0712 13:27:20.433498 140563755112256 train_utils.py:377] train in step: 124
I0712 13:27:20.502433 140563755112256 train_utils.py:377] train in step: 125
I0712 13:27:20.567661 140563755112256 train_utils.py:377] train in step: 126
I0712 13:27:20.633828 140563755112256 train_utils.py:377] train in step: 127
I0712 13:27:20.700945 140563755112256 train_utils.py:377] train in step: 128
I0712 13:27:20.768269 140563755112256 train_utils.py:377] train in step: 129
I0712 13:27:20.835999 140563755112256 train_utils.py:377] train in step: 130
I0712 13:27:20.900137 140563755112256 train_utils.py:377] train in step: 131
I0712 13:27:20.967329 140563755112256 train_utils.py:377] train in step: 132
I0712 13:27:21.035871 140563755112256 train_utils.py:377] train in step: 133
I0712 13:27:21.099638 140563755112256 train_utils.py:377] train in step: 134
I0712 13:27:21.172096 140563755112256 train_utils.py:377] train in step: 135
I0712 13:27:21.238974 140563755112256 train_utils.py:377] train in step: 136
I0712 13:27:21.306657 140563755112256 train_utils.py:377] train in step: 137
I0712 13:27:21.371952 140563755112256 train_utils.py:377] train in step: 138
I0712 13:27:21.438765 140563755112256 train_utils.py:377] train in step: 139
I0712 13:27:21.507179 140563755112256 train_utils.py:377] train in step: 140
I0712 13:27:21.571347 140563755112256 train_utils.py:377] train in step: 141
I0712 13:27:21.638330 140563755112256 train_utils.py:377] train in step: 142
I0712 13:27:21.706420 140563755112256 train_utils.py:377] train in step: 143
I0712 13:27:21.772786 140563755112256 train_utils.py:377] train in step: 144
I0712 13:27:21.842502 140563755112256 train_utils.py:377] train in step: 145
I0712 13:27:21.909224 140563755112256 train_utils.py:377] train in step: 146
I0712 13:27:21.975904 140563755112256 train_utils.py:377] train in step: 147
I0712 13:27:22.043427 140563755112256 train_utils.py:377] train in step: 148
I0712 13:27:22.107805 140563755112256 train_utils.py:377] train in step: 149
I0712 13:27:22.175780 140563755112256 train_utils.py:377] train in step: 150
I0712 13:27:22.242839 140563755112256 train_utils.py:377] train in step: 151
I0712 13:27:22.308691 140563755112256 train_utils.py:377] train in step: 152
I0712 13:27:22.375548 140563755112256 train_utils.py:377] train in step: 153
I0712 13:27:22.444927 140563755112256 train_utils.py:377] train in step: 154
I0712 13:27:22.514757 140563755112256 train_utils.py:377] train in step: 155
I0712 13:27:22.577537 140563755112256 train_utils.py:377] train in step: 156
I0712 13:27:22.646475 140563755112256 train_utils.py:377] train in step: 157
I0712 13:27:22.711935 140563755112256 train_utils.py:377] train in step: 158
I0712 13:27:22.778990 140563755112256 train_utils.py:377] train in step: 159
I0712 13:27:22.845577 140563755112256 train_utils.py:377] train in step: 160
I0712 13:27:22.912330 140563755112256 train_utils.py:377] train in step: 161
I0712 13:27:22.982617 140563755112256 train_utils.py:377] train in step: 162
I0712 13:27:23.053372 140563755112256 train_utils.py:377] train in step: 163
I0712 13:27:23.122320 140563755112256 train_utils.py:377] train in step: 164
I0712 13:27:23.193783 140563755112256 train_utils.py:377] train in step: 165
I0712 13:27:23.263107 140563755112256 train_utils.py:377] train in step: 166
I0712 13:27:23.334602 140563755112256 train_utils.py:377] train in step: 167
I0712 13:27:23.398384 140563755112256 train_utils.py:377] train in step: 168
I0712 13:27:23.466418 140563755112256 train_utils.py:377] train in step: 169
I0712 13:27:23.535203 140563755112256 train_utils.py:377] train in step: 170
I0712 13:27:23.602924 140563755112256 train_utils.py:377] train in step: 171
I0712 13:27:23.671848 140563755112256 train_utils.py:377] train in step: 172
I0712 13:27:23.740577 140563755112256 train_utils.py:377] train in step: 173
I0712 13:27:23.809708 140563755112256 train_utils.py:377] train in step: 174
I0712 13:27:23.878940 140563755112256 train_utils.py:377] train in step: 175
I0712 13:27:23.946584 140563755112256 train_utils.py:377] train in step: 176
I0712 13:27:24.015243 140563755112256 train_utils.py:377] train in step: 177
I0712 13:27:24.083152 140563755112256 train_utils.py:377] train in step: 178
I0712 13:27:24.151804 140563755112256 train_utils.py:377] train in step: 179
I0712 13:27:24.220528 140563755112256 train_utils.py:377] train in step: 180
I0712 13:27:24.289163 140563755112256 train_utils.py:377] train in step: 181
I0712 13:27:24.358013 140563755112256 train_utils.py:377] train in step: 182
I0712 13:27:24.426458 140563755112256 train_utils.py:377] train in step: 183
I0712 13:27:24.495086 140563755112256 train_utils.py:377] train in step: 184
I0712 13:27:24.566726 140563755112256 train_utils.py:377] train in step: 185
I0712 13:27:24.635772 140563755112256 train_utils.py:377] train in step: 186
I0712 13:27:24.703735 140563755112256 train_utils.py:377] train in step: 187
I0712 13:27:24.770577 140563755112256 train_utils.py:377] train in step: 188
I0712 13:27:24.839332 140563755112256 train_utils.py:377] train in step: 189
I0712 13:27:24.906650 140563755112256 train_utils.py:377] train in step: 190
I0712 13:27:24.975092 140563755112256 train_utils.py:377] train in step: 191
I0712 13:27:25.043573 140563755112256 train_utils.py:377] train in step: 192
I0712 13:27:25.114053 140563755112256 train_utils.py:377] train in step: 193
I0712 13:27:25.181307 140563755112256 train_utils.py:377] train in step: 194
I0712 13:27:25.249790 140563755112256 train_utils.py:377] train in step: 195
I0712 13:27:25.318559 140563755112256 train_utils.py:377] train in step: 196
I0712 13:27:25.388273 140563755112256 train_utils.py:377] train in step: 197
I0712 13:27:25.455297 140563755112256 train_utils.py:377] train in step: 198
I0712 13:27:25.535770 140563755112256 train_utils.py:377] train in step: 199
I0712 13:27:25.597713 140563755112256 train_utils.py:377] train in step: 200
I0712 13:27:26.307341 140563755112256 train_utils.py:396] train in step: 200, loss: 0.7389999628067017, acc: 0.49129998683929443
I0712 13:27:47.779643 140563755112256 train_utils.py:411] eval in step: 200, loss: 0.7133, acc: 0.5300
I0712 13:27:47.783999 140563755112256 train_utils.py:421] Testing...
I0712 13:27:51.450050 140563755112256 train_utils.py:424] test in step: 200, loss: 0.7555, acc: 0.4600
I0712 13:27:51.487135 140563755112256 train_utils.py:377] train in step: 201
I0712 13:27:51.544039 140563755112256 train_utils.py:377] train in step: 202
I0712 13:27:51.611127 140563755112256 train_utils.py:377] train in step: 203
I0712 13:27:51.680033 140563755112256 train_utils.py:377] train in step: 204
I0712 13:27:51.745163 140563755112256 train_utils.py:377] train in step: 205
I0712 13:27:51.814966 140563755112256 train_utils.py:377] train in step: 206
I0712 13:27:51.880928 140563755112256 train_utils.py:377] train in step: 207
I0712 13:27:51.947823 140563755112256 train_utils.py:377] train in step: 208
I0712 13:27:52.024336 140563755112256 train_utils.py:377] train in step: 209
I0712 13:27:52.091405 140563755112256 train_utils.py:377] train in step: 210
I0712 13:27:52.159482 140563755112256 train_utils.py:377] train in step: 211
I0712 13:27:52.226677 140563755112256 train_utils.py:377] train in step: 212
I0712 13:27:52.293621 140563755112256 train_utils.py:377] train in step: 213
I0712 13:27:52.361403 140563755112256 train_utils.py:377] train in step: 214
I0712 13:27:52.427443 140563755112256 train_utils.py:377] train in step: 215
I0712 13:27:52.493653 140563755112256 train_utils.py:377] train in step: 216
I0712 13:27:52.561002 140563755112256 train_utils.py:377] train in step: 217
I0712 13:27:52.627587 140563755112256 train_utils.py:377] train in step: 218
I0712 13:27:52.694676 140563755112256 train_utils.py:377] train in step: 219
I0712 13:27:52.767547 140563755112256 train_utils.py:377] train in step: 220
I0712 13:27:52.835793 140563755112256 train_utils.py:377] train in step: 221
I0712 13:27:52.902347 140563755112256 train_utils.py:377] train in step: 222
I0712 13:27:52.970886 140563755112256 train_utils.py:377] train in step: 223
I0712 13:27:53.036389 140563755112256 train_utils.py:377] train in step: 224
I0712 13:27:53.103176 140563755112256 train_utils.py:377] train in step: 225
I0712 13:27:53.174968 140563755112256 train_utils.py:377] train in step: 226
I0712 13:27:53.243613 140563755112256 train_utils.py:377] train in step: 227
I0712 13:27:53.310071 140563755112256 train_utils.py:377] train in step: 228
I0712 13:27:53.380443 140563755112256 train_utils.py:377] train in step: 229
I0712 13:27:53.464741 140563755112256 train_utils.py:377] train in step: 230
I0712 13:27:53.534520 140563755112256 train_utils.py:377] train in step: 231
I0712 13:27:53.600714 140563755112256 train_utils.py:377] train in step: 232
I0712 13:27:53.671466 140563755112256 train_utils.py:377] train in step: 233
I0712 13:27:53.737327 140563755112256 train_utils.py:377] train in step: 234
I0712 13:27:53.805054 140563755112256 train_utils.py:377] train in step: 235
I0712 13:27:53.873214 140563755112256 train_utils.py:377] train in step: 236
I0712 13:27:53.943028 140563755112256 train_utils.py:377] train in step: 237
I0712 13:27:54.012345 140563755112256 train_utils.py:377] train in step: 238
I0712 13:27:54.080945 140563755112256 train_utils.py:377] train in step: 239
I0712 13:27:54.149483 140563755112256 train_utils.py:377] train in step: 240
I0712 13:27:54.219731 140563755112256 train_utils.py:377] train in step: 241
I0712 13:27:54.289797 140563755112256 train_utils.py:377] train in step: 242
I0712 13:27:54.356704 140563755112256 train_utils.py:377] train in step: 243
I0712 13:27:54.425107 140563755112256 train_utils.py:377] train in step: 244
I0712 13:27:54.493855 140563755112256 train_utils.py:377] train in step: 245
I0712 13:27:54.563803 140563755112256 train_utils.py:377] train in step: 246
I0712 13:27:54.634517 140563755112256 train_utils.py:377] train in step: 247
I0712 13:27:54.702636 140563755112256 train_utils.py:377] train in step: 248
I0712 13:27:54.771393 140563755112256 train_utils.py:377] train in step: 249
I0712 13:27:54.839581 140563755112256 train_utils.py:377] train in step: 250
I0712 13:27:54.915334 140563755112256 train_utils.py:377] train in step: 251
I0712 13:27:54.978749 140563755112256 train_utils.py:377] train in step: 252
I0712 13:27:55.048284 140563755112256 train_utils.py:377] train in step: 253
I0712 13:27:55.114985 140563755112256 train_utils.py:377] train in step: 254
I0712 13:27:55.185307 140563755112256 train_utils.py:377] train in step: 255
I0712 13:27:55.255061 140563755112256 train_utils.py:377] train in step: 256
I0712 13:27:55.322067 140563755112256 train_utils.py:377] train in step: 257
I0712 13:27:55.390855 140563755112256 train_utils.py:377] train in step: 258
I0712 13:27:55.463224 140563755112256 train_utils.py:377] train in step: 259
I0712 13:27:55.531905 140563755112256 train_utils.py:377] train in step: 260
I0712 13:27:55.602572 140563755112256 train_utils.py:377] train in step: 261
I0712 13:27:55.671715 140563755112256 train_utils.py:377] train in step: 262
I0712 13:27:55.741843 140563755112256 train_utils.py:377] train in step: 263
I0712 13:27:55.808357 140563755112256 train_utils.py:377] train in step: 264
I0712 13:27:55.875830 140563755112256 train_utils.py:377] train in step: 265
I0712 13:27:55.944067 140563755112256 train_utils.py:377] train in step: 266
I0712 13:27:56.012930 140563755112256 train_utils.py:377] train in step: 267
I0712 13:27:56.081489 140563755112256 train_utils.py:377] train in step: 268
I0712 13:27:56.149715 140563755112256 train_utils.py:377] train in step: 269
I0712 13:27:56.224819 140563755112256 train_utils.py:377] train in step: 270
I0712 13:27:56.296748 140563755112256 train_utils.py:377] train in step: 271
I0712 13:27:56.367724 140563755112256 train_utils.py:377] train in step: 272
I0712 13:27:56.436559 140563755112256 train_utils.py:377] train in step: 273
I0712 13:27:56.507119 140563755112256 train_utils.py:377] train in step: 274
I0712 13:27:56.579581 140563755112256 train_utils.py:377] train in step: 275
I0712 13:27:56.645836 140563755112256 train_utils.py:377] train in step: 276
I0712 13:27:56.714497 140563755112256 train_utils.py:377] train in step: 277
I0712 13:27:56.783097 140563755112256 train_utils.py:377] train in step: 278
I0712 13:27:56.852749 140563755112256 train_utils.py:377] train in step: 279
I0712 13:27:56.922088 140563755112256 train_utils.py:377] train in step: 280
I0712 13:27:56.992667 140563755112256 train_utils.py:377] train in step: 281
I0712 13:27:57.060602 140563755112256 train_utils.py:377] train in step: 282
I0712 13:27:57.132941 140563755112256 train_utils.py:377] train in step: 283
I0712 13:27:57.198293 140563755112256 train_utils.py:377] train in step: 284
I0712 13:27:57.267293 140563755112256 train_utils.py:377] train in step: 285
I0712 13:27:57.334987 140563755112256 train_utils.py:377] train in step: 286
I0712 13:27:57.404368 140563755112256 train_utils.py:377] train in step: 287
I0712 13:27:57.471253 140563755112256 train_utils.py:377] train in step: 288
I0712 13:27:57.539912 140563755112256 train_utils.py:377] train in step: 289
I0712 13:27:57.615533 140563755112256 train_utils.py:377] train in step: 290
I0712 13:27:57.684470 140563755112256 train_utils.py:377] train in step: 291
I0712 13:27:57.755482 140563755112256 train_utils.py:377] train in step: 292
I0712 13:27:57.826088 140563755112256 train_utils.py:377] train in step: 293
I0712 13:27:57.894349 140563755112256 train_utils.py:377] train in step: 294
I0712 13:27:57.967560 140563755112256 train_utils.py:377] train in step: 295
I0712 13:27:58.034457 140563755112256 train_utils.py:377] train in step: 296
I0712 13:27:58.107630 140563755112256 train_utils.py:377] train in step: 297
I0712 13:27:58.175787 140563755112256 train_utils.py:377] train in step: 298
I0712 13:27:58.242419 140563755112256 train_utils.py:377] train in step: 299
I0712 13:27:58.314509 140563755112256 train_utils.py:377] train in step: 300
I0712 13:27:58.382493 140563755112256 train_utils.py:377] train in step: 301
I0712 13:27:58.458062 140563755112256 train_utils.py:377] train in step: 302
I0712 13:27:58.523912 140563755112256 train_utils.py:377] train in step: 303
I0712 13:27:58.592110 140563755112256 train_utils.py:377] train in step: 304
I0712 13:27:58.660813 140563755112256 train_utils.py:377] train in step: 305
I0712 13:27:58.730737 140563755112256 train_utils.py:377] train in step: 306
I0712 13:27:58.800245 140563755112256 train_utils.py:377] train in step: 307
I0712 13:27:58.867692 140563755112256 train_utils.py:377] train in step: 308
I0712 13:27:58.935247 140563755112256 train_utils.py:377] train in step: 309
I0712 13:27:59.003870 140563755112256 train_utils.py:377] train in step: 310
I0712 13:27:59.071919 140563755112256 train_utils.py:377] train in step: 311
I0712 13:27:59.140176 140563755112256 train_utils.py:377] train in step: 312
I0712 13:27:59.209125 140563755112256 train_utils.py:377] train in step: 313
I0712 13:27:59.279033 140563755112256 train_utils.py:377] train in step: 314
I0712 13:27:59.345621 140563755112256 train_utils.py:377] train in step: 315
I0712 13:27:59.414478 140563755112256 train_utils.py:377] train in step: 316
I0712 13:27:59.487195 140563755112256 train_utils.py:377] train in step: 317
I0712 13:27:59.553143 140563755112256 train_utils.py:377] train in step: 318
I0712 13:27:59.621282 140563755112256 train_utils.py:377] train in step: 319
I0712 13:27:59.699785 140563755112256 train_utils.py:377] train in step: 320
I0712 13:27:59.766092 140563755112256 train_utils.py:377] train in step: 321
I0712 13:27:59.834406 140563755112256 train_utils.py:377] train in step: 322
I0712 13:27:59.901290 140563755112256 train_utils.py:377] train in step: 323
I0712 13:27:59.969721 140563755112256 train_utils.py:377] train in step: 324
I0712 13:28:00.038588 140563755112256 train_utils.py:377] train in step: 325
I0712 13:28:00.106600 140563755112256 train_utils.py:377] train in step: 326
I0712 13:28:00.175421 140563755112256 train_utils.py:377] train in step: 327
I0712 13:28:00.242794 140563755112256 train_utils.py:377] train in step: 328
I0712 13:28:00.314231 140563755112256 train_utils.py:377] train in step: 329
I0712 13:28:00.393114 140563755112256 train_utils.py:377] train in step: 330
I0712 13:28:00.461835 140563755112256 train_utils.py:377] train in step: 331
I0712 13:28:00.532526 140563755112256 train_utils.py:377] train in step: 332
I0712 13:28:00.600952 140563755112256 train_utils.py:377] train in step: 333
I0712 13:28:00.670352 140563755112256 train_utils.py:377] train in step: 334
I0712 13:28:00.738959 140563755112256 train_utils.py:377] train in step: 335
I0712 13:28:00.807958 140563755112256 train_utils.py:377] train in step: 336
I0712 13:28:00.876284 140563755112256 train_utils.py:377] train in step: 337
I0712 13:28:00.946365 140563755112256 train_utils.py:377] train in step: 338
I0712 13:28:01.014254 140563755112256 train_utils.py:377] train in step: 339
I0712 13:28:01.084249 140563755112256 train_utils.py:377] train in step: 340
I0712 13:28:01.155583 140563755112256 train_utils.py:377] train in step: 341
I0712 13:28:01.223992 140563755112256 train_utils.py:377] train in step: 342
I0712 13:28:01.293550 140563755112256 train_utils.py:377] train in step: 343
I0712 13:28:01.361886 140563755112256 train_utils.py:377] train in step: 344
I0712 13:28:01.430778 140563755112256 train_utils.py:377] train in step: 345
I0712 13:28:01.499566 140563755112256 train_utils.py:377] train in step: 346
I0712 13:28:01.568862 140563755112256 train_utils.py:377] train in step: 347
I0712 13:28:01.637339 140563755112256 train_utils.py:377] train in step: 348
I0712 13:28:01.706401 140563755112256 train_utils.py:377] train in step: 349
I0712 13:28:01.776429 140563755112256 train_utils.py:377] train in step: 350
I0712 13:28:01.845793 140563755112256 train_utils.py:377] train in step: 351
I0712 13:28:01.913018 140563755112256 train_utils.py:377] train in step: 352
I0712 13:28:01.981737 140563755112256 train_utils.py:377] train in step: 353
I0712 13:28:02.050463 140563755112256 train_utils.py:377] train in step: 354
I0712 13:28:02.120313 140563755112256 train_utils.py:377] train in step: 355
I0712 13:28:02.187902 140563755112256 train_utils.py:377] train in step: 356
I0712 13:28:02.256708 140563755112256 train_utils.py:377] train in step: 357
I0712 13:28:02.329236 140563755112256 train_utils.py:377] train in step: 358
I0712 13:28:02.394788 140563755112256 train_utils.py:377] train in step: 359
I0712 13:28:02.463146 140563755112256 train_utils.py:377] train in step: 360
I0712 13:28:02.531880 140563755112256 train_utils.py:377] train in step: 361
I0712 13:28:02.601989 140563755112256 train_utils.py:377] train in step: 362
I0712 13:28:02.669663 140563755112256 train_utils.py:377] train in step: 363
I0712 13:28:02.738096 140563755112256 train_utils.py:377] train in step: 364
I0712 13:28:02.806678 140563755112256 train_utils.py:377] train in step: 365
I0712 13:28:02.875354 140563755112256 train_utils.py:377] train in step: 366
I0712 13:28:02.944482 140563755112256 train_utils.py:377] train in step: 367
I0712 13:28:03.014223 140563755112256 train_utils.py:377] train in step: 368
I0712 13:28:03.083336 140563755112256 train_utils.py:377] train in step: 369
I0712 13:28:03.153303 140563755112256 train_utils.py:377] train in step: 370
I0712 13:28:03.221855 140563755112256 train_utils.py:377] train in step: 371
I0712 13:28:03.290626 140563755112256 train_utils.py:377] train in step: 372
I0712 13:28:03.359789 140563755112256 train_utils.py:377] train in step: 373
I0712 13:28:03.428469 140563755112256 train_utils.py:377] train in step: 374
I0712 13:28:03.497276 140563755112256 train_utils.py:377] train in step: 375
I0712 13:28:03.566032 140563755112256 train_utils.py:377] train in step: 376
I0712 13:28:03.635106 140563755112256 train_utils.py:377] train in step: 377
I0712 13:28:03.704236 140563755112256 train_utils.py:377] train in step: 378
I0712 13:28:03.782943 140563755112256 train_utils.py:377] train in step: 379
I0712 13:28:03.849919 140563755112256 train_utils.py:377] train in step: 380
I0712 13:28:03.918362 140563755112256 train_utils.py:377] train in step: 381
I0712 13:28:03.987477 140563755112256 train_utils.py:377] train in step: 382
I0712 13:28:04.055968 140563755112256 train_utils.py:377] train in step: 383
I0712 13:28:04.126103 140563755112256 train_utils.py:377] train in step: 384
I0712 13:28:04.193390 140563755112256 train_utils.py:377] train in step: 385
I0712 13:28:04.262021 140563755112256 train_utils.py:377] train in step: 386
I0712 13:28:04.330374 140563755112256 train_utils.py:377] train in step: 387
I0712 13:28:04.398852 140563755112256 train_utils.py:377] train in step: 388
I0712 13:28:04.468066 140563755112256 train_utils.py:377] train in step: 389
I0712 13:28:04.536415 140563755112256 train_utils.py:377] train in step: 390
I0712 13:28:04.608029 140563755112256 train_utils.py:377] train in step: 391
I0712 13:28:04.674774 140563755112256 train_utils.py:377] train in step: 392
I0712 13:28:04.744547 140563755112256 train_utils.py:377] train in step: 393
I0712 13:28:04.811332 140563755112256 train_utils.py:377] train in step: 394
I0712 13:28:04.879507 140563755112256 train_utils.py:377] train in step: 395
I0712 13:28:04.947931 140563755112256 train_utils.py:377] train in step: 396
I0712 13:28:05.016273 140563755112256 train_utils.py:377] train in step: 397
I0712 13:28:05.085004 140563755112256 train_utils.py:377] train in step: 398
I0712 13:28:05.154051 140563755112256 train_utils.py:377] train in step: 399
I0712 13:28:05.222256 140563755112256 train_utils.py:377] train in step: 400
I0712 13:28:05.491127 140563755112256 train_utils.py:396] train in step: 400, loss: 0.7416999936103821, acc: 0.5174999833106995
I0712 13:28:08.983783 140563755112256 train_utils.py:411] eval in step: 400, loss: 0.6915, acc: 0.5400
I0712 13:28:08.986978 140563755112256 train_utils.py:421] Testing...
I0712 13:28:12.644973 140563755112256 train_utils.py:424] test in step: 400, loss: 0.7081, acc: 0.4900
I0712 13:28:12.675962 140563755112256 train_utils.py:377] train in step: 401
I0712 13:28:12.756361 140563755112256 train_utils.py:377] train in step: 402
I0712 13:28:12.820696 140563755112256 train_utils.py:377] train in step: 403
I0712 13:28:12.895027 140563755112256 train_utils.py:377] train in step: 404
I0712 13:28:12.964146 140563755112256 train_utils.py:377] train in step: 405
I0712 13:28:13.031649 140563755112256 train_utils.py:377] train in step: 406
I0712 13:28:13.105134 140563755112256 train_utils.py:377] train in step: 407
I0712 13:28:13.171759 140563755112256 train_utils.py:377] train in step: 408
I0712 13:28:13.240405 140563755112256 train_utils.py:377] train in step: 409
I0712 13:28:13.309130 140563755112256 train_utils.py:377] train in step: 410
I0712 13:28:13.387142 140563755112256 train_utils.py:377] train in step: 411
I0712 13:28:13.453372 140563755112256 train_utils.py:377] train in step: 412
I0712 13:28:13.521718 140563755112256 train_utils.py:377] train in step: 413
I0712 13:28:13.591433 140563755112256 train_utils.py:377] train in step: 414
I0712 13:28:13.660543 140563755112256 train_utils.py:377] train in step: 415
I0712 13:28:13.732238 140563755112256 train_utils.py:377] train in step: 416
I0712 13:28:13.798916 140563755112256 train_utils.py:377] train in step: 417
I0712 13:28:13.867272 140563755112256 train_utils.py:377] train in step: 418
I0712 13:28:13.936352 140563755112256 train_utils.py:377] train in step: 419
I0712 13:28:14.005359 140563755112256 train_utils.py:377] train in step: 420
I0712 13:28:14.073868 140563755112256 train_utils.py:377] train in step: 421
I0712 13:28:14.143649 140563755112256 train_utils.py:377] train in step: 422
I0712 13:28:14.211571 140563755112256 train_utils.py:377] train in step: 423
I0712 13:28:14.283008 140563755112256 train_utils.py:377] train in step: 424
I0712 13:28:14.351670 140563755112256 train_utils.py:377] train in step: 425
I0712 13:28:14.419831 140563755112256 train_utils.py:377] train in step: 426
I0712 13:28:14.489232 140563755112256 train_utils.py:377] train in step: 427
I0712 13:28:14.556136 140563755112256 train_utils.py:377] train in step: 428
I0712 13:28:14.625809 140563755112256 train_utils.py:377] train in step: 429
I0712 13:28:14.694012 140563755112256 train_utils.py:377] train in step: 430
I0712 13:28:14.762079 140563755112256 train_utils.py:377] train in step: 431
I0712 13:28:14.831228 140563755112256 train_utils.py:377] train in step: 432
I0712 13:28:14.898363 140563755112256 train_utils.py:377] train in step: 433
I0712 13:28:14.968154 140563755112256 train_utils.py:377] train in step: 434
I0712 13:28:15.038326 140563755112256 train_utils.py:377] train in step: 435
I0712 13:28:15.108028 140563755112256 train_utils.py:377] train in step: 436
I0712 13:28:15.176755 140563755112256 train_utils.py:377] train in step: 437
I0712 13:28:15.246195 140563755112256 train_utils.py:377] train in step: 438
I0712 13:28:15.318616 140563755112256 train_utils.py:377] train in step: 439
I0712 13:28:15.384666 140563755112256 train_utils.py:377] train in step: 440
I0712 13:28:15.453160 140563755112256 train_utils.py:377] train in step: 441
I0712 13:28:15.523805 140563755112256 train_utils.py:377] train in step: 442
I0712 13:28:15.590144 140563755112256 train_utils.py:377] train in step: 443
I0712 13:28:15.661826 140563755112256 train_utils.py:377] train in step: 444
I0712 13:28:15.730429 140563755112256 train_utils.py:377] train in step: 445
I0712 13:28:15.799124 140563755112256 train_utils.py:377] train in step: 446
I0712 13:28:15.867410 140563755112256 train_utils.py:377] train in step: 447
I0712 13:28:15.935213 140563755112256 train_utils.py:377] train in step: 448
I0712 13:28:16.004130 140563755112256 train_utils.py:377] train in step: 449
I0712 13:28:16.077029 140563755112256 train_utils.py:377] train in step: 450
I0712 13:28:16.144841 140563755112256 train_utils.py:377] train in step: 451
I0712 13:28:16.219645 140563755112256 train_utils.py:377] train in step: 452
I0712 13:28:16.288338 140563755112256 train_utils.py:377] train in step: 453
I0712 13:28:16.358312 140563755112256 train_utils.py:377] train in step: 454
I0712 13:28:16.425902 140563755112256 train_utils.py:377] train in step: 455
I0712 13:28:16.496515 140563755112256 train_utils.py:377] train in step: 456
I0712 13:28:16.565468 140563755112256 train_utils.py:377] train in step: 457
I0712 13:28:16.634048 140563755112256 train_utils.py:377] train in step: 458
I0712 13:28:16.703252 140563755112256 train_utils.py:377] train in step: 459
I0712 13:28:16.771823 140563755112256 train_utils.py:377] train in step: 460
I0712 13:28:16.840469 140563755112256 train_utils.py:377] train in step: 461
I0712 13:28:16.909489 140563755112256 train_utils.py:377] train in step: 462
I0712 13:28:16.978182 140563755112256 train_utils.py:377] train in step: 463
I0712 13:28:17.047064 140563755112256 train_utils.py:377] train in step: 464
I0712 13:28:17.115436 140563755112256 train_utils.py:377] train in step: 465
I0712 13:28:17.184209 140563755112256 train_utils.py:377] train in step: 466
I0712 13:28:17.255321 140563755112256 train_utils.py:377] train in step: 467
I0712 13:28:17.323241 140563755112256 train_utils.py:377] train in step: 468
I0712 13:28:17.394491 140563755112256 train_utils.py:377] train in step: 469
I0712 13:28:17.461246 140563755112256 train_utils.py:377] train in step: 470
I0712 13:28:17.529299 140563755112256 train_utils.py:377] train in step: 471
I0712 13:28:17.598898 140563755112256 train_utils.py:377] train in step: 472
I0712 13:28:17.667041 140563755112256 train_utils.py:377] train in step: 473
I0712 13:28:17.735930 140563755112256 train_utils.py:377] train in step: 474
I0712 13:28:17.805156 140563755112256 train_utils.py:377] train in step: 475
I0712 13:28:17.875517 140563755112256 train_utils.py:377] train in step: 476
I0712 13:28:17.942940 140563755112256 train_utils.py:377] train in step: 477
I0712 13:28:18.014090 140563755112256 train_utils.py:377] train in step: 478
I0712 13:28:18.083776 140563755112256 train_utils.py:377] train in step: 479
I0712 13:28:18.151032 140563755112256 train_utils.py:377] train in step: 480
I0712 13:28:18.219860 140563755112256 train_utils.py:377] train in step: 481
I0712 13:28:18.288467 140563755112256 train_utils.py:377] train in step: 482
I0712 13:28:18.357839 140563755112256 train_utils.py:377] train in step: 483
I0712 13:28:18.426622 140563755112256 train_utils.py:377] train in step: 484
I0712 13:28:18.500722 140563755112256 train_utils.py:377] train in step: 485
I0712 13:28:18.573538 140563755112256 train_utils.py:377] train in step: 486
I0712 13:28:18.639663 140563755112256 train_utils.py:377] train in step: 487
I0712 13:28:18.709606 140563755112256 train_utils.py:377] train in step: 488
I0712 13:28:18.780086 140563755112256 train_utils.py:377] train in step: 489
I0712 13:28:18.848211 140563755112256 train_utils.py:377] train in step: 490
I0712 13:28:18.916914 140563755112256 train_utils.py:377] train in step: 491
I0712 13:28:18.986414 140563755112256 train_utils.py:377] train in step: 492
I0712 13:28:19.054867 140563755112256 train_utils.py:377] train in step: 493
I0712 13:28:19.123936 140563755112256 train_utils.py:377] train in step: 494
I0712 13:28:19.200407 140563755112256 train_utils.py:377] train in step: 495
I0712 13:28:19.273700 140563755112256 train_utils.py:377] train in step: 496
I0712 13:28:19.338897 140563755112256 train_utils.py:377] train in step: 497
I0712 13:28:19.407610 140563755112256 train_utils.py:377] train in step: 498
I0712 13:28:19.476705 140563755112256 train_utils.py:377] train in step: 499
I0712 13:28:19.546731 140563755112256 train_utils.py:377] train in step: 500
I0712 13:28:19.613863 140563755112256 train_utils.py:377] train in step: 501
I0712 13:28:19.684421 140563755112256 train_utils.py:377] train in step: 502
I0712 13:28:19.754589 140563755112256 train_utils.py:377] train in step: 503
I0712 13:28:19.822340 140563755112256 train_utils.py:377] train in step: 504
I0712 13:28:19.897586 140563755112256 train_utils.py:377] train in step: 505
I0712 13:28:19.965180 140563755112256 train_utils.py:377] train in step: 506
I0712 13:28:20.034364 140563755112256 train_utils.py:377] train in step: 507
I0712 13:28:20.102989 140563755112256 train_utils.py:377] train in step: 508
I0712 13:28:20.172646 140563755112256 train_utils.py:377] train in step: 509
I0712 13:28:20.240902 140563755112256 train_utils.py:377] train in step: 510
I0712 13:28:20.310026 140563755112256 train_utils.py:377] train in step: 511
I0712 13:28:20.378407 140563755112256 train_utils.py:377] train in step: 512
I0712 13:28:20.447291 140563755112256 train_utils.py:377] train in step: 513
I0712 13:28:20.516759 140563755112256 train_utils.py:377] train in step: 514
I0712 13:28:20.588131 140563755112256 train_utils.py:377] train in step: 515
I0712 13:28:20.656647 140563755112256 train_utils.py:377] train in step: 516
I0712 13:28:20.726981 140563755112256 train_utils.py:377] train in step: 517
I0712 13:28:20.796418 140563755112256 train_utils.py:377] train in step: 518
I0712 13:28:20.864842 140563755112256 train_utils.py:377] train in step: 519
I0712 13:28:20.934111 140563755112256 train_utils.py:377] train in step: 520
I0712 13:28:21.002761 140563755112256 train_utils.py:377] train in step: 521
I0712 13:28:21.075070 140563755112256 train_utils.py:377] train in step: 522
I0712 13:28:21.140817 140563755112256 train_utils.py:377] train in step: 523
I0712 13:28:21.210832 140563755112256 train_utils.py:377] train in step: 524
I0712 13:28:21.279942 140563755112256 train_utils.py:377] train in step: 525
I0712 13:28:21.349820 140563755112256 train_utils.py:377] train in step: 526
I0712 13:28:21.420371 140563755112256 train_utils.py:377] train in step: 527
I0712 13:28:21.489457 140563755112256 train_utils.py:377] train in step: 528
I0712 13:28:21.559222 140563755112256 train_utils.py:377] train in step: 529
I0712 13:28:21.627444 140563755112256 train_utils.py:377] train in step: 530
I0712 13:28:21.696606 140563755112256 train_utils.py:377] train in step: 531
I0712 13:28:21.768367 140563755112256 train_utils.py:377] train in step: 532
I0712 13:28:21.834279 140563755112256 train_utils.py:377] train in step: 533
I0712 13:28:21.904836 140563755112256 train_utils.py:377] train in step: 534
I0712 13:28:21.974291 140563755112256 train_utils.py:377] train in step: 535
I0712 13:28:22.043912 140563755112256 train_utils.py:377] train in step: 536
I0712 13:28:22.112684 140563755112256 train_utils.py:377] train in step: 537
I0712 13:28:22.181757 140563755112256 train_utils.py:377] train in step: 538
I0712 13:28:22.252455 140563755112256 train_utils.py:377] train in step: 539
I0712 13:28:22.321614 140563755112256 train_utils.py:377] train in step: 540
I0712 13:28:22.393241 140563755112256 train_utils.py:377] train in step: 541
I0712 13:28:22.461384 140563755112256 train_utils.py:377] train in step: 542
I0712 13:28:22.530778 140563755112256 train_utils.py:377] train in step: 543
I0712 13:28:22.600776 140563755112256 train_utils.py:377] train in step: 544
I0712 13:28:22.670063 140563755112256 train_utils.py:377] train in step: 545
I0712 13:28:22.738477 140563755112256 train_utils.py:377] train in step: 546
I0712 13:28:22.807219 140563755112256 train_utils.py:377] train in step: 547
I0712 13:28:22.876110 140563755112256 train_utils.py:377] train in step: 548
I0712 13:28:22.945246 140563755112256 train_utils.py:377] train in step: 549
I0712 13:28:23.015548 140563755112256 train_utils.py:377] train in step: 550
I0712 13:28:23.083065 140563755112256 train_utils.py:377] train in step: 551
I0712 13:28:23.155452 140563755112256 train_utils.py:377] train in step: 552
I0712 13:28:23.222997 140563755112256 train_utils.py:377] train in step: 553
I0712 13:28:23.294340 140563755112256 train_utils.py:377] train in step: 554
I0712 13:28:23.363729 140563755112256 train_utils.py:377] train in step: 555
I0712 13:28:23.431907 140563755112256 train_utils.py:377] train in step: 556
I0712 13:28:23.500164 140563755112256 train_utils.py:377] train in step: 557
I0712 13:28:23.569600 140563755112256 train_utils.py:377] train in step: 558
I0712 13:28:23.638378 140563755112256 train_utils.py:377] train in step: 559
I0712 13:28:23.707346 140563755112256 train_utils.py:377] train in step: 560
I0712 13:28:23.775789 140563755112256 train_utils.py:377] train in step: 561
I0712 13:28:23.845394 140563755112256 train_utils.py:377] train in step: 562
I0712 13:28:23.918054 140563755112256 train_utils.py:377] train in step: 563
I0712 13:28:23.986915 140563755112256 train_utils.py:377] train in step: 564
I0712 13:28:24.055943 140563755112256 train_utils.py:377] train in step: 565
I0712 13:28:24.123463 140563755112256 train_utils.py:377] train in step: 566
I0712 13:28:24.192218 140563755112256 train_utils.py:377] train in step: 567
I0712 13:28:24.261014 140563755112256 train_utils.py:377] train in step: 568
I0712 13:28:24.330042 140563755112256 train_utils.py:377] train in step: 569
I0712 13:28:24.398814 140563755112256 train_utils.py:377] train in step: 570
I0712 13:28:24.467862 140563755112256 train_utils.py:377] train in step: 571
I0712 13:28:24.539387 140563755112256 train_utils.py:377] train in step: 572
I0712 13:28:24.608092 140563755112256 train_utils.py:377] train in step: 573
I0712 13:28:24.678700 140563755112256 train_utils.py:377] train in step: 574
I0712 13:28:24.746578 140563755112256 train_utils.py:377] train in step: 575
I0712 13:28:24.814991 140563755112256 train_utils.py:377] train in step: 576
I0712 13:28:24.884366 140563755112256 train_utils.py:377] train in step: 577
I0712 13:28:24.952357 140563755112256 train_utils.py:377] train in step: 578
I0712 13:28:25.021745 140563755112256 train_utils.py:377] train in step: 579
I0712 13:28:25.090813 140563755112256 train_utils.py:377] train in step: 580
I0712 13:28:25.159492 140563755112256 train_utils.py:377] train in step: 581
I0712 13:28:25.228991 140563755112256 train_utils.py:377] train in step: 582
I0712 13:28:25.298162 140563755112256 train_utils.py:377] train in step: 583
I0712 13:28:25.367150 140563755112256 train_utils.py:377] train in step: 584
I0712 13:28:25.435709 140563755112256 train_utils.py:377] train in step: 585
I0712 13:28:25.505119 140563755112256 train_utils.py:377] train in step: 586
I0712 13:28:25.573320 140563755112256 train_utils.py:377] train in step: 587
I0712 13:28:25.642282 140563755112256 train_utils.py:377] train in step: 588
I0712 13:28:25.711508 140563755112256 train_utils.py:377] train in step: 589
I0712 13:28:25.781491 140563755112256 train_utils.py:377] train in step: 590
I0712 13:28:25.850087 140563755112256 train_utils.py:377] train in step: 591
I0712 13:28:25.925796 140563755112256 train_utils.py:377] train in step: 592
I0712 13:28:25.998034 140563755112256 train_utils.py:377] train in step: 593
I0712 13:28:26.065060 140563755112256 train_utils.py:377] train in step: 594
I0712 13:28:26.133682 140563755112256 train_utils.py:377] train in step: 595
I0712 13:28:26.202512 140563755112256 train_utils.py:377] train in step: 596
I0712 13:28:26.271414 140563755112256 train_utils.py:377] train in step: 597
I0712 13:28:26.340473 140563755112256 train_utils.py:377] train in step: 598
I0712 13:28:26.409042 140563755112256 train_utils.py:377] train in step: 599
I0712 13:28:26.477573 140563755112256 train_utils.py:377] train in step: 600
I0712 13:28:26.555044 140563755112256 train_utils.py:396] train in step: 600, loss: 0.7094999551773071, acc: 0.5475000143051147
I0712 13:28:30.108705 140563755112256 train_utils.py:411] eval in step: 600, loss: 0.7118, acc: 0.5150
I0712 13:28:30.111975 140563755112256 train_utils.py:421] Testing...
I0712 13:28:33.742549 140563755112256 train_utils.py:424] test in step: 600, loss: 0.7008, acc: 0.5400
I0712 13:28:33.780912 140563755112256 train_utils.py:377] train in step: 601
I0712 13:28:33.841768 140563755112256 train_utils.py:377] train in step: 602
I0712 13:28:33.916110 140563755112256 train_utils.py:377] train in step: 603
I0712 13:28:34.006270 140563755112256 train_utils.py:377] train in step: 604
I0712 13:28:34.073005 140563755112256 train_utils.py:377] train in step: 605
I0712 13:28:34.141067 140563755112256 train_utils.py:377] train in step: 606
I0712 13:28:34.211551 140563755112256 train_utils.py:377] train in step: 607
I0712 13:28:34.277771 140563755112256 train_utils.py:377] train in step: 608
I0712 13:28:34.343636 140563755112256 train_utils.py:377] train in step: 609
I0712 13:28:34.411691 140563755112256 train_utils.py:377] train in step: 610
I0712 13:28:34.480665 140563755112256 train_utils.py:377] train in step: 611
I0712 13:28:34.550762 140563755112256 train_utils.py:377] train in step: 612
I0712 13:28:34.619277 140563755112256 train_utils.py:377] train in step: 613
I0712 13:28:34.690252 140563755112256 train_utils.py:377] train in step: 614
I0712 13:28:34.761370 140563755112256 train_utils.py:377] train in step: 615
I0712 13:28:34.830274 140563755112256 train_utils.py:377] train in step: 616
I0712 13:28:34.902299 140563755112256 train_utils.py:377] train in step: 617
I0712 13:28:34.969030 140563755112256 train_utils.py:377] train in step: 618
I0712 13:28:35.037791 140563755112256 train_utils.py:377] train in step: 619
I0712 13:28:35.107002 140563755112256 train_utils.py:377] train in step: 620
I0712 13:28:35.177440 140563755112256 train_utils.py:377] train in step: 621
I0712 13:28:35.244342 140563755112256 train_utils.py:377] train in step: 622
I0712 13:28:35.316428 140563755112256 train_utils.py:377] train in step: 623
I0712 13:28:35.384723 140563755112256 train_utils.py:377] train in step: 624
I0712 13:28:35.454859 140563755112256 train_utils.py:377] train in step: 625
I0712 13:28:35.529956 140563755112256 train_utils.py:377] train in step: 626
I0712 13:28:35.596151 140563755112256 train_utils.py:377] train in step: 627
I0712 13:28:35.665182 140563755112256 train_utils.py:377] train in step: 628
I0712 13:28:35.734924 140563755112256 train_utils.py:377] train in step: 629
I0712 13:28:35.808624 140563755112256 train_utils.py:377] train in step: 630
I0712 13:28:35.873873 140563755112256 train_utils.py:377] train in step: 631
I0712 13:28:35.942119 140563755112256 train_utils.py:377] train in step: 632
I0712 13:28:36.010472 140563755112256 train_utils.py:377] train in step: 633
I0712 13:28:36.085017 140563755112256 train_utils.py:377] train in step: 634
I0712 13:28:36.152214 140563755112256 train_utils.py:377] train in step: 635
I0712 13:28:36.221476 140563755112256 train_utils.py:377] train in step: 636
I0712 13:28:36.290986 140563755112256 train_utils.py:377] train in step: 637
I0712 13:28:36.357628 140563755112256 train_utils.py:377] train in step: 638
I0712 13:28:36.426080 140563755112256 train_utils.py:377] train in step: 639
I0712 13:28:36.495007 140563755112256 train_utils.py:377] train in step: 640
I0712 13:28:36.566210 140563755112256 train_utils.py:377] train in step: 641
I0712 13:28:36.632366 140563755112256 train_utils.py:377] train in step: 642
I0712 13:28:36.702996 140563755112256 train_utils.py:377] train in step: 643
I0712 13:28:36.770192 140563755112256 train_utils.py:377] train in step: 644
I0712 13:28:36.839640 140563755112256 train_utils.py:377] train in step: 645
I0712 13:28:36.907102 140563755112256 train_utils.py:377] train in step: 646
I0712 13:28:36.975690 140563755112256 train_utils.py:377] train in step: 647
I0712 13:28:37.043819 140563755112256 train_utils.py:377] train in step: 648
I0712 13:28:37.112004 140563755112256 train_utils.py:377] train in step: 649
I0712 13:28:37.180500 140563755112256 train_utils.py:377] train in step: 650
I0712 13:28:37.249846 140563755112256 train_utils.py:377] train in step: 651
I0712 13:28:37.317908 140563755112256 train_utils.py:377] train in step: 652
I0712 13:28:37.386512 140563755112256 train_utils.py:377] train in step: 653
I0712 13:28:37.457059 140563755112256 train_utils.py:377] train in step: 654
I0712 13:28:37.526381 140563755112256 train_utils.py:377] train in step: 655
I0712 13:28:37.594981 140563755112256 train_utils.py:377] train in step: 656
I0712 13:28:37.664009 140563755112256 train_utils.py:377] train in step: 657
I0712 13:28:37.733172 140563755112256 train_utils.py:377] train in step: 658
I0712 13:28:37.802019 140563755112256 train_utils.py:377] train in step: 659
I0712 13:28:37.870848 140563755112256 train_utils.py:377] train in step: 660
I0712 13:28:37.939699 140563755112256 train_utils.py:377] train in step: 661
I0712 13:28:38.008149 140563755112256 train_utils.py:377] train in step: 662
I0712 13:28:38.077491 140563755112256 train_utils.py:377] train in step: 663
I0712 13:28:38.152895 140563755112256 train_utils.py:377] train in step: 664
I0712 13:28:38.219985 140563755112256 train_utils.py:377] train in step: 665
I0712 13:28:38.288974 140563755112256 train_utils.py:377] train in step: 666
I0712 13:28:38.357767 140563755112256 train_utils.py:377] train in step: 667
I0712 13:28:38.428785 140563755112256 train_utils.py:377] train in step: 668
I0712 13:28:38.497153 140563755112256 train_utils.py:377] train in step: 669
I0712 13:28:38.566501 140563755112256 train_utils.py:377] train in step: 670
I0712 13:28:38.635060 140563755112256 train_utils.py:377] train in step: 671
I0712 13:28:38.704032 140563755112256 train_utils.py:377] train in step: 672
I0712 13:28:38.772742 140563755112256 train_utils.py:377] train in step: 673
I0712 13:28:38.842297 140563755112256 train_utils.py:377] train in step: 674
I0712 13:28:38.911386 140563755112256 train_utils.py:377] train in step: 675
I0712 13:28:38.981224 140563755112256 train_utils.py:377] train in step: 676
I0712 13:28:39.051476 140563755112256 train_utils.py:377] train in step: 677
I0712 13:28:39.120304 140563755112256 train_utils.py:377] train in step: 678
I0712 13:28:39.190848 140563755112256 train_utils.py:377] train in step: 679
I0712 13:28:39.257816 140563755112256 train_utils.py:377] train in step: 680
I0712 13:28:39.326571 140563755112256 train_utils.py:377] train in step: 681
I0712 13:28:39.396619 140563755112256 train_utils.py:377] train in step: 682
I0712 13:28:39.464782 140563755112256 train_utils.py:377] train in step: 683
I0712 13:28:39.535344 140563755112256 train_utils.py:377] train in step: 684
I0712 13:28:39.603195 140563755112256 train_utils.py:377] train in step: 685
I0712 13:28:39.673156 140563755112256 train_utils.py:377] train in step: 686
I0712 13:28:39.742708 140563755112256 train_utils.py:377] train in step: 687
I0712 13:28:39.813183 140563755112256 train_utils.py:377] train in step: 688
I0712 13:28:39.881606 140563755112256 train_utils.py:377] train in step: 689
I0712 13:28:39.949553 140563755112256 train_utils.py:377] train in step: 690
I0712 13:28:40.019017 140563755112256 train_utils.py:377] train in step: 691
I0712 13:28:40.088421 140563755112256 train_utils.py:377] train in step: 692
I0712 13:28:40.156152 140563755112256 train_utils.py:377] train in step: 693
I0712 13:28:40.225598 140563755112256 train_utils.py:377] train in step: 694
I0712 13:28:40.294670 140563755112256 train_utils.py:377] train in step: 695
I0712 13:28:40.363081 140563755112256 train_utils.py:377] train in step: 696
I0712 13:28:40.431771 140563755112256 train_utils.py:377] train in step: 697
I0712 13:28:40.500463 140563755112256 train_utils.py:377] train in step: 698
I0712 13:28:40.570280 140563755112256 train_utils.py:377] train in step: 699
I0712 13:28:40.638355 140563755112256 train_utils.py:377] train in step: 700
I0712 13:28:40.707422 140563755112256 train_utils.py:377] train in step: 701
I0712 13:28:40.776745 140563755112256 train_utils.py:377] train in step: 702
I0712 13:28:40.845675 140563755112256 train_utils.py:377] train in step: 703
I0712 13:28:40.915177 140563755112256 train_utils.py:377] train in step: 704
I0712 13:28:40.985523 140563755112256 train_utils.py:377] train in step: 705
I0712 13:28:41.054192 140563755112256 train_utils.py:377] train in step: 706
I0712 13:28:41.123005 140563755112256 train_utils.py:377] train in step: 707
I0712 13:28:41.192001 140563755112256 train_utils.py:377] train in step: 708
I0712 13:28:41.263047 140563755112256 train_utils.py:377] train in step: 709
I0712 13:28:41.331949 140563755112256 train_utils.py:377] train in step: 710
I0712 13:28:41.400314 140563755112256 train_utils.py:377] train in step: 711
I0712 13:28:41.469410 140563755112256 train_utils.py:377] train in step: 712
I0712 13:28:41.543862 140563755112256 train_utils.py:377] train in step: 713
I0712 13:28:41.621271 140563755112256 train_utils.py:377] train in step: 714
I0712 13:28:41.690609 140563755112256 train_utils.py:377] train in step: 715
I0712 13:28:41.759478 140563755112256 train_utils.py:377] train in step: 716
I0712 13:28:41.828092 140563755112256 train_utils.py:377] train in step: 717
I0712 13:28:41.898709 140563755112256 train_utils.py:377] train in step: 718
I0712 13:28:41.965640 140563755112256 train_utils.py:377] train in step: 719
I0712 13:28:42.035001 140563755112256 train_utils.py:377] train in step: 720
I0712 13:28:42.103594 140563755112256 train_utils.py:377] train in step: 721
I0712 13:28:42.173940 140563755112256 train_utils.py:377] train in step: 722
I0712 13:28:42.245064 140563755112256 train_utils.py:377] train in step: 723
I0712 13:28:42.315139 140563755112256 train_utils.py:377] train in step: 724
I0712 13:28:42.383648 140563755112256 train_utils.py:377] train in step: 725
I0712 13:28:42.453935 140563755112256 train_utils.py:377] train in step: 726
I0712 13:28:42.521054 140563755112256 train_utils.py:377] train in step: 727
I0712 13:28:42.589847 140563755112256 train_utils.py:377] train in step: 728
I0712 13:28:42.658595 140563755112256 train_utils.py:377] train in step: 729
I0712 13:28:42.727437 140563755112256 train_utils.py:377] train in step: 730
I0712 13:28:42.797313 140563755112256 train_utils.py:377] train in step: 731
I0712 13:28:42.866585 140563755112256 train_utils.py:377] train in step: 732
I0712 13:28:42.935575 140563755112256 train_utils.py:377] train in step: 733
I0712 13:28:43.003425 140563755112256 train_utils.py:377] train in step: 734
I0712 13:28:43.072324 140563755112256 train_utils.py:377] train in step: 735
I0712 13:28:43.141300 140563755112256 train_utils.py:377] train in step: 736
I0712 13:28:43.210443 140563755112256 train_utils.py:377] train in step: 737
I0712 13:28:43.281513 140563755112256 train_utils.py:377] train in step: 738
I0712 13:28:43.348701 140563755112256 train_utils.py:377] train in step: 739
I0712 13:28:43.418640 140563755112256 train_utils.py:377] train in step: 740
I0712 13:28:43.487354 140563755112256 train_utils.py:377] train in step: 741
I0712 13:28:43.557053 140563755112256 train_utils.py:377] train in step: 742
I0712 13:28:43.626965 140563755112256 train_utils.py:377] train in step: 743
I0712 13:28:43.695425 140563755112256 train_utils.py:377] train in step: 744
I0712 13:28:43.763967 140563755112256 train_utils.py:377] train in step: 745
I0712 13:28:43.833180 140563755112256 train_utils.py:377] train in step: 746
I0712 13:28:43.903289 140563755112256 train_utils.py:377] train in step: 747
I0712 13:28:43.972016 140563755112256 train_utils.py:377] train in step: 748
I0712 13:28:44.040396 140563755112256 train_utils.py:377] train in step: 749
I0712 13:28:44.108718 140563755112256 train_utils.py:377] train in step: 750
I0712 13:28:44.178303 140563755112256 train_utils.py:377] train in step: 751
I0712 13:28:44.247848 140563755112256 train_utils.py:377] train in step: 752
I0712 13:28:44.316009 140563755112256 train_utils.py:377] train in step: 753
I0712 13:28:44.385432 140563755112256 train_utils.py:377] train in step: 754
I0712 13:28:44.454002 140563755112256 train_utils.py:377] train in step: 755
I0712 13:28:44.522123 140563755112256 train_utils.py:377] train in step: 756
I0712 13:28:44.592584 140563755112256 train_utils.py:377] train in step: 757
I0712 13:28:44.662003 140563755112256 train_utils.py:377] train in step: 758
I0712 13:28:44.730485 140563755112256 train_utils.py:377] train in step: 759
I0712 13:28:44.800440 140563755112256 train_utils.py:377] train in step: 760
I0712 13:28:44.868978 140563755112256 train_utils.py:377] train in step: 761
I0712 13:28:44.937487 140563755112256 train_utils.py:377] train in step: 762
I0712 13:28:45.005900 140563755112256 train_utils.py:377] train in step: 763
I0712 13:28:45.074786 140563755112256 train_utils.py:377] train in step: 764
I0712 13:28:45.143788 140563755112256 train_utils.py:377] train in step: 765
I0712 13:28:45.214328 140563755112256 train_utils.py:377] train in step: 766
I0712 13:28:45.284367 140563755112256 train_utils.py:377] train in step: 767
I0712 13:28:45.353068 140563755112256 train_utils.py:377] train in step: 768
I0712 13:28:45.422700 140563755112256 train_utils.py:377] train in step: 769
I0712 13:28:45.490925 140563755112256 train_utils.py:377] train in step: 770
I0712 13:28:45.560597 140563755112256 train_utils.py:377] train in step: 771
I0712 13:28:45.629024 140563755112256 train_utils.py:377] train in step: 772
I0712 13:28:45.697890 140563755112256 train_utils.py:377] train in step: 773
I0712 13:28:45.766483 140563755112256 train_utils.py:377] train in step: 774
I0712 13:28:45.836594 140563755112256 train_utils.py:377] train in step: 775
I0712 13:28:45.904930 140563755112256 train_utils.py:377] train in step: 776
I0712 13:28:45.973169 140563755112256 train_utils.py:377] train in step: 777
I0712 13:28:46.042909 140563755112256 train_utils.py:377] train in step: 778
I0712 13:28:46.111736 140563755112256 train_utils.py:377] train in step: 779
I0712 13:28:46.187629 140563755112256 train_utils.py:377] train in step: 780
I0712 13:28:46.255778 140563755112256 train_utils.py:377] train in step: 781
I0712 13:28:46.324710 140563755112256 train_utils.py:377] train in step: 782
I0712 13:28:46.393724 140563755112256 train_utils.py:377] train in step: 783
I0712 13:28:46.462603 140563755112256 train_utils.py:377] train in step: 784
I0712 13:28:46.531156 140563755112256 train_utils.py:377] train in step: 785
I0712 13:28:46.599930 140563755112256 train_utils.py:377] train in step: 786
I0712 13:28:46.670246 140563755112256 train_utils.py:377] train in step: 787
I0712 13:28:46.738445 140563755112256 train_utils.py:377] train in step: 788
I0712 13:28:46.807894 140563755112256 train_utils.py:377] train in step: 789
I0712 13:28:46.880536 140563755112256 train_utils.py:377] train in step: 790
I0712 13:28:46.951180 140563755112256 train_utils.py:377] train in step: 791
I0712 13:28:47.020166 140563755112256 train_utils.py:377] train in step: 792
I0712 13:28:47.089642 140563755112256 train_utils.py:377] train in step: 793
I0712 13:28:47.157751 140563755112256 train_utils.py:377] train in step: 794
I0712 13:28:47.225931 140563755112256 train_utils.py:377] train in step: 795
I0712 13:28:47.295282 140563755112256 train_utils.py:377] train in step: 796
I0712 13:28:47.364058 140563755112256 train_utils.py:377] train in step: 797
I0712 13:28:47.433074 140563755112256 train_utils.py:377] train in step: 798
I0712 13:28:47.515735 140563755112256 train_utils.py:377] train in step: 799
I0712 13:28:47.595129 140563755112256 train_utils.py:377] train in step: 800
I0712 13:28:47.667396 140563755112256 train_utils.py:396] train in step: 800, loss: 0.7012999653816223, acc: 0.5324999690055847
I0712 13:28:51.264285 140563755112256 train_utils.py:411] eval in step: 800, loss: 0.7141, acc: 0.5000
I0712 13:28:51.267816 140563755112256 train_utils.py:421] Testing...
I0712 13:28:54.885348 140563755112256 train_utils.py:424] test in step: 800, loss: 0.7224, acc: 0.4800
I0712 13:28:54.922559 140563755112256 train_utils.py:377] train in step: 801
I0712 13:28:54.984280 140563755112256 train_utils.py:377] train in step: 802
I0712 13:28:55.052459 140563755112256 train_utils.py:377] train in step: 803
I0712 13:28:55.127203 140563755112256 train_utils.py:377] train in step: 804
I0712 13:28:55.192487 140563755112256 train_utils.py:377] train in step: 805
I0712 13:28:55.262749 140563755112256 train_utils.py:377] train in step: 806
I0712 13:28:55.330990 140563755112256 train_utils.py:377] train in step: 807
I0712 13:28:55.400756 140563755112256 train_utils.py:377] train in step: 808
I0712 13:28:55.482981 140563755112256 train_utils.py:377] train in step: 809
I0712 13:28:55.551342 140563755112256 train_utils.py:377] train in step: 810
I0712 13:28:55.621447 140563755112256 train_utils.py:377] train in step: 811
I0712 13:28:55.702790 140563755112256 train_utils.py:377] train in step: 812
I0712 13:28:55.771877 140563755112256 train_utils.py:377] train in step: 813
I0712 13:28:55.840862 140563755112256 train_utils.py:377] train in step: 814
I0712 13:28:55.910471 140563755112256 train_utils.py:377] train in step: 815
I0712 13:28:55.981437 140563755112256 train_utils.py:377] train in step: 816
I0712 13:28:56.050247 140563755112256 train_utils.py:377] train in step: 817
I0712 13:28:56.120630 140563755112256 train_utils.py:377] train in step: 818
I0712 13:28:56.190022 140563755112256 train_utils.py:377] train in step: 819
I0712 13:28:56.262489 140563755112256 train_utils.py:377] train in step: 820
I0712 13:28:56.331640 140563755112256 train_utils.py:377] train in step: 821
I0712 13:28:56.400376 140563755112256 train_utils.py:377] train in step: 822
I0712 13:28:56.470849 140563755112256 train_utils.py:377] train in step: 823
I0712 13:28:56.538234 140563755112256 train_utils.py:377] train in step: 824
I0712 13:28:56.607559 140563755112256 train_utils.py:377] train in step: 825
I0712 13:28:56.676565 140563755112256 train_utils.py:377] train in step: 826
I0712 13:28:56.745343 140563755112256 train_utils.py:377] train in step: 827
I0712 13:28:56.814308 140563755112256 train_utils.py:377] train in step: 828
I0712 13:28:56.883606 140563755112256 train_utils.py:377] train in step: 829
I0712 13:28:56.952771 140563755112256 train_utils.py:377] train in step: 830
I0712 13:28:57.023849 140563755112256 train_utils.py:377] train in step: 831
I0712 13:28:57.092798 140563755112256 train_utils.py:377] train in step: 832
I0712 13:28:57.160797 140563755112256 train_utils.py:377] train in step: 833
I0712 13:28:57.229739 140563755112256 train_utils.py:377] train in step: 834
I0712 13:28:57.298125 140563755112256 train_utils.py:377] train in step: 835
I0712 13:28:57.366806 140563755112256 train_utils.py:377] train in step: 836
I0712 13:28:57.436192 140563755112256 train_utils.py:377] train in step: 837
I0712 13:28:57.504913 140563755112256 train_utils.py:377] train in step: 838
I0712 13:28:57.575016 140563755112256 train_utils.py:377] train in step: 839
I0712 13:28:57.644871 140563755112256 train_utils.py:377] train in step: 840
I0712 13:28:57.714032 140563755112256 train_utils.py:377] train in step: 841
I0712 13:28:57.782262 140563755112256 train_utils.py:377] train in step: 842
I0712 13:28:57.851365 140563755112256 train_utils.py:377] train in step: 843
I0712 13:28:57.919994 140563755112256 train_utils.py:377] train in step: 844
I0712 13:28:57.988612 140563755112256 train_utils.py:377] train in step: 845
I0712 13:28:58.058455 140563755112256 train_utils.py:377] train in step: 846
I0712 13:28:58.128142 140563755112256 train_utils.py:377] train in step: 847
I0712 13:28:58.196988 140563755112256 train_utils.py:377] train in step: 848
I0712 13:28:58.269828 140563755112256 train_utils.py:377] train in step: 849
I0712 13:28:58.338552 140563755112256 train_utils.py:377] train in step: 850
I0712 13:28:58.408918 140563755112256 train_utils.py:377] train in step: 851
I0712 13:28:58.476630 140563755112256 train_utils.py:377] train in step: 852
I0712 13:28:58.545938 140563755112256 train_utils.py:377] train in step: 853
I0712 13:28:58.614613 140563755112256 train_utils.py:377] train in step: 854
I0712 13:28:58.683814 140563755112256 train_utils.py:377] train in step: 855
I0712 13:28:58.752720 140563755112256 train_utils.py:377] train in step: 856
I0712 13:28:58.820717 140563755112256 train_utils.py:377] train in step: 857
I0712 13:28:58.890467 140563755112256 train_utils.py:377] train in step: 858
I0712 13:28:58.970481 140563755112256 train_utils.py:377] train in step: 859
I0712 13:28:59.048909 140563755112256 train_utils.py:377] train in step: 860
I0712 13:28:59.117280 140563755112256 train_utils.py:377] train in step: 861
I0712 13:28:59.185498 140563755112256 train_utils.py:377] train in step: 862
I0712 13:28:59.254302 140563755112256 train_utils.py:377] train in step: 863
I0712 13:28:59.322643 140563755112256 train_utils.py:377] train in step: 864
I0712 13:28:59.392998 140563755112256 train_utils.py:377] train in step: 865
I0712 13:28:59.462216 140563755112256 train_utils.py:377] train in step: 866
I0712 13:28:59.530806 140563755112256 train_utils.py:377] train in step: 867
I0712 13:28:59.600502 140563755112256 train_utils.py:377] train in step: 868
I0712 13:28:59.671680 140563755112256 train_utils.py:377] train in step: 869
I0712 13:28:59.741210 140563755112256 train_utils.py:377] train in step: 870
I0712 13:28:59.809468 140563755112256 train_utils.py:377] train in step: 871
I0712 13:28:59.878334 140563755112256 train_utils.py:377] train in step: 872
I0712 13:28:59.947595 140563755112256 train_utils.py:377] train in step: 873
I0712 13:29:00.015992 140563755112256 train_utils.py:377] train in step: 874
I0712 13:29:00.084465 140563755112256 train_utils.py:377] train in step: 875
I0712 13:29:00.156187 140563755112256 train_utils.py:377] train in step: 876
I0712 13:29:00.225468 140563755112256 train_utils.py:377] train in step: 877
I0712 13:29:00.306962 140563755112256 train_utils.py:377] train in step: 878
I0712 13:29:00.381077 140563755112256 train_utils.py:377] train in step: 879
I0712 13:29:00.449642 140563755112256 train_utils.py:377] train in step: 880
I0712 13:29:00.517464 140563755112256 train_utils.py:377] train in step: 881
I0712 13:29:00.586876 140563755112256 train_utils.py:377] train in step: 882
I0712 13:29:00.655572 140563755112256 train_utils.py:377] train in step: 883
I0712 13:29:00.724431 140563755112256 train_utils.py:377] train in step: 884
I0712 13:29:00.795051 140563755112256 train_utils.py:377] train in step: 885
I0712 13:29:00.862637 140563755112256 train_utils.py:377] train in step: 886
I0712 13:29:00.931525 140563755112256 train_utils.py:377] train in step: 887
I0712 13:29:01.002552 140563755112256 train_utils.py:377] train in step: 888
I0712 13:29:01.077073 140563755112256 train_utils.py:377] train in step: 889
I0712 13:29:01.147269 140563755112256 train_utils.py:377] train in step: 890
I0712 13:29:01.217429 140563755112256 train_utils.py:377] train in step: 891
I0712 13:29:01.285857 140563755112256 train_utils.py:377] train in step: 892
I0712 13:29:01.354664 140563755112256 train_utils.py:377] train in step: 893
I0712 13:29:01.422990 140563755112256 train_utils.py:377] train in step: 894
I0712 13:29:01.492474 140563755112256 train_utils.py:377] train in step: 895
I0712 13:29:01.560866 140563755112256 train_utils.py:377] train in step: 896
I0712 13:29:01.629708 140563755112256 train_utils.py:377] train in step: 897
I0712 13:29:01.699325 140563755112256 train_utils.py:377] train in step: 898
I0712 13:29:01.780668 140563755112256 train_utils.py:377] train in step: 899
I0712 13:29:01.850681 140563755112256 train_utils.py:377] train in step: 900
I0712 13:29:01.920613 140563755112256 train_utils.py:377] train in step: 901
I0712 13:29:01.988044 140563755112256 train_utils.py:377] train in step: 902
I0712 13:29:02.056633 140563755112256 train_utils.py:377] train in step: 903
I0712 13:29:02.125769 140563755112256 train_utils.py:377] train in step: 904
I0712 13:29:02.194961 140563755112256 train_utils.py:377] train in step: 905
I0712 13:29:02.265189 140563755112256 train_utils.py:377] train in step: 906
I0712 13:29:02.333793 140563755112256 train_utils.py:377] train in step: 907
I0712 13:29:02.406508 140563755112256 train_utils.py:377] train in step: 908
I0712 13:29:02.473528 140563755112256 train_utils.py:377] train in step: 909
I0712 13:29:02.542569 140563755112256 train_utils.py:377] train in step: 910
I0712 13:29:02.611041 140563755112256 train_utils.py:377] train in step: 911
I0712 13:29:02.679352 140563755112256 train_utils.py:377] train in step: 912
I0712 13:29:02.748281 140563755112256 train_utils.py:377] train in step: 913
I0712 13:29:02.820322 140563755112256 train_utils.py:377] train in step: 914
I0712 13:29:02.888288 140563755112256 train_utils.py:377] train in step: 915
I0712 13:29:02.957038 140563755112256 train_utils.py:377] train in step: 916
I0712 13:29:03.028415 140563755112256 train_utils.py:377] train in step: 917
I0712 13:29:03.098283 140563755112256 train_utils.py:377] train in step: 918
I0712 13:29:03.169706 140563755112256 train_utils.py:377] train in step: 919
I0712 13:29:03.238543 140563755112256 train_utils.py:377] train in step: 920
I0712 13:29:03.307995 140563755112256 train_utils.py:377] train in step: 921
I0712 13:29:03.374842 140563755112256 train_utils.py:377] train in step: 922
I0712 13:29:03.444340 140563755112256 train_utils.py:377] train in step: 923
I0712 13:29:03.512558 140563755112256 train_utils.py:377] train in step: 924
I0712 13:29:03.581452 140563755112256 train_utils.py:377] train in step: 925
I0712 13:29:03.650583 140563755112256 train_utils.py:377] train in step: 926
I0712 13:29:03.720053 140563755112256 train_utils.py:377] train in step: 927
I0712 13:29:03.790265 140563755112256 train_utils.py:377] train in step: 928
I0712 13:29:03.862040 140563755112256 train_utils.py:377] train in step: 929
I0712 13:29:03.930792 140563755112256 train_utils.py:377] train in step: 930
I0712 13:29:03.998094 140563755112256 train_utils.py:377] train in step: 931
I0712 13:29:04.066793 140563755112256 train_utils.py:377] train in step: 932
I0712 13:29:04.137337 140563755112256 train_utils.py:377] train in step: 933
I0712 13:29:04.205915 140563755112256 train_utils.py:377] train in step: 934
I0712 13:29:04.274420 140563755112256 train_utils.py:377] train in step: 935
I0712 13:29:04.343561 140563755112256 train_utils.py:377] train in step: 936
I0712 13:29:04.413931 140563755112256 train_utils.py:377] train in step: 937
I0712 13:29:04.491760 140563755112256 train_utils.py:377] train in step: 938
I0712 13:29:04.565478 140563755112256 train_utils.py:377] train in step: 939
I0712 13:29:04.632334 140563755112256 train_utils.py:377] train in step: 940
I0712 13:29:04.700350 140563755112256 train_utils.py:377] train in step: 941
I0712 13:29:04.770066 140563755112256 train_utils.py:377] train in step: 942
I0712 13:29:04.839142 140563755112256 train_utils.py:377] train in step: 943
I0712 13:29:04.908136 140563755112256 train_utils.py:377] train in step: 944
I0712 13:29:04.977180 140563755112256 train_utils.py:377] train in step: 945
I0712 13:29:05.046273 140563755112256 train_utils.py:377] train in step: 946
I0712 13:29:05.114977 140563755112256 train_utils.py:377] train in step: 947
I0712 13:29:05.184684 140563755112256 train_utils.py:377] train in step: 948
I0712 13:29:05.255711 140563755112256 train_utils.py:377] train in step: 949
I0712 13:29:05.322277 140563755112256 train_utils.py:377] train in step: 950
I0712 13:29:05.392990 140563755112256 train_utils.py:377] train in step: 951
I0712 13:29:05.461660 140563755112256 train_utils.py:377] train in step: 952
I0712 13:29:05.530201 140563755112256 train_utils.py:377] train in step: 953
I0712 13:29:05.599269 140563755112256 train_utils.py:377] train in step: 954
I0712 13:29:05.668203 140563755112256 train_utils.py:377] train in step: 955
I0712 13:29:05.737586 140563755112256 train_utils.py:377] train in step: 956
I0712 13:29:05.806877 140563755112256 train_utils.py:377] train in step: 957
I0712 13:29:05.877815 140563755112256 train_utils.py:377] train in step: 958
I0712 13:29:05.948499 140563755112256 train_utils.py:377] train in step: 959
I0712 13:29:06.016727 140563755112256 train_utils.py:377] train in step: 960
I0712 13:29:06.086155 140563755112256 train_utils.py:377] train in step: 961
I0712 13:29:06.155309 140563755112256 train_utils.py:377] train in step: 962
I0712 13:29:06.221625 140563755112256 train_utils.py:377] train in step: 963
I0712 13:29:06.290577 140563755112256 train_utils.py:377] train in step: 964
I0712 13:29:06.359357 140563755112256 train_utils.py:377] train in step: 965
I0712 13:29:06.431158 140563755112256 train_utils.py:377] train in step: 966
I0712 13:29:06.506061 140563755112256 train_utils.py:377] train in step: 967
I0712 13:29:06.572843 140563755112256 train_utils.py:377] train in step: 968
I0712 13:29:06.640519 140563755112256 train_utils.py:377] train in step: 969
I0712 13:29:06.709704 140563755112256 train_utils.py:377] train in step: 970
I0712 13:29:06.778222 140563755112256 train_utils.py:377] train in step: 971
I0712 13:29:06.849936 140563755112256 train_utils.py:377] train in step: 972
I0712 13:29:06.918627 140563755112256 train_utils.py:377] train in step: 973
I0712 13:29:06.987101 140563755112256 train_utils.py:377] train in step: 974
I0712 13:29:07.058304 140563755112256 train_utils.py:377] train in step: 975
I0712 13:29:07.128055 140563755112256 train_utils.py:377] train in step: 976
I0712 13:29:07.195748 140563755112256 train_utils.py:377] train in step: 977
I0712 13:29:07.266419 140563755112256 train_utils.py:377] train in step: 978
I0712 13:29:07.335369 140563755112256 train_utils.py:377] train in step: 979
I0712 13:29:07.403789 140563755112256 train_utils.py:377] train in step: 980
I0712 13:29:07.473341 140563755112256 train_utils.py:377] train in step: 981
I0712 13:29:07.541780 140563755112256 train_utils.py:377] train in step: 982
I0712 13:29:07.612536 140563755112256 train_utils.py:377] train in step: 983
I0712 13:29:07.681065 140563755112256 train_utils.py:377] train in step: 984
I0712 13:29:07.749402 140563755112256 train_utils.py:377] train in step: 985
I0712 13:29:07.821030 140563755112256 train_utils.py:377] train in step: 986
I0712 13:29:07.892370 140563755112256 train_utils.py:377] train in step: 987
I0712 13:29:07.958761 140563755112256 train_utils.py:377] train in step: 988
I0712 13:29:08.027473 140563755112256 train_utils.py:377] train in step: 989
I0712 13:29:08.096571 140563755112256 train_utils.py:377] train in step: 990
I0712 13:29:08.165711 140563755112256 train_utils.py:377] train in step: 991
I0712 13:29:08.233814 140563755112256 train_utils.py:377] train in step: 992
I0712 13:29:08.304564 140563755112256 train_utils.py:377] train in step: 993
I0712 13:29:08.371628 140563755112256 train_utils.py:377] train in step: 994
I0712 13:29:08.440395 140563755112256 train_utils.py:377] train in step: 995
I0712 13:29:08.510163 140563755112256 train_utils.py:377] train in step: 996
I0712 13:29:08.580831 140563755112256 train_utils.py:377] train in step: 997
I0712 13:29:08.649465 140563755112256 train_utils.py:377] train in step: 998
I0712 13:29:08.718974 140563755112256 train_utils.py:377] train in step: 999
I0712 13:29:08.787854 140563755112256 train_utils.py:377] train in step: 1000
I0712 13:29:08.792259 140563755112256 checkpoints.py:120] Saving checkpoint at step: 1000
I0712 13:29:08.927214 140563755112256 checkpoints.py:149] Saved checkpoint at trained_models/matching/bigbird/checkpoint_1000
I0712 13:29:08.974531 140563755112256 train_utils.py:396] train in step: 1000, loss: 0.6973999738693237, acc: 0.5174999833106995
I0712 13:29:12.518805 140563755112256 train_utils.py:411] eval in step: 1000, loss: 0.6928, acc: 0.5150
I0712 13:29:12.521881 140563755112256 train_utils.py:421] Testing...
I0712 13:29:16.275617 140563755112256 train_utils.py:424] test in step: 1000, loss: 0.6928, acc: 0.5150
