2022-07-12 13:41:58.250923: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:41:58.251192: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:41:58.251382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2022-07-12 13:41:58.251581: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2022-07-12 13:41:58.251760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:41:58.251960: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2022-07-12 13:41:58.252140: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2022-07-12 13:41:58.252205: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0712 13:41:58.252645 140545052657472 train.py:67] ===========Config Dict============
I0712 13:41:58.252931 140545052657472 train.py:68] available_devices:
- 0
- 1
- 2
- 3
base_type: dual_encoder
batch_size: 4
checkpoint_freq: 10000
data_dir: google_datasets/doc_retrieval/tsv_data/
emb_dim: 128
eval_frequency: 200
factors: constant * linear_warmup * rsqrt_decay
learning_rate: 0.05
max_eval_target_length: 200
max_length: 4000
max_predict_token_length: 50
max_target_length: 200
mlp_dim: 512
model_type: performer
num_eval_steps: 50
num_heads: 4
num_layers: 4
num_train_steps: 1001
pooling_mode: CLS
prompt: ''
qkv_dim: 128
random_seed: 0
restore_checkpoints: true
sampling_temperature: 0.6
sampling_top_k: 20
save_checkpoints: true
task_name: aan_pairs
tokenizer: char
trial: 0
vocab_file_path: google_datasets/doc_retrieval/aan/
warmup: 1000
weight_decay: 0.1

I0712 13:41:58.265799 140545052657472 xla_bridge.py:340] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0712 13:41:59.348335 140545052657472 xla_bridge.py:340] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0712 13:41:59.348977 140545052657472 xla_bridge.py:340] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0712 13:41:59.349186 140545052657472 train.py:80] GPU devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)]
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
I0712 13:41:59.349337 140545052657472 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_train.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
I0712 13:41:59.419135 140545052657472 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_eval.tsv
INFO:tensorflow:google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 13:41:59.451266 140545052657472 input_pipeline.py:32] google_datasets/doc_retrieval/tsv_data/aan_pairs_test.tsv
I0712 13:42:02.010536 140545052657472 input_pipeline.py:60] Data sample: OrderedDict([('label', 0.0), ('id1', b'C04-1130'), ('id2', b'P11-2072'), ('text1', b'b"1  Introduction    Word sense disambiguation (WSD) has long  been a central issue in Natural Language  Processing (NLP). In many NLP tasks, such as  Machine Translation, Information Retrieval etc.,  WSD plays a very important role in improving  the quality of systems. Many different algorithms  have been used for this task, including some  machine learning (ML) algorithms, such as  Na?ve Bayesian model, decision trees, and  example based learners. Since different  algorithms have different strengths and perform  well on different feature space, classifier  combination is a reasonable candidate to achieve  better performance by taking advantages of  different approaches. In the field of ML,  ensembles of classifiers have been shown to be  successful in last decade (Dietterich 1997). For the  specific task of WSD, classifier combination has  been received more and more attention in recent  years.  Kilgarriff and Rosenzweig (2000) presented  the first empirical study. They combined the  output of the participating SENSEVAL1 systems  via simple voting. Pedersen (2000) built an  ensemble of Na?ve Bayesian classifiers, each of  which is based on lexical features that represent  co-occurring words in varying sized windows of  context. The sense that receives majority of the  votes was assigned as the final selection.  Stevenson and Wilks (2001) presented a  classifier combination framework where three  different disambiguation modules were  combined using a memory-based approach.  Hoste et al (2002) used word experts consisted  of four memory-based learners trained on  different context. Output of the word experts is  based on majority voting or weighted voting.  Florian et al(2002) and Florian and Yarowsky  (2002) used six different classifiers as  components of their combination. They  compared several different strategies of  combination, which include combining the  posterior distribution, combination based on  order statistics and several different voting.    Klein et al (2002) combined a number of  different first-order classifiers using majority  voting, weighted voting and maximum entropy.  In Park (2003), a committee of classifiers was  used to learn from the unlabeled examples. The  label of an unlabeled example is predicted by  weighted majority voting. Frank at al. (2003)  presented a locally weighted Na?ve Bayesian  model. For a given test instance, they first chose  k-nearest-neighbors from training samples for it,  then constructed a Na?ve Bayesian classifier by  using these k-nearest-neighbors in stead of all  training samples.   This paper presents a new combinational  approach. We firstly construct a series of Na?ve  Bayesian classifiers along a sequence of orderly  varying sized windows of context, and make  sense selection for both training samples and test  samples using these classifiers. We thus get a  trajectory of sense selection for each sample, and  then use the sense trajectory based  k-nearest-neighbors to make final decision for     test samples.  This method is motivated by an observation  that there is an unavoidable uncertainty when a  classifier is used to make sense selection. Our  approach aims to alleviate this uncertainty and  thus make more robust utilization of context  while perform well. Experiments show our  approach outperform some other algorithms on  both robustness and performance.  The remainder of this paper is organized as  follows: Section 2 gives the motivation of our  approach, describes the uncertainty in sense  selection brought by classifiers themselves. In  section 3, we present the decision trajectory  based approach. We then implement some  experiments in section 4, and give some  evaluations and discussions in section 5. Finally,  we draw some conclusions.    2  The Trajectory of Sense Selection    Our method is originally motivated by an  observation on relation between sense selection  by a classifier and the context it uses to make this  selection.  As well known, context is the only means to  identify the sense of a polysemous word. Ide  (1998) identified three types of context:  micro-context, topical context and domain. In  practice, a context window ( l , r ), which  includes l  words to the left and r  words to  the right of the target word, is predetermined by  human or chosen automatically by a performance  criterion. Only information in the context  window is then used for classifiers and  disambiguating. What is the best window size for  WSD has been long for a problem. Weaver (1955)  hoped we could find a minimum value of the  window size which can lead to the correct choice  of sense for the target ambiguous word.  Yarowsky (1994) argued the optimal value is  sensitive to the type of ambiguity. Semantic or  topic-based ambiguities warrant a larger window  (from 20 to 50), while more local syntactic  ambiguities warrant a smaller window (3 or 4).  Leacock at el (1998) showed the local context is  superior to topical context as an indicator of  word sense. Yarowsky (2002) suggested that  different algorithms prefer different window  sizes.   Followed by these works, it is clear that  different window sizes might cause different  sense selection for an occurrence of the target  word even when a same algorithm is used.  Yarowsky (2002) gave a investigation on how  the performance changes with different window  sizes for several different algorithms and several  different types of word. In fact, even for human,  different window sizes might cause different  sense selections for a same occurrence of an  ambiguous word. For example, considering word  ???(It has two different senses: ?read? and  ?think?) in senesce S1.    S1: ?/  ?/ ?/?/  ?/   ??/   ?/  ?/.  (I) (think) (this) (book) (worthy) (a) (read)    When we use a context window (1,1), it is not  clear which sense should be more possible in this  sentence. When we use (3,3), because the  collocation with ?  give a very strong  indication for ??s sense, it is natural that we  select the sense of ?read? for ?. When we use  window (6,6), we select the sense of ?think? for  it.  Here, the occurrence of the ambiguous word is  the same; it is the difference of context windows  that make the sense selection different. Since the  context window is a built-in parameter of a  classifier, as long as we use a classifier to  distinguish an ambiguous word, we had to  choose a window size. Supposing a classifier is  an observer, choosing a window size is necessary  for the observer to implement an observation.  Different choices of the window size might cause  different observational results for the same  occurrence. That means there is an uncertainty  brought by observer itself. It reminds us that the  relation between the window size and the sense  selection is to some extent similar with the  relation between a particle?s position and its  momentum in Heisenberg Uncertainty Principle.   By the Uncertainty Principle, when we  measure the position and the momentum of a  particle, we cannot measure them with  zero-variance simultaneously. In Quantum  Theory, the wave-function is used to describe the  state of a particle. The method to deal with this  problem in Quantum Theory suggests us an idea  to deal with the similar problem in WSD.   Firstly, since the existence of the uncertainty  of sense selection at different window sizes,  sense selection for the target word at only one  context window cannot give a complete  description of its sense. To grasp a complete  description of its sense, it is necessary to get  sense selections along a series of observation, i.e.  using a sequence of context window to get a  trajectory of sense selection.   Secondly, unlike that in Uncertainty Principle,  the intuition is that, in most of time, when we  have enough observations, we can be doubtlessly     sure the sense of the target word. So, we make  final unique sense selection based on the  trajectory of sense selection. Since the final  selection is based on a sense trajectory along  different window sizes, we thus think it may  helpful to alleviate the uncertainty brought by  difference of context windows.   In this way, our approach aims to improve  robustness of WSD. Here the robustness means  that sense selection is not sensitive to the  window size. This kind of robustness is  especially important to WSD system in noise or  oral corpus, where there are many occasional  inserted words near the targetword. Besides  robustness, to achieve better performance is also  necessary, if robustness is at a low level of  performance, it is useless.     3  Decision Trajectory Based WSD    In our approach, we firstly use Na?ve Bayesian  (NB) algorithm to construct a sense selection  trajectory along a sequence of orderly varying  sized windows of context for each sample,  including both training samples and test samples.  Then we use k-nearest-neighbors(KNN)  algorithm to make final decision for each test  sample based on these trajectories.   Let w  be an ambiguous word, it has n   different senses, 1s ? is ? ns . Supposing we  have q  training samples 1S ? jS ? qS , where  iq  samples are tagged with sense is ,  qqi =? . We present our approach in two  stages: training stage and test stage. Figure 1  gives a skeleton of the algorithm.   In the training stage, we first choose a  sequence of context windows.     mT : ( 1p ,? kp ... mp )  Where kp =( kl , kr ) is a context window  which includes kl  words to the left of word w   and kr  words to the right. We call mT  a  trajectory of context windows, kp  is a window  point in this trajectory. For example, a trajectory  ((1,1), (3,3), (5,5), (7,7), (9,9)) includes 5 points.   For each window point kp  in mT , we  construct a classier by using NB algorithm based  on context word in kp . Let )( kpC  denote the  classifier, it can be thought as an operator that make  sense selection upon samples. With the change of  the window point, we can get a operate vector:   ))(),...,(),...,(( 1 mk pCpCpCC =    Training stage:  1. To construct a operator vector C  along a  sequence of context windows Tm :      ))(),...,(),...,(( 1 mk pCpCpCC = .  )( kpC  is a NB classifier learned by all the  tagged data using kp  as the context  window.    2.  For each training sample jS , to operate C   upon it to construct a sense trajectory, j?   ( qj ,...,1= ).    Test stage:   1.  For a new sample S , to construct its  decision trajectory ?  by operating C   upon it.   2.  For qj ,...,1= , to calculate ),( jd ??   3.  to make KNN-based sense choice for S .  Figure 1. The algorithm of trajectory-based WSD      For a sample jS  for sense is , we use  )( kpC ( jS ) to denote using )(kpC  to classify  jS , we can get a sense selection denoted by  )( kj p? , i.e., )( kpC ( jS )= )( kj p? . We call  )( kj p?  a point decision. If )( kj p? = is , we  borrow a term to call jS  an eigen-sample of the  operator )( kpC , is  is its eigenvaluve.  With the change of the window point, we get a  sequence of point decisions for sample jS   along the window trajectory mT , we denoted it  by    j? =( )( 1pj? ,?, )( mj p? )  We call it decision trajectory of sample jS   along the context windows trajectory mT . If all  elements of j?  is is , i.e. j? =( is ,?, is ), we  call jS  an eigen-sample of operator C , j?  is  a eigen-trajectory of C .   In this way, we transfer training samples into  training decision trajectories, which will be used  as instance for final KNN-based sense selection.  An eigen-trajectory is a good indication for a  sample, but when all the training samples are  eigen-samples, it is not a good thing for  disambiguating new samples. We will discuss  this case in section 5.2.   After finishing training stage of our approach,  we have a context windows trajectory mT , a  sequence of classifiers )( kpC  along mT ? and     a decision trajectory for each training sample. All  these compose of our classifier for a new sample  in test. When a new sample is given, we first  calculate a decision trajectory ?  for it by using  C  operating upon it. Let      ? =( )( 1p? ,?, )( mp? )   We then calculate the similarity between ?  and  j? , kj ,...2,1=  by using (3.1).  m pp Sim m i iji j ? == 1 ))(),(( ),( ??? ??      (3.1)  Where 1),( =yx?  at yx = , and 0),( =yx?   at yx ? . We then choose h  training decision  trajectory samples as ? ?s h  nearest neighbors,  supposing that ih  samples are tagged with  sense is  among these nearest neighbors,  hhi =? , ii qh ? , then by solving (3.2), We  choose ?is  as the final sense selection for the  new sample.  ? ???? ? = ihj j ni Simi 11 ),(maxarg ??            (3.2)  If all training samples are eign-samples, the  similarity between ?  and j?  for the same  sense are the same, (3.2) is changed to:   },...,1|,})({{|maxarg 1 mkipi k ni === ?? ? ? (3.3)  Then the final KNN-based decision is simplified  to majority voting along the decision trajectory  of the new sample.    4  Experiments    4.1 Experimental Data     All experimental data were extracted from  Chinese People Daily from January 1995 to  December 1996. Eight Chinese ambiguous words  were used in the experiments as listed in the first  column of Table1. In the Second column, we  give some information about samples used in  experiments. The number before each bracket is  the number of senses. Numbers in each bracket  are amounts of samples used for each sense.  They were annotated by a Chinese native speaker,  and checked by another native speaker. Some  samples without inter-agreement between two  native speakers had been excluded.  Only word co-occurrences in given windows  are used as features through all experiments in  this paper.  4.2  Experimental  Method    In order to do a comparative study, we have  implemented not only our algorithm, but also  four other related algorithms in our experiments.  They fall into two classes. NB (Manning and  Schutze 1999) and KNN (Ng and Lee 1996) are  two components of our approach. Locally  weighted NB(LWNB, Frank et al 2003) and  Ensemble NB(ENB Pedersen 2000) are two  combinational approaches. Since our aim is to  compare not only the performance but also the  robustness of these algorithms, we implemented  each algorithm in following way.  We note our approach TB_KNN when (3.2) is  used for final decision, and TB_VOTE when (3.3)  is used for final decision.   We firstly constructed a sequence of context  windows kp =( kl , kr ) 40,...,1=k  in  following way:   1. Initiate: 1,0 11 == rl   2. Generate next window:    ?? ? <=+= =+== ++ ++ kkkkkk kkkkkk rlifrrll rlifrrll 11 11 ,1 1,   39,...,1=k     We then constructed a sequence of window  trajectories.  ),...,( 1 ii ppT =    40,...,1=i   We implemented TB_KNN and TB_VOTE on  each trajectory from 1T  to 40T .   Obviously, our iT -based sense selection and  ip -based selection in fact make use of same  context surrounding the target word. ip  is the  biggest window along iT . We implemented NB  classifiers (noted by P) from 1p  to 40p .    KNN was implemented along the same  sequence of context window, from 1p  to 40p .   For the implementation of algorithm LWNB,  we used the measure in (Ng and Lee 1996) to find k  nearest neighbors for each sample, and then  constructed a NB classifier according to  (Frank2003). This algorithm was also implemented  for each context window along the sequence from  1p to 40p .  ENB was implemented according to (Petersen  2000). Different left and right window sizes we   used is (1,2,3,4,5,6,10,15,20). Since one  implementation of this algorithm make use of all  these different window sizes. It cannot be  implemented along above windows sequence, so  there is only one implementation for this     algorithm.  For each ambiguous word, we implemented  above experiments respectively, each experiment  was a 10-fold cross-validation, at each time 90%  of the data were used as training data, 5% were  used as development data, and other 5% were  used as test data.    4.3 Experimental Results    We give the results curves for word ??? in  Figure 2 and for word ??? in Figure 3. In both  figures, x-axis is the context window, from (0,1)  to (20,20), y-axis is F-measure, and different  marker style is for different algorithms. Results  curves for other six target words have similar  shapes.   We list a summary of results for all 8 words in  Table 1. In TB_KNN column, there are three values:  mean, maximum and standard variance of  F-measure of 40 different trajectories from 1T  to  40T . Results are summarized in the same way in  column TB_VOTE. For column P, KNN and  LWNB, three values are mean, maximum and  standard variance of F-measure of 40 different  points from 1p  to 40p . In column ENB, there is  only one F-measure.    5  Evaluation    5.1 Comparison with other algorithms    As we have mentioned, we compare results of  each algorithm on both performance and  robustness. Performance can be compared  directly from F-measure point-wise along a  sequence of context windows(or trajectories). We  also use mean and maximum (max) along the  sequence to give an overall comparison.  Robustness of an algorithm means that sense  decision varies gracefully with the change of  context windows (or trajectories) it uses.  Intuitionally, it can be reflected by a  context-performance curve, a flat curve is more  robust than a sharp one. We also use standard  variance (S.V.) along a sequence of sense  selection to give an overall comparison. A  sequence with small standard variance is more  robust than that with a big one.  From Figures 2 and 3, we can get an  intuitional impression that TB_KNN not only  achieve the best performance at most of points,  but also has the flattest curve shape. This means  TB_KNN outperforms other algorithm on both  performance and robustness. This can be detailed  in Table 1 by comparing mean/max/S.V. of  TB_KNN with their correspondences in other  algorithms.  Comparing values in the TB_KNN column  with their correspondences in column P, we can  find all values of TB_KNN are consistently  better thanthose in P. For ?mean? and ?max?, a  bigger one is better, while for S.V., a little one is  better. Comparing values in the TB_KNN  column with their correspondences in column  KNN, we can find nearly all values of TB_KNN  are better than those in KNN. (Except that  KNN?s max and S.V. for word ??? are better  then those in TB_KNN). All differences are  significant. This means our decision trajectory  based classifier is better than a NB classifier or a  KNN classifier. The combination takes  advantages of both NB and KNN methods. It  seems that KNN directly based on word  co-occurrence features suffers deeply from data  sparseness. While KNN based on decision  trajectory can alleviate the influence of data  sparseness. In our final KNN decision, sense  selection is also not sensitive to the number of  nearest neighbors.  Comparing values in TB_KNN column with  their correspondences in column LWNB, we can  find most of values in TB_KNN are better than  their correspondences in LWNB. But the  differences are not so bigger than those described  points in the trajectory FM ea su re 1.0 .9 .8 .7 ? TB_K TB_V P KNN LWNB Figure 2. context-performance curves for ???  points in the trajectory FM ea su re 1.0 .9 .8 .7 ? TB_K TB_V P KNN LWNB   Figure 3. context-performance curves for ???     Word Num-Sen TB_KNN TB_VOTE  P KNN LWNB ENB  ?  3(68,73,35) 0.88/0.91/0.03 0.86/0.89/0.03 0.83/0.88/0.06 0.86/0.91/0.04 0.87/0.92/0.04 0.89  ? 3(31,62,64) 0.95/0.98/0.04 0.94/0.96/0.05 0.90/0.95/0.05 0.83/0.91/0.05 0.90/0.96/0.04 0.96  ? 3(25,28,18) 0.89/0.94/0.03 0.88/0.94/0.04 0.80/0.90/0.10 0.69/0.82/0.07 0.76/0.87/0.07 0.82  ? 4(42,36,31,28) 0.80/0.84/0.04 0.79/0.83/0.04 0.74/0.83/0.06 0.75/0.81/0.05 0.74/0.80/ 0.04 0.79  ? 2(24,33) 0.93/ 0.97/0.03 0.92/0.97/0.04 0.88/0.95/0.05 0.85/0.92/0.05 0.91/ 0.97/0.04 0.89  ? 2(40,36) 0.91/0.96/0.06 0.89/0.94/0.07 0.85/0.96/0.18 0.89/0.97/0.06 0.87/0.97/0.06 0.84  ? 2(43,52) 0.86/0.89/0.03 0.84/0.87/0.04 0.83/0.88/0.04 0.73/0.80/0.03 0.82/0.88/0.04 0.89  ? 2(15,15) 0.83/ 0.92/0.05 0.82/ 0.89/0.05 0.77/0.87/0.07 0.49/0.82/0.13 0.79/0.89/0.06 0.77  Table 1 result summary    in above paragraph, especially when the number  of training samples is relatively big. In Frank et  al.(2003), the number of training samples is  large.(Most of them are more than several  hundreds.) They used 50 local training samples  to construct a NB classifier. It is always  impossible in our experiments and in most WSD  tasks.    Although not all of the values of mean in  TB_KNN column are bigger than their  correspondences in ENB, all maximums are  bigger (or equal) than those in ENB. Comparing  with ENB, We think the trajectory based approach  may make use of NB decisions in a more  systematical way than selecting some classifiers  for voting in ENB, and also, our approach receives  benefits from the final KNN decision, which can  make some exceptions under consideration.  Let us give a discussion on how our  trajectory-based approach makes use of  information incontext.  Firstly, although each NB classifier use  bag-of-words as its features, because window  size for NB classifiers is extended sequentially,  the decision trajectory thus reflects influences  brought by context words in different positions.  That is to say, changing the position of a  co-occurrence word in a sentence might cause  different final decision in trajectory-based  approach. While in point-based approach, as  long as the co-occurrence word is in the context  window, a classifier based on bag-of-words  features always makes the same selection no  matter how to change the position of that word.  From this view, the trajectory-based approach in  fact makes use of position information of words  in context.  Secondly, because of its implicit utilization of  position information of context words, it may  make use of information from some decisions  locally correct but globally wrong. For example,  we consider sentence S1 in section 2 again.     S1:?/ ?/ ?/?/ ?/    ??/  ?/  ?/.  (I) (think) (this) (book) (worthy) (a)  (read)    On the one hand, as we have said, when we  use context window (3,3), we select the sense of  ?read? for ?. Although it is a wrong sense  selection for this word in this sentence (when  context window is (6,6)), it is a correct selection  for the local collocation (when ? collocates  with ?, its sense is ?read?). By saving this  information, we cannot only make use of  information of sense selection for the sentence,  but also information for this collocation. In other  words, the sentence S1 gives us two samples for  different senses of the target word.   On the other hand, that a polysemous word  changes their probability for different sense with  the change of context window is one type of  pattern for sense ambiguity, the trajectory based  approach seems an efficient way to grasp this  pattern of ambiguity.    5.2 Trajectory    In TB_KNN, we need to calculate a sense  decision trajectory for each training sample, not  all of these trajectories are eigen-trajecories. In  TB_VOTE, we don?t calculate sense decision  trajectories for training samples, all training  decision trajectories are regarded as eigen-  trajectory, final decision for a new sample  reduces to majority voting along the trajectory.  Comparing TB_KNN and TB_VOTE, we can  find that both performance and robustness of  TB_VOTE fall. This means existence of  non-eigen-trajectory is in fact helpful, which can  make some exceptions under consideration by  using KNN.     In above experiments, we generated a  trajectory by adding one context word each time.  We further explored if a looser trajectory can get  the same performance. We first excluded even  points in original trajectories in above     experiments to get somenew trajectories. For  example, by excluding even points of the  trajectory },...,{ 40140 ppT = , we got:    20,...,1},,..,,...,{ 3912120 \' == ? kpppT k    Note this 20\'T  is different from 20T  in above  experiments, where 20T  is:   20,...,1},,..,,...,{ 20120 == kpppT k    In this way, we got 20 different trajectories  TG2: 20\'1\' ,...,TT , jT \'  includes half number  of points comparing with its correspondence  jT2  in above experiments. The longest  trajectory includes 20 points. We repeated above  TB_KNN experiment along these new  trajectories. Results are listed in column  TB_KNN TG2 in Table2. We excluded even  points to generate TG3 and TG4 which include  at most 10 and 5 points respectively in their  trajectories. We also repeated same TB_KNN  experiment on TG3 and TG4.      TB_KNN TG2 TB_KNN TG3 TB_KNN TG4 ? 0.87/0.90/0.03 0.87/0.91/0.03 0.86/0.89/0.03  ? 0.95/0.97/0.01 0.95/0.96/0.01 0.94/0.95/0.02  ? 0.90/0.93/0.02 0.90/0.91/0.02 0.90/0.93/0.03  ? 0.80/0.84/0.02 0.78/0.82/0.03 0.77/0.81/0.02  ? 0.93/0.97/0.03 0.93/0.95/0.02 0.92/0.96/0.03  ? 0.91/0.94/0.06 0.91/0.93/0.06 0.90/0.94/0.09  ? 0.86/0.90/0.03 0.86/0.90/0.04 0.82/0.85/0.03  ? 0.83/0.92/0.05 0.85/0.92/0.05 0.82/0.92/0.07  Table 2: shorter length in the trajectory   From Table 2, we can find that performance  of classifiers using trajectories with small  number of points do not decrease significantly.  That is to say, a shorter trajectory can also  achieve good performance.     6  conclusions    This paper presents a new type of classifier  combination method. We firstly construct a  sequence of NB classifiers along orderly varying  sized windows of context, and get a trajectory of  sense selection for each sample, then use the  sense trajectory based KNN to make final  decision for test samples. Experiments show that  our approach outperforms some other algorithms  on both robustness and performance.  We will do further investigations on the  trajectory to see if there exists some skeletal  points like quantum numbers in the  wavefunction in Quantum Theory. "'), ('text2', b"b'1 Introduction Synchronous context-free grammar (SCFG) is widely used for machine translation. There are many different ways to extract SCFGs from data. Hiero (Chiang, 2005) represents a more restricted form of SCFG, while GHKM (Galley et al, 2004) uses a general form of SCFG. In this paper, we discuss some of the practical issues that arise from decoding general SCFGs that are seldom discussed in the literature. We focus on parsing grammars extracted using the method put forth by Galley et al (2004), but the solutions to these issues are applicable to other general forms of SCFG with many nonterminals. The GHKM grammar extraction method produces a large number of unary rules. Unary rules are the rules that have exactly one nonterminal and no terminals on the source side. They may be problematic for decoders since they may create cycles, which are unary production chains that contain duplicated dynamic programming states. In later sections, we discuss why unary rules are problematic and investigate two possible solutions. GHKM grammars often have rules with many right-hand-side nonterminals and require binarization to ensure O(n3) time parsing. However, binarization creates a large number of virtual nonterminals. We discuss the challenges of, and possible solutions to, issues arising from having a large number of virtual nonterminals. We also compare binarizing the grammar with filtering rules according to scope, a concept introduced by Hopkins and Langmead (2010). By explicitly considering the effect of anchoring terminals on input sentences, scope3 rules encompass a much larger set of rules than Chomsky normal form but they can still be parsed in O(n3) time. Unlike phrase-based machine translation, GHKM grammars are less flexible in how they can segment sentence pairs into phrases because they are restricted not only by alignments between words in sentence pairs, but also by target-side parse trees. In general, GHKM grammars suffer more from data sparsity than phrasal rules. To alleviate this issue, we discuss adding glue rules and phrases extracted using methods commonly used in phrase-based machine translation. 2 Handling unary rules Unary rules are common in GHKM grammars. We observed that as many as 10% of the rules extracted from a Chinese-English parallel corpus are unary. Some unary rules are the result of alignment errors, but other ones might be useful. For example, Chinese lacks determiners, and English determiners usually remain unaligned to any Chinese words. Extracted grammars include rules that reflect this fact: NP ? NP, the NP NP ? NP, a NP 413 However, unary rules can be problematic: ? Unary production cycles corrupt the translation hypergraph generated by the decoder. A hypergraph containing a unary cycle cannot be topologically sorted. Many algorithms for parameter tuning and coarse-to-fine decoding, such as the inside-outside algorithm and cube-pruning, cannot be run in the presence of unary cycles. ? The existence of many unary rules of the form ?NP ? NP, the NP? quickly fills a pruning bin with guesses of English words to insert without any source-side lexical evidence. The most obvious way of eliminating problematic unary rules would be converting grammars into Chomsky normal form.However, this may result in bloated grammars. In this section, we present two different ways to handle unary rules. The first involves modifying the grammar extraction method, and the second involves modifying the decoder. 2.1 Modifying grammar extraction We can modify the grammar extraction method such that it does not extract any unary rules. Galley et al (2004) extracts rules by segmenting the target-side parse parse tree based on frontier nodes. We modify the definition of a frontier node in the following way. We label frontier nodes in the English parse tree, and examine the Chinese span each frontier node covers. If a frontier node covers the same span as the frontier node that immediately dominates it, then the dominated node is no longer considered a frontier. This modification prevents unary rules from being extracted. Figure 1 shows an example of an English-Chinese sentence pair with the English side automatically parsed. Frontier nodes in the tree in the original GHKM rule extraction method are marked with a box. With the modification, only the top boldfaced NP would be considered a frontier node. The GHKM rule extraction results in the following rules: NPB ????, the snowy egret NP ? NPB, NPB PP ? NP, with NP NP ? PP, romance PP With the change, only the following rule is extracted: NP NPB NNP romance PP IN with NP NPB DT the JJ snowy NN egret ?? ? ? ? Figure 1: A sentence fragment pair with erroneous alignment and tokenization NP ????, romance with the snowy egret We examine the effect of this modification has on translation performance in Section 5. 2.2 Modifying the decoder Modifying how grammars are extracted has an obvious down side, i.e., the loss of generality. In the previous example, the modification results in a bad rule, which is the result of bad alignments. Before the modification, the rule set includes a good rule: NPB ????, the snowy egret which can be applied at test time. Because of this, one may still want to decode with all available unary rules. We handle unary rules inside the decoder in the following ways: ? Unary cycle detection The na?ve way to detect unary cycles is backtracking on a unary chain to see if a newly generated item has been generated before. The running time of this is constrained only by the number of possible items in a chart span. In practice, however, this is often not a problem: if all unary derivations have positive costs and a priority queue is used to expand unary derivations, 414 only the best K unary items will be generated, where K is the pruning constant. ? Ban negative cost unary rules When tuning feature weights, an optimizer may try feature weights that may give negative costs to unary productions. This causes unary derivations to go on forever. The solution is to set a maximum length for unary chains, or to ban negative unary productions outright. 3 Issues with binarization 3.1 Filtering and binarization Synchronous binarization (Zhang et al, 2006) is an effective method to reduceSCFG parsing complexity and allow early language model integration. However, it creates virtual nonterminals which require special attention at parsing time. Alternatively, we can filter rules that have more than scope-3 to parse in O(n3) time with unbinarized rules. This requires Earley (Earley, 1970) style parsing, which does implicit binarization at decoding time. Scopefiltering may filter out unnecessarily long rules that may never be applied, but it may also throw out rules with useful contextual information. In addition, scope-filtering does not accommodate early language model state integration. We compare the two with an experiment. For the rest of the section, we discuss issues created by virtual nonterminals. 3.2 Handling virtual nonterminals One aspect of grammar binarization that is rarely mentioned is how to assign probabilities to binarized grammar rules. The na?ve solution is to assign probability one to any rule whose left-hand side is a virtual nonterminal. This maintains the original model. However, it is generally not fair to put chart items of virtual nonterminals and those of regular nonterminals in the same bin, because virtual items have artificially low costs. One possible solution is adding a heuristic to push up the cost of virtual items for fair comparison. For our experiments, we use an outside estimate as a heuristic for a virtual item. Consider the following rule binarization (only the source side shown): A ? BCD : ? log(p) ? V ? BC : 0A ? VD : ? log(p) A ? BCD is the orginal rule and ? log(p) is the cost of the rule. In decoding time, when a chart item is generated from the binarized rule V ? BC, we add ? log(p) to its total cost as an optimistic estimate of the cost to build the original unbinarized rule. The heuristic is used only for pruning purposes, and it does not change the real cost. The idea is similar to A* parsing (Klein and Manning, 2003). One complication is that a binarized rule can arise from multiple different unbinarized rules. In this case, we pick the lowest cost among the unbinarized rules as the heuristic. Another approach for handling virtual nonterminals would be giving virtual items separate bins and avoiding pruning them at all. This is usually not practical for GHKM grammars, because of the large number of nonterminals. 4 Adding flexibility 4.1 Glue rules Because of data sparsity, an SCFG extracted from data may fail to parse sentences at test time. For example, consider the following rules: NP ? JJ NN, JJ NN JJ ? c1, e1 JJ ? c2, e2 NN ? c3, e3 This set of rules is able to parse the word sequence c1 c3 and c2 c3 but not c1 c2 c3, if we have not seen ?NP ? JJ JJ NN? at training time. Because SCFGs neither model adjunction, nor are they markovized, with a small amount of data, such problems can occur. Therefore, we may opt to add glue rules as used in Hiero (Chiang, 2005): S ? C, C S ? S C, S C where S is thegoal state and C is the glue nonterminal that can produce any nonterminals. We refer to these glue rules as the monotonic glue rules. We rely on GHKM rules for reordering when we use the monotonic glue rules. However, we can also allow glue rules to reorder constituents. Wu (1997) presents a better-constrained grammar designed to only produce tail-recursive parses. See Table 1 for the complete set of rules. We refer to these rules as ABC glue rules. These rules always generate left415 S ? A A ? [A B] B ? ? B A ? S ? B A ? [B B] B ? ? A A ? S ? C A ? [C B] B ? ? C A ? A ? [A C] B ? ? B C ? A ? [B C] B ? ? A C ? A ? [C C] B ? ? C C ? Table 1: The ABC Grammar. We follow the convention of Wu (1997) that square brackets stand for straight rules and angle brackets stand for inverted rules. heavy derivations, weeding out ambiguity and making search more efficient. We learn probabilities of ABC glue rules by using expectation maximization (Dempster et al, 1977) to train a word-level Inversion Transduction Grammar from data. In our experiments, depending on the configuration, the decoder failed to parse about 5% of sentences without glue rules, which illustrates their necessity. Although it is reasonable to believe that reordering should always have evidence in data, as with GHKM rules, we may wish to reorder based on evidence from the language model. In our experiments, we compare the ABC glue rules with the monotonic glue rules. 4.2 Adding phrases GHKM grammars are more restricted than the phrase extraction methods used in phrase-based models, since, in GHKM grammar extraction, phrase segmentation is constrained by parse trees. This may be a good thing, but it suffers from loss of flexibility, and it also cannot use non-constituent phrases. We use the method of Koehn et al (2003) to extract phrases, and, for each phrase, we add a rule with the glue nonterminal as the left-hand side and the phrase pair as the right-hand side. We experiment to see whether adding phrases is beneficial. There have been other efforts to extend GHKM grammar to allow more flexible rule extraction. Galley et al (2006) introduce composed rules where minimal GHKM rules are fused to form larger rules. Zollmann and Venugopal (2006) introduce a model that allows more generalized rules to be extracted. BLEU Baseline + monotonic glue rules 20.99 No-unary + monotonic glue rules 23.83 No-unary + ABC glue rules 23.94 No-unary (scope-filtered) + monotonic 23.99 No-unary (scope-filtered) + ABC glue rules 24.09 No-unary + ABC glue rules + phrases 23.43 Table 2: BLEU score results for Chinese-English with different settings 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. Terminal-aware synchronous binarization (Fang et al, 2011) was appliedto all GHKM grammars that are not scopefiltered. MERT (Och, 2003) was used to tune parameters. We used a 392-sentence development set with four references for parameter tuning, and a 428sentence test set with four references for testing. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). For the experiment that incorporated phrases, the phrase pairs were extracted from the same corpus with the same set of alignments. We have limited the maximum size of phrases to be four. 5.2 Results Our result is summarized in Table 2. The baseline GHKM grammar with monotonic glue rules yielded a worse result than the no-unary grammar with the same glue rules. The difference is statistically significant at p < 0.05 based on 1000 iterations of paired bootstrap resampling (Koehn, 2004). Compared to using monotonic glue rules, using ABC glue rules brought slight improvements for both the no-unary setting and the scope-filtered setting, but the differences are not statistically significant. In terms of decoding speed and memory usage, using ABC glues and monotonic glue rules were virtually identical. The fact that glue rules are seldom used at decoding time may account for why there is 416 little difference in using monotonic glue rules and using ABC glue rules. Out of all the rules that were applied to decoding our test set, less than one percent were glue rules, and among the glue rules, straight glue rules outnumbered inverted ones by three to one. Compared with binarized no-unary rules, scope3 filtered no-unary rules retained 87% of the rules but still managed to have slightly better BLEU score. However, the score difference is not statistically significant. Because the size of the grammar is smaller, compared to using no-unary grammar, it used less memory at decoding time. However, decoding speed was somewhat slower. This is because the decoder employs Early-style dotted rules to handle unbinarized rules, and in order to decode with scope-3 rules, the decoder needs to build dotted items, which are not pruned until a rule is completely matched, thus leading to slower decoding. Adding phrases made the translation result slightly worse. The difference is not statistically significant. There are two possible explanations for this. Since there were more features to tune, MERT may have not done a good job. We believe the more important reason is that once a phrase is used, only glue rules can be used to continue the derivation, thereby losing the richer information offered by GHKM grammar.6 Conclusion In this paper, we discussed several issues concerning decoding with synchronous context-free grammars, focusing on grammars resulting from the GHKM extraction method. We discussed different ways to handle cycles. We presented a modified grammar extraction scheme that eliminates unary rules. We also presented a way to decode with unary rules in the grammar, and examined several different issues resulting from binarizing SCFGs. We finally discussed adding flexibility to SCFGs by adding glue rules and phrases. Acknowledgments We would like to thank the anonymous reviewers for their helpful comments. This work was supported by NSF grants IIS0546554 and IIS-0910611.'")])
INFO:tensorflow:Finished getting dataset.
I0712 13:42:02.073061 140545052657472 input_pipeline.py:91] Finished getting dataset.
I0712 13:42:02.073242 140545052657472 input_pipeline.py:94] Using char-level/byte dataset..
I0712 13:42:02.213785 140545052657472 train.py:106] Vocab Size: 257
I0712 13:42:15.929603 140545052657472 checkpoints.py:242] Found no checkpoint directory at trained_models/matching/performer
I0712 13:42:16.564475 140545052657472 train_utils.py:370] Starting training
I0712 13:42:16.564717 140545052657472 train_utils.py:371] ====================
I0712 13:42:44.387224 140545052657472 train_utils.py:377] train in step: 0
I0712 13:42:44.416938 140545052657472 train_utils.py:377] train in step: 1
I0712 13:42:44.447370 140545052657472 train_utils.py:377] train in step: 2
I0712 13:42:44.472390 140545052657472 train_utils.py:377] train in step: 3
I0712 13:42:44.494996 140545052657472 train_utils.py:377] train in step: 4
I0712 13:42:44.517649 140545052657472 train_utils.py:377] train in step: 5
I0712 13:42:44.539847 140545052657472 train_utils.py:377] train in step: 6
I0712 13:42:44.565808 140545052657472 train_utils.py:377] train in step: 7
I0712 13:42:44.599561 140545052657472 train_utils.py:377] train in step: 8
I0712 13:42:44.637435 140545052657472 train_utils.py:377] train in step: 9
I0712 13:42:44.662404 140545052657472 train_utils.py:377] train in step: 10
I0712 13:42:44.689657 140545052657472 train_utils.py:377] train in step: 11
I0712 13:42:44.717637 140545052657472 train_utils.py:377] train in step: 12
I0712 13:42:44.749991 140545052657472 train_utils.py:377] train in step: 13
I0712 13:42:44.781259 140545052657472 train_utils.py:377] train in step: 14
I0712 13:42:44.812475 140545052657472 train_utils.py:377] train in step: 15
I0712 13:42:44.840141 140545052657472 train_utils.py:377] train in step: 16
I0712 13:42:44.870106 140545052657472 train_utils.py:377] train in step: 17
I0712 13:42:44.897259 140545052657472 train_utils.py:377] train in step: 18
I0712 13:42:44.929469 140545052657472 train_utils.py:377] train in step: 19
I0712 13:42:44.958375 140545052657472 train_utils.py:377] train in step: 20
I0712 13:42:44.986564 140545052657472 train_utils.py:377] train in step: 21
I0712 13:42:45.016512 140545052657472 train_utils.py:377] train in step: 22
I0712 13:42:45.044667 140545052657472 train_utils.py:377] train in step: 23
I0712 13:42:45.074901 140545052657472 train_utils.py:377] train in step: 24
I0712 13:42:45.103796 140545052657472 train_utils.py:377] train in step: 25
I0712 13:42:45.132612 140545052657472 train_utils.py:377] train in step: 26
I0712 13:42:45.161262 140545052657472 train_utils.py:377] train in step: 27
I0712 13:42:45.191317 140545052657472 train_utils.py:377] train in step: 28
I0712 13:42:45.219629 140545052657472 train_utils.py:377] train in step: 29
I0712 13:42:45.248404 140545052657472 train_utils.py:377] train in step: 30
I0712 13:42:45.277478 140545052657472 train_utils.py:377] train in step: 31
I0712 13:42:45.306908 140545052657472 train_utils.py:377] train in step: 32
I0712 13:42:45.339193 140545052657472 train_utils.py:377] train in step: 33
I0712 13:42:45.368472 140545052657472 train_utils.py:377] train in step: 34
I0712 13:42:45.401817 140545052657472 train_utils.py:377] train in step: 35
I0712 13:42:45.432440 140545052657472 train_utils.py:377] train in step: 36
I0712 13:42:45.464002 140545052657472 train_utils.py:377] train in step: 37
I0712 13:42:45.493328 140545052657472 train_utils.py:377] train in step: 38
I0712 13:42:45.521515 140545052657472 train_utils.py:377] train in step: 39
I0712 13:42:45.554525 140545052657472 train_utils.py:377] train in step: 40
I0712 13:42:45.581906 140545052657472 train_utils.py:377] train in step: 41
I0712 13:42:45.610418 140545052657472 train_utils.py:377] train in step: 42
I0712 13:42:45.640695 140545052657472 train_utils.py:377] train in step: 43
I0712 13:42:45.670090 140545052657472 train_utils.py:377] train in step: 44
I0712 13:42:45.699511 140545052657472 train_utils.py:377] train in step: 45
I0712 13:42:45.728525 140545052657472 train_utils.py:377] train in step: 46
I0712 13:42:45.758414 140545052657472 train_utils.py:377] train in step: 47
I0712 13:42:45.787971 140545052657472 train_utils.py:377] train in step: 48
I0712 13:42:45.817259 140545052657472 train_utils.py:377] train in step: 49
I0712 13:42:45.847106 140545052657472 train_utils.py:377] train in step: 50
I0712 13:42:45.876270 140545052657472 train_utils.py:377] train in step: 51
I0712 13:42:45.907435 140545052657472 train_utils.py:377] train in step: 52
I0712 13:42:45.937827 140545052657472 train_utils.py:377] train in step: 53
I0712 13:42:45.970242 140545052657472 train_utils.py:377] train in step: 54
I0712 13:42:45.996968 140545052657472 train_utils.py:377] train in step: 55
I0712 13:42:46.026926 140545052657472 train_utils.py:377] train in step: 56
I0712 13:42:46.057178 140545052657472 train_utils.py:377] train in step: 57
I0712 13:42:46.085707 140545052657472 train_utils.py:377] train in step: 58
I0712 13:42:46.121192 140545052657472 train_utils.py:377] train in step: 59
I0712 13:42:46.151861 140545052657472 train_utils.py:377] train in step: 60
I0712 13:42:46.178003 140545052657472 train_utils.py:377] train in step: 61
I0712 13:42:46.207698 140545052657472 train_utils.py:377] train in step: 62
I0712 13:42:46.237065 140545052657472 train_utils.py:377] train in step: 63
I0712 13:42:46.266587 140545052657472 train_utils.py:377] train in step: 64
I0712 13:42:46.297565 140545052657472 train_utils.py:377] train in step: 65
I0712 13:42:46.326766 140545052657472 train_utils.py:377] train in step: 66
I0712 13:42:46.357199 140545052657472 train_utils.py:377] train in step: 67
I0712 13:42:46.386639 140545052657472 train_utils.py:377] train in step: 68
I0712 13:42:46.418167 140545052657472 train_utils.py:377] train in step: 69
I0712 13:42:46.446861 140545052657472 train_utils.py:377] train in step: 70
I0712 13:42:46.478045 140545052657472 train_utils.py:377] train in step: 71
I0712 13:42:46.507357 140545052657472 train_utils.py:377] train in step: 72
I0712 13:42:46.542003 140545052657472 train_utils.py:377] train in step: 73
I0712 13:42:46.570607 140545052657472 train_utils.py:377] train in step: 74
I0712 13:42:46.603311 140545052657472 train_utils.py:377] train in step: 75
I0712 13:42:46.633141 140545052657472 train_utils.py:377] train in step: 76
I0712 13:42:46.662145 140545052657472 train_utils.py:377] train in step: 77
I0712 13:42:46.692417 140545052657472 train_utils.py:377] train in step: 78
I0712 13:42:46.737948 140545052657472 train_utils.py:377] train in step: 79
I0712 13:42:46.768953 140545052657472 train_utils.py:377] train in step: 80
I0712 13:42:46.798497 140545052657472 train_utils.py:377] train in step: 81
I0712 13:42:46.827415 140545052657472 train_utils.py:377] train in step: 82
I0712 13:42:46.859350 140545052657472 train_utils.py:377] train in step: 83
I0712 13:42:46.888504 140545052657472 train_utils.py:377] train in step: 84
I0712 13:42:46.920065 140545052657472 train_utils.py:377] train in step: 85
I0712 13:42:46.948734 140545052657472 train_utils.py:377] train in step: 86
I0712 13:42:46.978409 140545052657472 train_utils.py:377] train in step: 87
I0712 13:42:47.007071 140545052657472 train_utils.py:377] train in step: 88
I0712 13:42:47.036577 140545052657472 train_utils.py:377] train in step: 89
I0712 13:42:47.065837 140545052657472 train_utils.py:377] train in step: 90
I0712 13:42:47.096638 140545052657472 train_utils.py:377] train in step: 91
I0712 13:42:47.126462 140545052657472 train_utils.py:377] train in step: 92
I0712 13:42:47.159149 140545052657472 train_utils.py:377] train in step: 93
I0712 13:42:47.187482 140545052657472 train_utils.py:377] train in step: 94
I0712 13:42:47.216981 140545052657472 train_utils.py:377] train in step: 95
I0712 13:42:47.246127 140545052657472 train_utils.py:377] train in step: 96
I0712 13:42:47.276789 140545052657472 train_utils.py:377] train in step: 97
I0712 13:42:47.306329 140545052657472 train_utils.py:377] train in step: 98
I0712 13:42:47.337260 140545052657472 train_utils.py:377] train in step: 99
I0712 13:42:47.367143 140545052657472 train_utils.py:377] train in step: 100
I0712 13:42:47.398064 140545052657472 train_utils.py:377] train in step: 101
I0712 13:42:47.429236 140545052657472 train_utils.py:377] train in step: 102
I0712 13:42:47.458196 140545052657472 train_utils.py:377] train in step: 103
I0712 13:42:47.487171 140545052657472 train_utils.py:377] train in step: 104
I0712 13:42:47.520614 140545052657472 train_utils.py:377] train in step: 105
I0712 13:42:47.547514 140545052657472 train_utils.py:377] train in step: 106
I0712 13:42:47.577555 140545052657472 train_utils.py:377] train in step: 107
I0712 13:42:47.606437 140545052657472 train_utils.py:377] train in step: 108
I0712 13:42:47.635946 140545052657472 train_utils.py:377] train in step: 109
I0712 13:42:47.665543 140545052657472 train_utils.py:377] train in step: 110
I0712 13:42:47.695341 140545052657472 train_utils.py:377] train in step: 111
I0712 13:42:47.729465 140545052657472 train_utils.py:377] train in step: 112
I0712 13:42:47.758594 140545052657472 train_utils.py:377] train in step: 113
I0712 13:42:47.788213 140545052657472 train_utils.py:377] train in step: 114
I0712 13:42:47.818722 140545052657472 train_utils.py:377] train in step: 115
I0712 13:42:47.848368 140545052657472 train_utils.py:377] train in step: 116
I0712 13:42:47.879959 140545052657472 train_utils.py:377] train in step: 117
I0712 13:42:47.907688 140545052657472 train_utils.py:377] train in step: 118
I0712 13:42:47.940194 140545052657472 train_utils.py:377] train in step: 119
I0712 13:42:47.968461 140545052657472 train_utils.py:377] train in step: 120
I0712 13:42:48.001241 140545052657472 train_utils.py:377] train in step: 121
I0712 13:42:48.032953 140545052657472 train_utils.py:377] train in step: 122
I0712 13:42:48.063169 140545052657472 train_utils.py:377] train in step: 123
I0712 13:42:48.093242 140545052657472 train_utils.py:377] train in step: 124
I0712 13:42:48.122091 140545052657472 train_utils.py:377] train in step: 125
I0712 13:42:48.151529 140545052657472 train_utils.py:377] train in step: 126
I0712 13:42:48.185525 140545052657472 train_utils.py:377] train in step: 127
I0712 13:42:48.211248 140545052657472 train_utils.py:377] train in step: 128
I0712 13:42:48.241907 140545052657472 train_utils.py:377] train in step: 129
I0712 13:42:48.271508 140545052657472 train_utils.py:377] train in step: 130
I0712 13:42:48.305153 140545052657472 train_utils.py:377] train in step: 131
I0712 13:42:48.332595 140545052657472 train_utils.py:377] train in step: 132
I0712 13:42:48.366580 140545052657472 train_utils.py:377] train in step: 133
I0712 13:42:48.393949 140545052657472 train_utils.py:377] train in step: 134
I0712 13:42:48.425018 140545052657472 train_utils.py:377] train in step: 135
I0712 13:42:48.454740 140545052657472 train_utils.py:377] train in step: 136
I0712 13:42:48.484079 140545052657472 train_utils.py:377] train in step: 137
I0712 13:42:48.515206 140545052657472 train_utils.py:377] train in step: 138
I0712 13:42:48.544839 140545052657472 train_utils.py:377] train in step: 139
I0712 13:42:48.575808 140545052657472 train_utils.py:377] train in step: 140
I0712 13:42:48.606827 140545052657472 train_utils.py:377] train in step: 141
I0712 13:42:48.636547 140545052657472 train_utils.py:377] train in step: 142
I0712 13:42:48.668323 140545052657472 train_utils.py:377] train in step: 143
I0712 13:42:48.700475 140545052657472 train_utils.py:377] train in step: 144
I0712 13:42:48.733803 140545052657472 train_utils.py:377] train in step: 145
I0712 13:42:48.765517 140545052657472 train_utils.py:377] train in step: 146
I0712 13:42:48.794690 140545052657472 train_utils.py:377] train in step: 147
I0712 13:42:48.825561 140545052657472 train_utils.py:377] train in step: 148
I0712 13:42:48.856213 140545052657472 train_utils.py:377] train in step: 149
I0712 13:42:48.887410 140545052657472 train_utils.py:377] train in step: 150
I0712 13:42:48.917719 140545052657472 train_utils.py:377] train in step: 151
I0712 13:42:48.948277 140545052657472 train_utils.py:377] train in step: 152
I0712 13:42:48.978641 140545052657472 train_utils.py:377] train in step: 153
I0712 13:42:49.009569 140545052657472 train_utils.py:377] train in step: 154
I0712 13:42:49.039965 140545052657472 train_utils.py:377] train in step: 155
I0712 13:42:49.069955 140545052657472 train_utils.py:377] train in step: 156
I0712 13:42:49.100834 140545052657472 train_utils.py:377] train in step: 157
I0712 13:42:49.131420 140545052657472 train_utils.py:377] train in step: 158
I0712 13:42:49.163483 140545052657472 train_utils.py:377] train in step: 159
I0712 13:42:49.193268 140545052657472 train_utils.py:377] train in step: 160
I0712 13:42:49.229584 140545052657472 train_utils.py:377] train in step: 161
I0712 13:42:49.258608 140545052657472 train_utils.py:377] train in step: 162
I0712 13:42:49.289007 140545052657472 train_utils.py:377] train in step: 163
I0712 13:42:49.319845 140545052657472 train_utils.py:377] train in step: 164
I0712 13:42:49.351725 140545052657472 train_utils.py:377] train in step: 165
I0712 13:42:49.384001 140545052657472 train_utils.py:377] train in step: 166
I0712 13:42:49.418920 140545052657472 train_utils.py:377] train in step: 167
I0712 13:42:49.446766 140545052657472 train_utils.py:377] train in step: 168
I0712 13:42:49.475826 140545052657472 train_utils.py:377] train in step: 169
I0712 13:42:49.508153 140545052657472 train_utils.py:377] train in step: 170
I0712 13:42:49.536770 140545052657472 train_utils.py:377] train in step: 171
I0712 13:42:49.569685 140545052657472 train_utils.py:377] train in step: 172
I0712 13:42:49.597455 140545052657472 train_utils.py:377] train in step: 173
I0712 13:42:49.632431 140545052657472 train_utils.py:377] train in step: 174
I0712 13:42:49.660437 140545052657472 train_utils.py:377] train in step: 175
I0712 13:42:49.690827 140545052657472 train_utils.py:377] train in step: 176
I0712 13:42:49.721083 140545052657472 train_utils.py:377] train in step: 177
I0712 13:42:49.751397 140545052657472 train_utils.py:377] train in step: 178
I0712 13:42:49.782203 140545052657472 train_utils.py:377] train in step: 179
I0712 13:42:49.812872 140545052657472 train_utils.py:377] train in step: 180
I0712 13:42:49.843865 140545052657472 train_utils.py:377] train in step: 181
I0712 13:42:49.873581 140545052657472 train_utils.py:377] train in step: 182
I0712 13:42:49.905816 140545052657472 train_utils.py:377] train in step: 183
I0712 13:42:49.936169 140545052657472 train_utils.py:377] train in step: 184
I0712 13:42:49.967738 140545052657472 train_utils.py:377] train in step: 185
I0712 13:42:49.998814 140545052657472 train_utils.py:377] train in step: 186
I0712 13:42:50.031624 140545052657472 train_utils.py:377] train in step: 187
I0712 13:42:50.061750 140545052657472 train_utils.py:377] train in step: 188
I0712 13:42:50.093481 140545052657472 train_utils.py:377] train in step: 189
I0712 13:42:50.135076 140545052657472 train_utils.py:377] train in step: 190
I0712 13:42:50.161687 140545052657472 train_utils.py:377] train in step: 191
I0712 13:42:50.191946 140545052657472 train_utils.py:377] train in step: 192
I0712 13:42:50.222737 140545052657472 train_utils.py:377] train in step: 193
I0712 13:42:50.257577 140545052657472 train_utils.py:377] train in step: 194
I0712 13:42:50.284317 140545052657472 train_utils.py:377] train in step: 195
I0712 13:42:50.315570 140545052657472 train_utils.py:377] train in step: 196
I0712 13:42:50.345030 140545052657472 train_utils.py:377] train in step: 197
I0712 13:42:50.376743 140545052657472 train_utils.py:377] train in step: 198
I0712 13:42:50.406595 140545052657472 train_utils.py:377] train in step: 199
I0712 13:42:50.437026 140545052657472 train_utils.py:377] train in step: 200
I0712 13:42:51.136471 140545052657472 train_utils.py:396] train in step: 200, loss: 0.7288999557495117, acc: 0.5123999714851379
I0712 13:42:58.011354 140545052657472 train_utils.py:411] eval in step: 200, loss: 0.7021, acc: 0.5100
I0712 13:42:58.014570 140545052657472 train_utils.py:421] Testing...
I0712 13:43:01.293905 140545052657472 train_utils.py:424] test in step: 200, loss: 0.7169, acc: 0.4500
I0712 13:43:01.324883 140545052657472 train_utils.py:377] train in step: 201
I0712 13:43:01.347528 140545052657472 train_utils.py:377] train in step: 202
I0712 13:43:01.368600 140545052657472 train_utils.py:377] train in step: 203
I0712 13:43:01.390707 140545052657472 train_utils.py:377] train in step: 204
I0712 13:43:01.413548 140545052657472 train_utils.py:377] train in step: 205
I0712 13:43:01.443440 140545052657472 train_utils.py:377] train in step: 206
I0712 13:43:01.481386 140545052657472 train_utils.py:377] train in step: 207
I0712 13:43:01.513225 140545052657472 train_utils.py:377] train in step: 208
I0712 13:43:01.543235 140545052657472 train_utils.py:377] train in step: 209
I0712 13:43:01.573443 140545052657472 train_utils.py:377] train in step: 210
I0712 13:43:01.604668 140545052657472 train_utils.py:377] train in step: 211
I0712 13:43:01.636235 140545052657472 train_utils.py:377] train in step: 212
I0712 13:43:01.666116 140545052657472 train_utils.py:377] train in step: 213
I0712 13:43:01.698190 140545052657472 train_utils.py:377] train in step: 214
I0712 13:43:01.729245 140545052657472 train_utils.py:377] train in step: 215
I0712 13:43:01.760182 140545052657472 train_utils.py:377] train in step: 216
I0712 13:43:01.791757 140545052657472 train_utils.py:377] train in step: 217
I0712 13:43:01.829200 140545052657472 train_utils.py:377] train in step: 218
I0712 13:43:01.860700 140545052657472 train_utils.py:377] train in step: 219
I0712 13:43:01.891441 140545052657472 train_utils.py:377] train in step: 220
I0712 13:43:01.920958 140545052657472 train_utils.py:377] train in step: 221
I0712 13:43:01.951543 140545052657472 train_utils.py:377] train in step: 222
I0712 13:43:01.983022 140545052657472 train_utils.py:377] train in step: 223
I0712 13:43:02.012793 140545052657472 train_utils.py:377] train in step: 224
I0712 13:43:02.042963 140545052657472 train_utils.py:377] train in step: 225
I0712 13:43:02.073253 140545052657472 train_utils.py:377] train in step: 226
I0712 13:43:02.104318 140545052657472 train_utils.py:377] train in step: 227
I0712 13:43:02.134418 140545052657472 train_utils.py:377] train in step: 228
I0712 13:43:02.165122 140545052657472 train_utils.py:377] train in step: 229
I0712 13:43:02.195352 140545052657472 train_utils.py:377] train in step: 230
I0712 13:43:02.225737 140545052657472 train_utils.py:377] train in step: 231
I0712 13:43:02.256604 140545052657472 train_utils.py:377] train in step: 232
I0712 13:43:02.286742 140545052657472 train_utils.py:377] train in step: 233
I0712 13:43:02.320161 140545052657472 train_utils.py:377] train in step: 234
I0712 13:43:02.348976 140545052657472 train_utils.py:377] train in step: 235
I0712 13:43:02.380509 140545052657472 train_utils.py:377] train in step: 236
I0712 13:43:02.410909 140545052657472 train_utils.py:377] train in step: 237
I0712 13:43:02.442355 140545052657472 train_utils.py:377] train in step: 238
I0712 13:43:02.472223 140545052657472 train_utils.py:377] train in step: 239
I0712 13:43:02.510512 140545052657472 train_utils.py:377] train in step: 240
I0712 13:43:02.544580 140545052657472 train_utils.py:377] train in step: 241
I0712 13:43:02.575507 140545052657472 train_utils.py:377] train in step: 242
I0712 13:43:02.605661 140545052657472 train_utils.py:377] train in step: 243
I0712 13:43:02.636110 140545052657472 train_utils.py:377] train in step: 244
I0712 13:43:02.666510 140545052657472 train_utils.py:377] train in step: 245
I0712 13:43:02.697040 140545052657472 train_utils.py:377] train in step: 246
I0712 13:43:02.728123 140545052657472 train_utils.py:377] train in step: 247
I0712 13:43:02.758432 140545052657472 train_utils.py:377] train in step: 248
I0712 13:43:02.788546 140545052657472 train_utils.py:377] train in step: 249
I0712 13:43:02.819077 140545052657472 train_utils.py:377] train in step: 250
I0712 13:43:02.849527 140545052657472 train_utils.py:377] train in step: 251
I0712 13:43:02.881420 140545052657472 train_utils.py:377] train in step: 252
I0712 13:43:02.910793 140545052657472 train_utils.py:377] train in step: 253
I0712 13:43:02.942344 140545052657472 train_utils.py:377] train in step: 254
I0712 13:43:02.972696 140545052657472 train_utils.py:377] train in step: 255
I0712 13:43:03.004028 140545052657472 train_utils.py:377] train in step: 256
I0712 13:43:03.034090 140545052657472 train_utils.py:377] train in step: 257
I0712 13:43:03.063926 140545052657472 train_utils.py:377] train in step: 258
I0712 13:43:03.094367 140545052657472 train_utils.py:377] train in step: 259
I0712 13:43:03.126599 140545052657472 train_utils.py:377] train in step: 260
I0712 13:43:03.156804 140545052657472 train_utils.py:377] train in step: 261
I0712 13:43:03.187782 140545052657472 train_utils.py:377] train in step: 262
I0712 13:43:03.218242 140545052657472 train_utils.py:377] train in step: 263
I0712 13:43:03.248705 140545052657472 train_utils.py:377] train in step: 264
I0712 13:43:03.278696 140545052657472 train_utils.py:377] train in step: 265
I0712 13:43:03.309123 140545052657472 train_utils.py:377] train in step: 266
I0712 13:43:03.339850 140545052657472 train_utils.py:377] train in step: 267
I0712 13:43:03.370293 140545052657472 train_utils.py:377] train in step: 268
I0712 13:43:03.400583 140545052657472 train_utils.py:377] train in step: 269
I0712 13:43:03.431723 140545052657472 train_utils.py:377] train in step: 270
I0712 13:43:03.461479 140545052657472 train_utils.py:377] train in step: 271
I0712 13:43:03.492425 140545052657472 train_utils.py:377] train in step: 272
I0712 13:43:03.522430 140545052657472 train_utils.py:377] train in step: 273
I0712 13:43:03.555282 140545052657472 train_utils.py:377] train in step: 274
I0712 13:43:03.586105 140545052657472 train_utils.py:377] train in step: 275
I0712 13:43:03.615258 140545052657472 train_utils.py:377] train in step: 276
I0712 13:43:03.648770 140545052657472 train_utils.py:377] train in step: 277
I0712 13:43:03.679136 140545052657472 train_utils.py:377] train in step: 278
I0712 13:43:03.708822 140545052657472 train_utils.py:377] train in step: 279
I0712 13:43:03.740762 140545052657472 train_utils.py:377] train in step: 280
I0712 13:43:03.770712 140545052657472 train_utils.py:377] train in step: 281
I0712 13:43:03.800866 140545052657472 train_utils.py:377] train in step: 282
I0712 13:43:03.839840 140545052657472 train_utils.py:377] train in step: 283
I0712 13:43:03.869746 140545052657472 train_utils.py:377] train in step: 284
I0712 13:43:03.906306 140545052657472 train_utils.py:377] train in step: 285
I0712 13:43:03.936589 140545052657472 train_utils.py:377] train in step: 286
I0712 13:43:03.966796 140545052657472 train_utils.py:377] train in step: 287
I0712 13:43:03.997144 140545052657472 train_utils.py:377] train in step: 288
I0712 13:43:04.036685 140545052657472 train_utils.py:377] train in step: 289
I0712 13:43:04.066202 140545052657472 train_utils.py:377] train in step: 290
I0712 13:43:04.100136 140545052657472 train_utils.py:377] train in step: 291
I0712 13:43:04.128236 140545052657472 train_utils.py:377] train in step: 292
I0712 13:43:04.159887 140545052657472 train_utils.py:377] train in step: 293
I0712 13:43:04.197890 140545052657472 train_utils.py:377] train in step: 294
I0712 13:43:04.227275 140545052657472 train_utils.py:377] train in step: 295
I0712 13:43:04.257537 140545052657472 train_utils.py:377] train in step: 296
I0712 13:43:04.288070 140545052657472 train_utils.py:377] train in step: 297
I0712 13:43:04.318453 140545052657472 train_utils.py:377] train in step: 298
I0712 13:43:04.352021 140545052657472 train_utils.py:377] train in step: 299
I0712 13:43:04.380125 140545052657472 train_utils.py:377] train in step: 300
I0712 13:43:04.411008 140545052657472 train_utils.py:377] train in step: 301
I0712 13:43:04.441992 140545052657472 train_utils.py:377] train in step: 302
I0712 13:43:04.473031 140545052657472 train_utils.py:377] train in step: 303
I0712 13:43:04.504306 140545052657472 train_utils.py:377] train in step: 304
I0712 13:43:04.546786 140545052657472 train_utils.py:377] train in step: 305
I0712 13:43:04.578450 140545052657472 train_utils.py:377] train in step: 306
I0712 13:43:04.610674 140545052657472 train_utils.py:377] train in step: 307
I0712 13:43:04.641538 140545052657472 train_utils.py:377] train in step: 308
I0712 13:43:04.674603 140545052657472 train_utils.py:377] train in step: 309
I0712 13:43:04.702845 140545052657472 train_utils.py:377] train in step: 310
I0712 13:43:04.733151 140545052657472 train_utils.py:377] train in step: 311
I0712 13:43:04.764268 140545052657472 train_utils.py:377] train in step: 312
I0712 13:43:04.795921 140545052657472 train_utils.py:377] train in step: 313
I0712 13:43:04.828196 140545052657472 train_utils.py:377] train in step: 314
I0712 13:43:04.859028 140545052657472 train_utils.py:377] train in step: 315
I0712 13:43:04.889056 140545052657472 train_utils.py:377] train in step: 316
I0712 13:43:04.920033 140545052657472 train_utils.py:377] train in step: 317
I0712 13:43:04.950633 140545052657472 train_utils.py:377] train in step: 318
I0712 13:43:04.981346 140545052657472 train_utils.py:377] train in step: 319
I0712 13:43:05.012686 140545052657472 train_utils.py:377] train in step: 320
I0712 13:43:05.043493 140545052657472 train_utils.py:377] train in step: 321
I0712 13:43:05.073858 140545052657472 train_utils.py:377] train in step: 322
I0712 13:43:05.104742 140545052657472 train_utils.py:377] train in step: 323
I0712 13:43:05.135022 140545052657472 train_utils.py:377] train in step: 324
I0712 13:43:05.166447 140545052657472 train_utils.py:377] train in step: 325
I0712 13:43:05.197569 140545052657472 train_utils.py:377] train in step: 326
I0712 13:43:05.228479 140545052657472 train_utils.py:377] train in step: 327
I0712 13:43:05.261239 140545052657472 train_utils.py:377] train in step: 328
I0712 13:43:05.292126 140545052657472 train_utils.py:377] train in step: 329
I0712 13:43:05.325804 140545052657472 train_utils.py:377] train in step: 330
I0712 13:43:05.356248 140545052657472 train_utils.py:377] train in step: 331
I0712 13:43:05.388694 140545052657472 train_utils.py:377] train in step: 332
I0712 13:43:05.418550 140545052657472 train_utils.py:377] train in step: 333
I0712 13:43:05.449180 140545052657472 train_utils.py:377] train in step: 334
I0712 13:43:05.479696 140545052657472 train_utils.py:377] train in step: 335
I0712 13:43:05.510773 140545052657472 train_utils.py:377] train in step: 336
I0712 13:43:05.541104 140545052657472 train_utils.py:377] train in step: 337
I0712 13:43:05.572023 140545052657472 train_utils.py:377] train in step: 338
I0712 13:43:05.603100 140545052657472 train_utils.py:377] train in step: 339
I0712 13:43:05.633796 140545052657472 train_utils.py:377] train in step: 340
I0712 13:43:05.664683 140545052657472 train_utils.py:377] train in step: 341
I0712 13:43:05.695776 140545052657472 train_utils.py:377] train in step: 342
I0712 13:43:05.726001 140545052657472 train_utils.py:377] train in step: 343
I0712 13:43:05.758538 140545052657472 train_utils.py:377] train in step: 344
I0712 13:43:05.790616 140545052657472 train_utils.py:377] train in step: 345
I0712 13:43:05.820085 140545052657472 train_utils.py:377] train in step: 346
I0712 13:43:05.851451 140545052657472 train_utils.py:377] train in step: 347
I0712 13:43:05.882280 140545052657472 train_utils.py:377] train in step: 348
I0712 13:43:05.913246 140545052657472 train_utils.py:377] train in step: 349
I0712 13:43:05.946782 140545052657472 train_utils.py:377] train in step: 350
I0712 13:43:05.979537 140545052657472 train_utils.py:377] train in step: 351
I0712 13:43:06.013999 140545052657472 train_utils.py:377] train in step: 352
I0712 13:43:06.044159 140545052657472 train_utils.py:377] train in step: 353
I0712 13:43:06.075287 140545052657472 train_utils.py:377] train in step: 354
I0712 13:43:06.106289 140545052657472 train_utils.py:377] train in step: 355
I0712 13:43:06.137071 140545052657472 train_utils.py:377] train in step: 356
I0712 13:43:06.167908 140545052657472 train_utils.py:377] train in step: 357
I0712 13:43:06.198654 140545052657472 train_utils.py:377] train in step: 358
I0712 13:43:06.231312 140545052657472 train_utils.py:377] train in step: 359
I0712 13:43:06.261371 140545052657472 train_utils.py:377] train in step: 360
I0712 13:43:06.291537 140545052657472 train_utils.py:377] train in step: 361
I0712 13:43:06.322316 140545052657472 train_utils.py:377] train in step: 362
I0712 13:43:06.353880 140545052657472 train_utils.py:377] train in step: 363
I0712 13:43:06.384173 140545052657472 train_utils.py:377] train in step: 364
I0712 13:43:06.416126 140545052657472 train_utils.py:377] train in step: 365
I0712 13:43:06.445631 140545052657472 train_utils.py:377] train in step: 366
I0712 13:43:06.476553 140545052657472 train_utils.py:377] train in step: 367
I0712 13:43:06.507641 140545052657472 train_utils.py:377] train in step: 368
I0712 13:43:06.539517 140545052657472 train_utils.py:377] train in step: 369
I0712 13:43:06.571525 140545052657472 train_utils.py:377] train in step: 370
I0712 13:43:06.601851 140545052657472 train_utils.py:377] train in step: 371
I0712 13:43:06.634548 140545052657472 train_utils.py:377] train in step: 372
I0712 13:43:06.664702 140545052657472 train_utils.py:377] train in step: 373
I0712 13:43:06.695868 140545052657472 train_utils.py:377] train in step: 374
I0712 13:43:06.728963 140545052657472 train_utils.py:377] train in step: 375
I0712 13:43:06.758978 140545052657472 train_utils.py:377] train in step: 376
I0712 13:43:06.789375 140545052657472 train_utils.py:377] train in step: 377
I0712 13:43:06.820862 140545052657472 train_utils.py:377] train in step: 378
I0712 13:43:06.851716 140545052657472 train_utils.py:377] train in step: 379
I0712 13:43:06.881703 140545052657472 train_utils.py:377] train in step: 380
I0712 13:43:06.912467 140545052657472 train_utils.py:377] train in step: 381
I0712 13:43:06.943436 140545052657472 train_utils.py:377] train in step: 382
I0712 13:43:06.974200 140545052657472 train_utils.py:377] train in step: 383
I0712 13:43:07.005467 140545052657472 train_utils.py:377] train in step: 384
I0712 13:43:07.036309 140545052657472 train_utils.py:377] train in step: 385
I0712 13:43:07.067080 140545052657472 train_utils.py:377] train in step: 386
I0712 13:43:07.098168 140545052657472 train_utils.py:377] train in step: 387
I0712 13:43:07.128778 140545052657472 train_utils.py:377] train in step: 388
I0712 13:43:07.159471 140545052657472 train_utils.py:377] train in step: 389
I0712 13:43:07.190081 140545052657472 train_utils.py:377] train in step: 390
I0712 13:43:07.222476 140545052657472 train_utils.py:377] train in step: 391
I0712 13:43:07.254674 140545052657472 train_utils.py:377] train in step: 392
I0712 13:43:07.284838 140545052657472 train_utils.py:377] train in step: 393
I0712 13:43:07.316069 140545052657472 train_utils.py:377] train in step: 394
I0712 13:43:07.346761 140545052657472 train_utils.py:377] train in step: 395
I0712 13:43:07.378051 140545052657472 train_utils.py:377] train in step: 396
I0712 13:43:07.409242 140545052657472 train_utils.py:377] train in step: 397
I0712 13:43:07.440090 140545052657472 train_utils.py:377] train in step: 398
I0712 13:43:07.471034 140545052657472 train_utils.py:377] train in step: 399
I0712 13:43:07.502235 140545052657472 train_utils.py:377] train in step: 400
I0712 13:43:07.751682 140545052657472 train_utils.py:396] train in step: 400, loss: 0.7342999577522278, acc: 0.5199999809265137
I0712 13:43:10.852439 140545052657472 train_utils.py:411] eval in step: 400, loss: 0.7345, acc: 0.4400
I0712 13:43:10.858676 140545052657472 train_utils.py:421] Testing...
I0712 13:43:14.104899 140545052657472 train_utils.py:424] test in step: 400, loss: 0.7059, acc: 0.5100
I0712 13:43:14.143687 140545052657472 train_utils.py:377] train in step: 401
I0712 13:43:14.167510 140545052657472 train_utils.py:377] train in step: 402
I0712 13:43:14.187622 140545052657472 train_utils.py:377] train in step: 403
I0712 13:43:14.209179 140545052657472 train_utils.py:377] train in step: 404
I0712 13:43:14.231984 140545052657472 train_utils.py:377] train in step: 405
I0712 13:43:14.260703 140545052657472 train_utils.py:377] train in step: 406
I0712 13:43:14.298876 140545052657472 train_utils.py:377] train in step: 407
I0712 13:43:14.339855 140545052657472 train_utils.py:377] train in step: 408
I0712 13:43:14.373540 140545052657472 train_utils.py:377] train in step: 409
I0712 13:43:14.402831 140545052657472 train_utils.py:377] train in step: 410
I0712 13:43:14.434504 140545052657472 train_utils.py:377] train in step: 411
I0712 13:43:14.465304 140545052657472 train_utils.py:377] train in step: 412
I0712 13:43:14.495682 140545052657472 train_utils.py:377] train in step: 413
I0712 13:43:14.531828 140545052657472 train_utils.py:377] train in step: 414
I0712 13:43:14.569590 140545052657472 train_utils.py:377] train in step: 415
I0712 13:43:14.603947 140545052657472 train_utils.py:377] train in step: 416
I0712 13:43:14.639515 140545052657472 train_utils.py:377] train in step: 417
I0712 13:43:14.671439 140545052657472 train_utils.py:377] train in step: 418
I0712 13:43:14.702360 140545052657472 train_utils.py:377] train in step: 419
I0712 13:43:14.733544 140545052657472 train_utils.py:377] train in step: 420
I0712 13:43:14.763190 140545052657472 train_utils.py:377] train in step: 421
I0712 13:43:14.793640 140545052657472 train_utils.py:377] train in step: 422
I0712 13:43:14.825240 140545052657472 train_utils.py:377] train in step: 423
I0712 13:43:14.857181 140545052657472 train_utils.py:377] train in step: 424
I0712 13:43:14.887748 140545052657472 train_utils.py:377] train in step: 425
I0712 13:43:14.918654 140545052657472 train_utils.py:377] train in step: 426
I0712 13:43:14.950448 140545052657472 train_utils.py:377] train in step: 427
I0712 13:43:14.982083 140545052657472 train_utils.py:377] train in step: 428
I0712 13:43:15.012673 140545052657472 train_utils.py:377] train in step: 429
I0712 13:43:15.047726 140545052657472 train_utils.py:377] train in step: 430
I0712 13:43:15.077579 140545052657472 train_utils.py:377] train in step: 431
I0712 13:43:15.107588 140545052657472 train_utils.py:377] train in step: 432
I0712 13:43:15.138295 140545052657472 train_utils.py:377] train in step: 433
I0712 13:43:15.169804 140545052657472 train_utils.py:377] train in step: 434
I0712 13:43:15.201773 140545052657472 train_utils.py:377] train in step: 435
I0712 13:43:15.235416 140545052657472 train_utils.py:377] train in step: 436
I0712 13:43:15.271644 140545052657472 train_utils.py:377] train in step: 437
I0712 13:43:15.303719 140545052657472 train_utils.py:377] train in step: 438
I0712 13:43:15.337825 140545052657472 train_utils.py:377] train in step: 439
I0712 13:43:15.367766 140545052657472 train_utils.py:377] train in step: 440
I0712 13:43:15.398027 140545052657472 train_utils.py:377] train in step: 441
I0712 13:43:15.429763 140545052657472 train_utils.py:377] train in step: 442
I0712 13:43:15.460216 140545052657472 train_utils.py:377] train in step: 443
I0712 13:43:15.492623 140545052657472 train_utils.py:377] train in step: 444
I0712 13:43:15.522290 140545052657472 train_utils.py:377] train in step: 445
I0712 13:43:15.552974 140545052657472 train_utils.py:377] train in step: 446
I0712 13:43:15.583836 140545052657472 train_utils.py:377] train in step: 447
I0712 13:43:15.615275 140545052657472 train_utils.py:377] train in step: 448
I0712 13:43:15.644951 140545052657472 train_utils.py:377] train in step: 449
I0712 13:43:15.675940 140545052657472 train_utils.py:377] train in step: 450
I0712 13:43:15.706968 140545052657472 train_utils.py:377] train in step: 451
I0712 13:43:15.737659 140545052657472 train_utils.py:377] train in step: 452
I0712 13:43:15.768619 140545052657472 train_utils.py:377] train in step: 453
I0712 13:43:15.798941 140545052657472 train_utils.py:377] train in step: 454
I0712 13:43:15.830901 140545052657472 train_utils.py:377] train in step: 455
I0712 13:43:15.861135 140545052657472 train_utils.py:377] train in step: 456
I0712 13:43:15.891859 140545052657472 train_utils.py:377] train in step: 457
I0712 13:43:15.923027 140545052657472 train_utils.py:377] train in step: 458
I0712 13:43:15.954762 140545052657472 train_utils.py:377] train in step: 459
I0712 13:43:15.989684 140545052657472 train_utils.py:377] train in step: 460
I0712 13:43:16.020564 140545052657472 train_utils.py:377] train in step: 461
I0712 13:43:16.050501 140545052657472 train_utils.py:377] train in step: 462
I0712 13:43:16.080776 140545052657472 train_utils.py:377] train in step: 463
I0712 13:43:16.111934 140545052657472 train_utils.py:377] train in step: 464
I0712 13:43:16.142064 140545052657472 train_utils.py:377] train in step: 465
I0712 13:43:16.172636 140545052657472 train_utils.py:377] train in step: 466
I0712 13:43:16.204792 140545052657472 train_utils.py:377] train in step: 467
I0712 13:43:16.237122 140545052657472 train_utils.py:377] train in step: 468
I0712 13:43:16.267118 140545052657472 train_utils.py:377] train in step: 469
I0712 13:43:16.298356 140545052657472 train_utils.py:377] train in step: 470
I0712 13:43:16.329369 140545052657472 train_utils.py:377] train in step: 471
I0712 13:43:16.360310 140545052657472 train_utils.py:377] train in step: 472
I0712 13:43:16.391159 140545052657472 train_utils.py:377] train in step: 473
I0712 13:43:16.421460 140545052657472 train_utils.py:377] train in step: 474
I0712 13:43:16.452543 140545052657472 train_utils.py:377] train in step: 475
I0712 13:43:16.483644 140545052657472 train_utils.py:377] train in step: 476
I0712 13:43:16.515188 140545052657472 train_utils.py:377] train in step: 477
I0712 13:43:16.546903 140545052657472 train_utils.py:377] train in step: 478
I0712 13:43:16.579106 140545052657472 train_utils.py:377] train in step: 479
I0712 13:43:16.615263 140545052657472 train_utils.py:377] train in step: 480
I0712 13:43:16.650204 140545052657472 train_utils.py:377] train in step: 481
I0712 13:43:16.680799 140545052657472 train_utils.py:377] train in step: 482
I0712 13:43:16.710916 140545052657472 train_utils.py:377] train in step: 483
I0712 13:43:16.741110 140545052657472 train_utils.py:377] train in step: 484
I0712 13:43:16.772066 140545052657472 train_utils.py:377] train in step: 485
I0712 13:43:16.802664 140545052657472 train_utils.py:377] train in step: 486
I0712 13:43:16.833667 140545052657472 train_utils.py:377] train in step: 487
I0712 13:43:16.866587 140545052657472 train_utils.py:377] train in step: 488
I0712 13:43:16.898215 140545052657472 train_utils.py:377] train in step: 489
I0712 13:43:16.928876 140545052657472 train_utils.py:377] train in step: 490
I0712 13:43:16.959664 140545052657472 train_utils.py:377] train in step: 491
I0712 13:43:16.990264 140545052657472 train_utils.py:377] train in step: 492
I0712 13:43:17.021637 140545052657472 train_utils.py:377] train in step: 493
I0712 13:43:17.052298 140545052657472 train_utils.py:377] train in step: 494
I0712 13:43:17.083322 140545052657472 train_utils.py:377] train in step: 495
I0712 13:43:17.113682 140545052657472 train_utils.py:377] train in step: 496
I0712 13:43:17.144515 140545052657472 train_utils.py:377] train in step: 497
I0712 13:43:17.176199 140545052657472 train_utils.py:377] train in step: 498
I0712 13:43:17.208267 140545052657472 train_utils.py:377] train in step: 499
I0712 13:43:17.240579 140545052657472 train_utils.py:377] train in step: 500
I0712 13:43:17.278833 140545052657472 train_utils.py:377] train in step: 501
I0712 13:43:17.312783 140545052657472 train_utils.py:377] train in step: 502
I0712 13:43:17.343896 140545052657472 train_utils.py:377] train in step: 503
I0712 13:43:17.374428 140545052657472 train_utils.py:377] train in step: 504
I0712 13:43:17.404567 140545052657472 train_utils.py:377] train in step: 505
I0712 13:43:17.435124 140545052657472 train_utils.py:377] train in step: 506
I0712 13:43:17.465873 140545052657472 train_utils.py:377] train in step: 507
I0712 13:43:17.497390 140545052657472 train_utils.py:377] train in step: 508
I0712 13:43:17.528410 140545052657472 train_utils.py:377] train in step: 509
I0712 13:43:17.559962 140545052657472 train_utils.py:377] train in step: 510
I0712 13:43:17.589969 140545052657472 train_utils.py:377] train in step: 511
I0712 13:43:17.620761 140545052657472 train_utils.py:377] train in step: 512
I0712 13:43:17.651587 140545052657472 train_utils.py:377] train in step: 513
I0712 13:43:17.682163 140545052657472 train_utils.py:377] train in step: 514
I0712 13:43:17.712617 140545052657472 train_utils.py:377] train in step: 515
I0712 13:43:17.743468 140545052657472 train_utils.py:377] train in step: 516
I0712 13:43:17.774517 140545052657472 train_utils.py:377] train in step: 517
I0712 13:43:17.805339 140545052657472 train_utils.py:377] train in step: 518
I0712 13:43:17.836539 140545052657472 train_utils.py:377] train in step: 519
I0712 13:43:17.867349 140545052657472 train_utils.py:377] train in step: 520
I0712 13:43:17.897300 140545052657472 train_utils.py:377] train in step: 521
I0712 13:43:17.930481 140545052657472 train_utils.py:377] train in step: 522
I0712 13:43:17.964540 140545052657472 train_utils.py:377] train in step: 523
I0712 13:43:17.994503 140545052657472 train_utils.py:377] train in step: 524
I0712 13:43:18.025670 140545052657472 train_utils.py:377] train in step: 525
I0712 13:43:18.055933 140545052657472 train_utils.py:377] train in step: 526
I0712 13:43:18.087319 140545052657472 train_utils.py:377] train in step: 527
I0712 13:43:18.117691 140545052657472 train_utils.py:377] train in step: 528
I0712 13:43:18.148583 140545052657472 train_utils.py:377] train in step: 529
I0712 13:43:18.180966 140545052657472 train_utils.py:377] train in step: 530
I0712 13:43:18.210718 140545052657472 train_utils.py:377] train in step: 531
I0712 13:43:18.242059 140545052657472 train_utils.py:377] train in step: 532
I0712 13:43:18.272154 140545052657472 train_utils.py:377] train in step: 533
I0712 13:43:18.302916 140545052657472 train_utils.py:377] train in step: 534
I0712 13:43:18.334319 140545052657472 train_utils.py:377] train in step: 535
I0712 13:43:18.365056 140545052657472 train_utils.py:377] train in step: 536
I0712 13:43:18.395354 140545052657472 train_utils.py:377] train in step: 537
I0712 13:43:18.425851 140545052657472 train_utils.py:377] train in step: 538
I0712 13:43:18.460001 140545052657472 train_utils.py:377] train in step: 539
I0712 13:43:18.487940 140545052657472 train_utils.py:377] train in step: 540
I0712 13:43:18.518478 140545052657472 train_utils.py:377] train in step: 541
I0712 13:43:18.550804 140545052657472 train_utils.py:377] train in step: 542
I0712 13:43:18.582472 140545052657472 train_utils.py:377] train in step: 543
I0712 13:43:18.622084 140545052657472 train_utils.py:377] train in step: 544
I0712 13:43:18.653659 140545052657472 train_utils.py:377] train in step: 545
I0712 13:43:18.685292 140545052657472 train_utils.py:377] train in step: 546
I0712 13:43:18.715940 140545052657472 train_utils.py:377] train in step: 547
I0712 13:43:18.746790 140545052657472 train_utils.py:377] train in step: 548
I0712 13:43:18.777557 140545052657472 train_utils.py:377] train in step: 549
I0712 13:43:18.808591 140545052657472 train_utils.py:377] train in step: 550
I0712 13:43:18.839472 140545052657472 train_utils.py:377] train in step: 551
I0712 13:43:18.870149 140545052657472 train_utils.py:377] train in step: 552
I0712 13:43:18.900505 140545052657472 train_utils.py:377] train in step: 553
I0712 13:43:18.931438 140545052657472 train_utils.py:377] train in step: 554
I0712 13:43:18.962160 140545052657472 train_utils.py:377] train in step: 555
I0712 13:43:18.992986 140545052657472 train_utils.py:377] train in step: 556
I0712 13:43:19.025098 140545052657472 train_utils.py:377] train in step: 557
I0712 13:43:19.054761 140545052657472 train_utils.py:377] train in step: 558
I0712 13:43:19.087609 140545052657472 train_utils.py:377] train in step: 559
I0712 13:43:19.116155 140545052657472 train_utils.py:377] train in step: 560
I0712 13:43:19.147000 140545052657472 train_utils.py:377] train in step: 561
I0712 13:43:19.177812 140545052657472 train_utils.py:377] train in step: 562
I0712 13:43:19.208607 140545052657472 train_utils.py:377] train in step: 563
I0712 13:43:19.239254 140545052657472 train_utils.py:377] train in step: 564
I0712 13:43:19.272674 140545052657472 train_utils.py:377] train in step: 565
I0712 13:43:19.306777 140545052657472 train_utils.py:377] train in step: 566
I0712 13:43:19.337509 140545052657472 train_utils.py:377] train in step: 567
I0712 13:43:19.368435 140545052657472 train_utils.py:377] train in step: 568
I0712 13:43:19.398831 140545052657472 train_utils.py:377] train in step: 569
I0712 13:43:19.429562 140545052657472 train_utils.py:377] train in step: 570
I0712 13:43:19.460262 140545052657472 train_utils.py:377] train in step: 571
I0712 13:43:19.491809 140545052657472 train_utils.py:377] train in step: 572
I0712 13:43:19.522642 140545052657472 train_utils.py:377] train in step: 573
I0712 13:43:19.554959 140545052657472 train_utils.py:377] train in step: 574
I0712 13:43:19.586354 140545052657472 train_utils.py:377] train in step: 575
I0712 13:43:19.615379 140545052657472 train_utils.py:377] train in step: 576
I0712 13:43:19.646221 140545052657472 train_utils.py:377] train in step: 577
I0712 13:43:19.677814 140545052657472 train_utils.py:377] train in step: 578
I0712 13:43:19.708017 140545052657472 train_utils.py:377] train in step: 579
I0712 13:43:19.739480 140545052657472 train_utils.py:377] train in step: 580
I0712 13:43:19.770098 140545052657472 train_utils.py:377] train in step: 581
I0712 13:43:19.801719 140545052657472 train_utils.py:377] train in step: 582
I0712 13:43:19.833554 140545052657472 train_utils.py:377] train in step: 583
I0712 13:43:19.864806 140545052657472 train_utils.py:377] train in step: 584
I0712 13:43:19.896522 140545052657472 train_utils.py:377] train in step: 585
I0712 13:43:19.926434 140545052657472 train_utils.py:377] train in step: 586
I0712 13:43:19.958120 140545052657472 train_utils.py:377] train in step: 587
I0712 13:43:19.989133 140545052657472 train_utils.py:377] train in step: 588
I0712 13:43:20.019959 140545052657472 train_utils.py:377] train in step: 589
I0712 13:43:20.050786 140545052657472 train_utils.py:377] train in step: 590
I0712 13:43:20.083070 140545052657472 train_utils.py:377] train in step: 591
I0712 13:43:20.112929 140545052657472 train_utils.py:377] train in step: 592
I0712 13:43:20.142878 140545052657472 train_utils.py:377] train in step: 593
I0712 13:43:20.175078 140545052657472 train_utils.py:377] train in step: 594
I0712 13:43:20.207572 140545052657472 train_utils.py:377] train in step: 595
I0712 13:43:20.236803 140545052657472 train_utils.py:377] train in step: 596
I0712 13:43:20.267871 140545052657472 train_utils.py:377] train in step: 597
I0712 13:43:20.298252 140545052657472 train_utils.py:377] train in step: 598
I0712 13:43:20.329273 140545052657472 train_utils.py:377] train in step: 599
I0712 13:43:20.360638 140545052657472 train_utils.py:377] train in step: 600
I0712 13:43:20.430389 140545052657472 train_utils.py:396] train in step: 600, loss: 0.7098000049591064, acc: 0.5238000154495239
I0712 13:43:23.544524 140545052657472 train_utils.py:411] eval in step: 600, loss: 0.7031, acc: 0.5300
I0712 13:43:23.548086 140545052657472 train_utils.py:421] Testing...
I0712 13:43:26.583419 140545052657472 train_utils.py:424] test in step: 600, loss: 0.7258, acc: 0.5350
I0712 13:43:26.612570 140545052657472 train_utils.py:377] train in step: 601
I0712 13:43:26.637644 140545052657472 train_utils.py:377] train in step: 602
I0712 13:43:26.659601 140545052657472 train_utils.py:377] train in step: 603
I0712 13:43:26.678023 140545052657472 train_utils.py:377] train in step: 604
I0712 13:43:26.700981 140545052657472 train_utils.py:377] train in step: 605
I0712 13:43:26.740185 140545052657472 train_utils.py:377] train in step: 606
I0712 13:43:26.771357 140545052657472 train_utils.py:377] train in step: 607
I0712 13:43:26.803715 140545052657472 train_utils.py:377] train in step: 608
I0712 13:43:26.833023 140545052657472 train_utils.py:377] train in step: 609
I0712 13:43:26.865094 140545052657472 train_utils.py:377] train in step: 610
I0712 13:43:26.894818 140545052657472 train_utils.py:377] train in step: 611
I0712 13:43:26.925427 140545052657472 train_utils.py:377] train in step: 612
I0712 13:43:26.956481 140545052657472 train_utils.py:377] train in step: 613
I0712 13:43:26.986610 140545052657472 train_utils.py:377] train in step: 614
I0712 13:43:27.017853 140545052657472 train_utils.py:377] train in step: 615
I0712 13:43:27.048769 140545052657472 train_utils.py:377] train in step: 616
I0712 13:43:27.079058 140545052657472 train_utils.py:377] train in step: 617
I0712 13:43:27.109686 140545052657472 train_utils.py:377] train in step: 618
I0712 13:43:27.140751 140545052657472 train_utils.py:377] train in step: 619
I0712 13:43:27.171341 140545052657472 train_utils.py:377] train in step: 620
I0712 13:43:27.202563 140545052657472 train_utils.py:377] train in step: 621
I0712 13:43:27.239511 140545052657472 train_utils.py:377] train in step: 622
I0712 13:43:27.272544 140545052657472 train_utils.py:377] train in step: 623
I0712 13:43:27.303507 140545052657472 train_utils.py:377] train in step: 624
I0712 13:43:27.334017 140545052657472 train_utils.py:377] train in step: 625
I0712 13:43:27.366199 140545052657472 train_utils.py:377] train in step: 626
I0712 13:43:27.397657 140545052657472 train_utils.py:377] train in step: 627
I0712 13:43:27.427947 140545052657472 train_utils.py:377] train in step: 628
I0712 13:43:27.458551 140545052657472 train_utils.py:377] train in step: 629
I0712 13:43:27.489164 140545052657472 train_utils.py:377] train in step: 630
I0712 13:43:27.520119 140545052657472 train_utils.py:377] train in step: 631
I0712 13:43:27.552614 140545052657472 train_utils.py:377] train in step: 632
I0712 13:43:27.582368 140545052657472 train_utils.py:377] train in step: 633
I0712 13:43:27.613141 140545052657472 train_utils.py:377] train in step: 634
I0712 13:43:27.644020 140545052657472 train_utils.py:377] train in step: 635
I0712 13:43:27.674626 140545052657472 train_utils.py:377] train in step: 636
I0712 13:43:27.705945 140545052657472 train_utils.py:377] train in step: 637
I0712 13:43:27.736716 140545052657472 train_utils.py:377] train in step: 638
I0712 13:43:27.767919 140545052657472 train_utils.py:377] train in step: 639
I0712 13:43:27.798052 140545052657472 train_utils.py:377] train in step: 640
I0712 13:43:27.829223 140545052657472 train_utils.py:377] train in step: 641
I0712 13:43:27.860754 140545052657472 train_utils.py:377] train in step: 642
I0712 13:43:27.891321 140545052657472 train_utils.py:377] train in step: 643
I0712 13:43:27.921673 140545052657472 train_utils.py:377] train in step: 644
I0712 13:43:27.952293 140545052657472 train_utils.py:377] train in step: 645
I0712 13:43:27.983677 140545052657472 train_utils.py:377] train in step: 646
I0712 13:43:28.015128 140545052657472 train_utils.py:377] train in step: 647
I0712 13:43:28.045385 140545052657472 train_utils.py:377] train in step: 648
I0712 13:43:28.075980 140545052657472 train_utils.py:377] train in step: 649
I0712 13:43:28.106666 140545052657472 train_utils.py:377] train in step: 650
I0712 13:43:28.137353 140545052657472 train_utils.py:377] train in step: 651
I0712 13:43:28.168105 140545052657472 train_utils.py:377] train in step: 652
I0712 13:43:28.199133 140545052657472 train_utils.py:377] train in step: 653
I0712 13:43:28.229800 140545052657472 train_utils.py:377] train in step: 654
I0712 13:43:28.260715 140545052657472 train_utils.py:377] train in step: 655
I0712 13:43:28.291484 140545052657472 train_utils.py:377] train in step: 656
I0712 13:43:28.322346 140545052657472 train_utils.py:377] train in step: 657
I0712 13:43:28.354269 140545052657472 train_utils.py:377] train in step: 658
I0712 13:43:28.384457 140545052657472 train_utils.py:377] train in step: 659
I0712 13:43:28.415338 140545052657472 train_utils.py:377] train in step: 660
I0712 13:43:28.445916 140545052657472 train_utils.py:377] train in step: 661
I0712 13:43:28.476993 140545052657472 train_utils.py:377] train in step: 662
I0712 13:43:28.507542 140545052657472 train_utils.py:377] train in step: 663
I0712 13:43:28.538364 140545052657472 train_utils.py:377] train in step: 664
I0712 13:43:28.569778 140545052657472 train_utils.py:377] train in step: 665
I0712 13:43:28.600474 140545052657472 train_utils.py:377] train in step: 666
I0712 13:43:28.636946 140545052657472 train_utils.py:377] train in step: 667
I0712 13:43:28.668797 140545052657472 train_utils.py:377] train in step: 668
I0712 13:43:28.700266 140545052657472 train_utils.py:377] train in step: 669
I0712 13:43:28.730823 140545052657472 train_utils.py:377] train in step: 670
I0712 13:43:28.761741 140545052657472 train_utils.py:377] train in step: 671
I0712 13:43:28.792391 140545052657472 train_utils.py:377] train in step: 672
I0712 13:43:28.823879 140545052657472 train_utils.py:377] train in step: 673
I0712 13:43:28.855806 140545052657472 train_utils.py:377] train in step: 674
I0712 13:43:28.886502 140545052657472 train_utils.py:377] train in step: 675
I0712 13:43:28.919167 140545052657472 train_utils.py:377] train in step: 676
I0712 13:43:28.949071 140545052657472 train_utils.py:377] train in step: 677
I0712 13:43:28.981926 140545052657472 train_utils.py:377] train in step: 678
I0712 13:43:29.012202 140545052657472 train_utils.py:377] train in step: 679
I0712 13:43:29.044088 140545052657472 train_utils.py:377] train in step: 680
I0712 13:43:29.073757 140545052657472 train_utils.py:377] train in step: 681
I0712 13:43:29.104351 140545052657472 train_utils.py:377] train in step: 682
I0712 13:43:29.135306 140545052657472 train_utils.py:377] train in step: 683
I0712 13:43:29.166882 140545052657472 train_utils.py:377] train in step: 684
I0712 13:43:29.197869 140545052657472 train_utils.py:377] train in step: 685
I0712 13:43:29.228494 140545052657472 train_utils.py:377] train in step: 686
I0712 13:43:29.259627 140545052657472 train_utils.py:377] train in step: 687
I0712 13:43:29.324277 140545052657472 train_utils.py:377] train in step: 688
I0712 13:43:29.356535 140545052657472 train_utils.py:377] train in step: 689
I0712 13:43:29.388129 140545052657472 train_utils.py:377] train in step: 690
I0712 13:43:29.418982 140545052657472 train_utils.py:377] train in step: 691
I0712 13:43:29.450053 140545052657472 train_utils.py:377] train in step: 692
I0712 13:43:29.479489 140545052657472 train_utils.py:377] train in step: 693
I0712 13:43:29.509505 140545052657472 train_utils.py:377] train in step: 694
I0712 13:43:29.541962 140545052657472 train_utils.py:377] train in step: 695
I0712 13:43:29.571990 140545052657472 train_utils.py:377] train in step: 696
I0712 13:43:29.602435 140545052657472 train_utils.py:377] train in step: 697
I0712 13:43:29.632623 140545052657472 train_utils.py:377] train in step: 698
I0712 13:43:29.663120 140545052657472 train_utils.py:377] train in step: 699
I0712 13:43:29.694679 140545052657472 train_utils.py:377] train in step: 700
I0712 13:43:29.727200 140545052657472 train_utils.py:377] train in step: 701
I0712 13:43:29.757501 140545052657472 train_utils.py:377] train in step: 702
I0712 13:43:29.789804 140545052657472 train_utils.py:377] train in step: 703
I0712 13:43:29.820225 140545052657472 train_utils.py:377] train in step: 704
I0712 13:43:29.850863 140545052657472 train_utils.py:377] train in step: 705
I0712 13:43:29.882928 140545052657472 train_utils.py:377] train in step: 706
I0712 13:43:29.913016 140545052657472 train_utils.py:377] train in step: 707
I0712 13:43:29.944920 140545052657472 train_utils.py:377] train in step: 708
I0712 13:43:29.974471 140545052657472 train_utils.py:377] train in step: 709
I0712 13:43:30.005387 140545052657472 train_utils.py:377] train in step: 710
I0712 13:43:30.036311 140545052657472 train_utils.py:377] train in step: 711
I0712 13:43:30.068435 140545052657472 train_utils.py:377] train in step: 712
I0712 13:43:30.099300 140545052657472 train_utils.py:377] train in step: 713
I0712 13:43:30.129563 140545052657472 train_utils.py:377] train in step: 714
I0712 13:43:30.161204 140545052657472 train_utils.py:377] train in step: 715
I0712 13:43:30.191136 140545052657472 train_utils.py:377] train in step: 716
I0712 13:43:30.222777 140545052657472 train_utils.py:377] train in step: 717
I0712 13:43:30.252927 140545052657472 train_utils.py:377] train in step: 718
I0712 13:43:30.283897 140545052657472 train_utils.py:377] train in step: 719
I0712 13:43:30.315212 140545052657472 train_utils.py:377] train in step: 720
I0712 13:43:30.345312 140545052657472 train_utils.py:377] train in step: 721
I0712 13:43:30.375810 140545052657472 train_utils.py:377] train in step: 722
I0712 13:43:30.406578 140545052657472 train_utils.py:377] train in step: 723
I0712 13:43:30.437436 140545052657472 train_utils.py:377] train in step: 724
I0712 13:43:30.467980 140545052657472 train_utils.py:377] train in step: 725
I0712 13:43:30.499332 140545052657472 train_utils.py:377] train in step: 726
I0712 13:43:30.529732 140545052657472 train_utils.py:377] train in step: 727
I0712 13:43:30.560336 140545052657472 train_utils.py:377] train in step: 728
I0712 13:43:30.592757 140545052657472 train_utils.py:377] train in step: 729
I0712 13:43:30.623261 140545052657472 train_utils.py:377] train in step: 730
I0712 13:43:30.655565 140545052657472 train_utils.py:377] train in step: 731
I0712 13:43:30.685752 140545052657472 train_utils.py:377] train in step: 732
I0712 13:43:30.718363 140545052657472 train_utils.py:377] train in step: 733
I0712 13:43:30.748462 140545052657472 train_utils.py:377] train in step: 734
I0712 13:43:30.781187 140545052657472 train_utils.py:377] train in step: 735
I0712 13:43:30.810275 140545052657472 train_utils.py:377] train in step: 736
I0712 13:43:30.841014 140545052657472 train_utils.py:377] train in step: 737
I0712 13:43:30.871587 140545052657472 train_utils.py:377] train in step: 738
I0712 13:43:30.902532 140545052657472 train_utils.py:377] train in step: 739
I0712 13:43:30.934781 140545052657472 train_utils.py:377] train in step: 740
I0712 13:43:30.965401 140545052657472 train_utils.py:377] train in step: 741
I0712 13:43:30.995430 140545052657472 train_utils.py:377] train in step: 742
I0712 13:43:31.027162 140545052657472 train_utils.py:377] train in step: 743
I0712 13:43:31.057211 140545052657472 train_utils.py:377] train in step: 744
I0712 13:43:31.088208 140545052657472 train_utils.py:377] train in step: 745
I0712 13:43:31.118946 140545052657472 train_utils.py:377] train in step: 746
I0712 13:43:31.149515 140545052657472 train_utils.py:377] train in step: 747
I0712 13:43:31.180372 140545052657472 train_utils.py:377] train in step: 748
I0712 13:43:31.211730 140545052657472 train_utils.py:377] train in step: 749
I0712 13:43:31.242444 140545052657472 train_utils.py:377] train in step: 750
I0712 13:43:31.272840 140545052657472 train_utils.py:377] train in step: 751
I0712 13:43:31.303854 140545052657472 train_utils.py:377] train in step: 752
I0712 13:43:31.335536 140545052657472 train_utils.py:377] train in step: 753
I0712 13:43:31.367733 140545052657472 train_utils.py:377] train in step: 754
I0712 13:43:31.399750 140545052657472 train_utils.py:377] train in step: 755
I0712 13:43:31.430881 140545052657472 train_utils.py:377] train in step: 756
I0712 13:43:31.461149 140545052657472 train_utils.py:377] train in step: 757
I0712 13:43:31.491974 140545052657472 train_utils.py:377] train in step: 758
I0712 13:43:31.522514 140545052657472 train_utils.py:377] train in step: 759
I0712 13:43:31.553758 140545052657472 train_utils.py:377] train in step: 760
I0712 13:43:31.584401 140545052657472 train_utils.py:377] train in step: 761
I0712 13:43:31.615268 140545052657472 train_utils.py:377] train in step: 762
I0712 13:43:31.647772 140545052657472 train_utils.py:377] train in step: 763
I0712 13:43:31.678006 140545052657472 train_utils.py:377] train in step: 764
I0712 13:43:31.708904 140545052657472 train_utils.py:377] train in step: 765
I0712 13:43:31.739479 140545052657472 train_utils.py:377] train in step: 766
I0712 13:43:31.771002 140545052657472 train_utils.py:377] train in step: 767
I0712 13:43:31.802353 140545052657472 train_utils.py:377] train in step: 768
I0712 13:43:31.832046 140545052657472 train_utils.py:377] train in step: 769
I0712 13:43:31.862776 140545052657472 train_utils.py:377] train in step: 770
I0712 13:43:31.893559 140545052657472 train_utils.py:377] train in step: 771
I0712 13:43:31.924260 140545052657472 train_utils.py:377] train in step: 772
I0712 13:43:31.957017 140545052657472 train_utils.py:377] train in step: 773
I0712 13:43:31.989320 140545052657472 train_utils.py:377] train in step: 774
I0712 13:43:32.020604 140545052657472 train_utils.py:377] train in step: 775
I0712 13:43:32.051796 140545052657472 train_utils.py:377] train in step: 776
I0712 13:43:32.081680 140545052657472 train_utils.py:377] train in step: 777
I0712 13:43:32.113778 140545052657472 train_utils.py:377] train in step: 778
I0712 13:43:32.143737 140545052657472 train_utils.py:377] train in step: 779
I0712 13:43:32.174345 140545052657472 train_utils.py:377] train in step: 780
I0712 13:43:32.205927 140545052657472 train_utils.py:377] train in step: 781
I0712 13:43:32.237586 140545052657472 train_utils.py:377] train in step: 782
I0712 13:43:32.268674 140545052657472 train_utils.py:377] train in step: 783
I0712 13:43:32.298832 140545052657472 train_utils.py:377] train in step: 784
I0712 13:43:32.330626 140545052657472 train_utils.py:377] train in step: 785
I0712 13:43:32.363111 140545052657472 train_utils.py:377] train in step: 786
I0712 13:43:32.392412 140545052657472 train_utils.py:377] train in step: 787
I0712 13:43:32.422801 140545052657472 train_utils.py:377] train in step: 788
I0712 13:43:32.454292 140545052657472 train_utils.py:377] train in step: 789
I0712 13:43:32.484297 140545052657472 train_utils.py:377] train in step: 790
I0712 13:43:32.515638 140545052657472 train_utils.py:377] train in step: 791
I0712 13:43:32.545785 140545052657472 train_utils.py:377] train in step: 792
I0712 13:43:32.578236 140545052657472 train_utils.py:377] train in step: 793
I0712 13:43:32.610157 140545052657472 train_utils.py:377] train in step: 794
I0712 13:43:32.641907 140545052657472 train_utils.py:377] train in step: 795
I0712 13:43:32.671710 140545052657472 train_utils.py:377] train in step: 796
I0712 13:43:32.703331 140545052657472 train_utils.py:377] train in step: 797
I0712 13:43:32.739242 140545052657472 train_utils.py:377] train in step: 798
I0712 13:43:32.775527 140545052657472 train_utils.py:377] train in step: 799
I0712 13:43:32.805879 140545052657472 train_utils.py:377] train in step: 800
I0712 13:43:32.874035 140545052657472 train_utils.py:396] train in step: 800, loss: 0.7098000049591064, acc: 0.5049999952316284
I0712 13:43:35.974364 140545052657472 train_utils.py:411] eval in step: 800, loss: 0.6964, acc: 0.5100
I0712 13:43:35.978121 140545052657472 train_utils.py:421] Testing...
I0712 13:43:39.117938 140545052657472 train_utils.py:424] test in step: 800, loss: 0.6933, acc: 0.5250
I0712 13:43:39.149547 140545052657472 train_utils.py:377] train in step: 801
I0712 13:43:39.175145 140545052657472 train_utils.py:377] train in step: 802
I0712 13:43:39.196155 140545052657472 train_utils.py:377] train in step: 803
I0712 13:43:39.218890 140545052657472 train_utils.py:377] train in step: 804
I0712 13:43:39.239955 140545052657472 train_utils.py:377] train in step: 805
I0712 13:43:39.266216 140545052657472 train_utils.py:377] train in step: 806
I0712 13:43:39.297349 140545052657472 train_utils.py:377] train in step: 807
I0712 13:43:39.328330 140545052657472 train_utils.py:377] train in step: 808
I0712 13:43:39.366672 140545052657472 train_utils.py:377] train in step: 809
I0712 13:43:39.398303 140545052657472 train_utils.py:377] train in step: 810
I0712 13:43:39.431741 140545052657472 train_utils.py:377] train in step: 811
I0712 13:43:39.463069 140545052657472 train_utils.py:377] train in step: 812
I0712 13:43:39.493488 140545052657472 train_utils.py:377] train in step: 813
I0712 13:43:39.524352 140545052657472 train_utils.py:377] train in step: 814
I0712 13:43:39.555621 140545052657472 train_utils.py:377] train in step: 815
I0712 13:43:39.588205 140545052657472 train_utils.py:377] train in step: 816
I0712 13:43:39.620011 140545052657472 train_utils.py:377] train in step: 817
I0712 13:43:39.649804 140545052657472 train_utils.py:377] train in step: 818
I0712 13:43:39.680976 140545052657472 train_utils.py:377] train in step: 819
I0712 13:43:39.712784 140545052657472 train_utils.py:377] train in step: 820
I0712 13:43:39.743510 140545052657472 train_utils.py:377] train in step: 821
I0712 13:43:39.775107 140545052657472 train_utils.py:377] train in step: 822
I0712 13:43:39.806007 140545052657472 train_utils.py:377] train in step: 823
I0712 13:43:39.836344 140545052657472 train_utils.py:377] train in step: 824
I0712 13:43:39.867196 140545052657472 train_utils.py:377] train in step: 825
I0712 13:43:39.898288 140545052657472 train_utils.py:377] train in step: 826
I0712 13:43:39.928798 140545052657472 train_utils.py:377] train in step: 827
I0712 13:43:39.961495 140545052657472 train_utils.py:377] train in step: 828
I0712 13:43:39.991375 140545052657472 train_utils.py:377] train in step: 829
I0712 13:43:40.022371 140545052657472 train_utils.py:377] train in step: 830
I0712 13:43:40.055064 140545052657472 train_utils.py:377] train in step: 831
I0712 13:43:40.088364 140545052657472 train_utils.py:377] train in step: 832
I0712 13:43:40.119401 140545052657472 train_utils.py:377] train in step: 833
I0712 13:43:40.149838 140545052657472 train_utils.py:377] train in step: 834
I0712 13:43:40.181698 140545052657472 train_utils.py:377] train in step: 835
I0712 13:43:40.212125 140545052657472 train_utils.py:377] train in step: 836
I0712 13:43:40.243124 140545052657472 train_utils.py:377] train in step: 837
I0712 13:43:40.273822 140545052657472 train_utils.py:377] train in step: 838
I0712 13:43:40.304785 140545052657472 train_utils.py:377] train in step: 839
I0712 13:43:40.337030 140545052657472 train_utils.py:377] train in step: 840
I0712 13:43:40.367722 140545052657472 train_utils.py:377] train in step: 841
I0712 13:43:40.398180 140545052657472 train_utils.py:377] train in step: 842
I0712 13:43:40.429379 140545052657472 train_utils.py:377] train in step: 843
I0712 13:43:40.460900 140545052657472 train_utils.py:377] train in step: 844
I0712 13:43:40.491136 140545052657472 train_utils.py:377] train in step: 845
I0712 13:43:40.521673 140545052657472 train_utils.py:377] train in step: 846
I0712 13:43:40.553869 140545052657472 train_utils.py:377] train in step: 847
I0712 13:43:40.584748 140545052657472 train_utils.py:377] train in step: 848
I0712 13:43:40.615610 140545052657472 train_utils.py:377] train in step: 849
I0712 13:43:40.646517 140545052657472 train_utils.py:377] train in step: 850
I0712 13:43:40.678019 140545052657472 train_utils.py:377] train in step: 851
I0712 13:43:40.714683 140545052657472 train_utils.py:377] train in step: 852
I0712 13:43:40.744486 140545052657472 train_utils.py:377] train in step: 853
I0712 13:43:40.777368 140545052657472 train_utils.py:377] train in step: 854
I0712 13:43:40.807181 140545052657472 train_utils.py:377] train in step: 855
I0712 13:43:40.838398 140545052657472 train_utils.py:377] train in step: 856
I0712 13:43:40.869414 140545052657472 train_utils.py:377] train in step: 857
I0712 13:43:40.899797 140545052657472 train_utils.py:377] train in step: 858
I0712 13:43:40.931539 140545052657472 train_utils.py:377] train in step: 859
I0712 13:43:40.961711 140545052657472 train_utils.py:377] train in step: 860
I0712 13:43:40.992987 140545052657472 train_utils.py:377] train in step: 861
I0712 13:43:41.024562 140545052657472 train_utils.py:377] train in step: 862
I0712 13:43:41.055621 140545052657472 train_utils.py:377] train in step: 863
I0712 13:43:41.086765 140545052657472 train_utils.py:377] train in step: 864
I0712 13:43:41.117907 140545052657472 train_utils.py:377] train in step: 865
I0712 13:43:41.148478 140545052657472 train_utils.py:377] train in step: 866
I0712 13:43:41.180334 140545052657472 train_utils.py:377] train in step: 867
I0712 13:43:41.211185 140545052657472 train_utils.py:377] train in step: 868
I0712 13:43:41.241829 140545052657472 train_utils.py:377] train in step: 869
I0712 13:43:41.274560 140545052657472 train_utils.py:377] train in step: 870
I0712 13:43:41.305481 140545052657472 train_utils.py:377] train in step: 871
I0712 13:43:41.336504 140545052657472 train_utils.py:377] train in step: 872
I0712 13:43:41.367413 140545052657472 train_utils.py:377] train in step: 873
I0712 13:43:41.398818 140545052657472 train_utils.py:377] train in step: 874
I0712 13:43:41.429703 140545052657472 train_utils.py:377] train in step: 875
I0712 13:43:41.460539 140545052657472 train_utils.py:377] train in step: 876
I0712 13:43:41.491313 140545052657472 train_utils.py:377] train in step: 877
I0712 13:43:41.522363 140545052657472 train_utils.py:377] train in step: 878
I0712 13:43:41.553936 140545052657472 train_utils.py:377] train in step: 879
I0712 13:43:41.583986 140545052657472 train_utils.py:377] train in step: 880
I0712 13:43:41.613881 140545052657472 train_utils.py:377] train in step: 881
I0712 13:43:41.644886 140545052657472 train_utils.py:377] train in step: 882
I0712 13:43:41.675751 140545052657472 train_utils.py:377] train in step: 883
I0712 13:43:41.706627 140545052657472 train_utils.py:377] train in step: 884
I0712 13:43:41.737732 140545052657472 train_utils.py:377] train in step: 885
I0712 13:43:41.768713 140545052657472 train_utils.py:377] train in step: 886
I0712 13:43:41.800156 140545052657472 train_utils.py:377] train in step: 887
I0712 13:43:41.830083 140545052657472 train_utils.py:377] train in step: 888
I0712 13:43:41.860831 140545052657472 train_utils.py:377] train in step: 889
I0712 13:43:41.891726 140545052657472 train_utils.py:377] train in step: 890
I0712 13:43:41.922659 140545052657472 train_utils.py:377] train in step: 891
I0712 13:43:41.954120 140545052657472 train_utils.py:377] train in step: 892
I0712 13:43:41.983968 140545052657472 train_utils.py:377] train in step: 893
I0712 13:43:42.016034 140545052657472 train_utils.py:377] train in step: 894
I0712 13:43:42.047293 140545052657472 train_utils.py:377] train in step: 895
I0712 13:43:42.078593 140545052657472 train_utils.py:377] train in step: 896
I0712 13:43:42.108565 140545052657472 train_utils.py:377] train in step: 897
I0712 13:43:42.139902 140545052657472 train_utils.py:377] train in step: 898
I0712 13:43:42.170578 140545052657472 train_utils.py:377] train in step: 899
I0712 13:43:42.201570 140545052657472 train_utils.py:377] train in step: 900
I0712 13:43:42.232377 140545052657472 train_utils.py:377] train in step: 901
I0712 13:43:42.262617 140545052657472 train_utils.py:377] train in step: 902
I0712 13:43:42.294161 140545052657472 train_utils.py:377] train in step: 903
I0712 13:43:42.324904 140545052657472 train_utils.py:377] train in step: 904
I0712 13:43:42.355833 140545052657472 train_utils.py:377] train in step: 905
I0712 13:43:42.386577 140545052657472 train_utils.py:377] train in step: 906
I0712 13:43:42.417834 140545052657472 train_utils.py:377] train in step: 907
I0712 13:43:42.447988 140545052657472 train_utils.py:377] train in step: 908
I0712 13:43:42.479266 140545052657472 train_utils.py:377] train in step: 909
I0712 13:43:42.510126 140545052657472 train_utils.py:377] train in step: 910
I0712 13:43:42.541163 140545052657472 train_utils.py:377] train in step: 911
I0712 13:43:42.571973 140545052657472 train_utils.py:377] train in step: 912
I0712 13:43:42.603816 140545052657472 train_utils.py:377] train in step: 913
I0712 13:43:42.634444 140545052657472 train_utils.py:377] train in step: 914
I0712 13:43:42.665056 140545052657472 train_utils.py:377] train in step: 915
I0712 13:43:42.696181 140545052657472 train_utils.py:377] train in step: 916
I0712 13:43:42.726985 140545052657472 train_utils.py:377] train in step: 917
I0712 13:43:42.761620 140545052657472 train_utils.py:377] train in step: 918
I0712 13:43:42.792309 140545052657472 train_utils.py:377] train in step: 919
I0712 13:43:42.823605 140545052657472 train_utils.py:377] train in step: 920
I0712 13:43:42.853557 140545052657472 train_utils.py:377] train in step: 921
I0712 13:43:42.884737 140545052657472 train_utils.py:377] train in step: 922
I0712 13:43:42.915549 140545052657472 train_utils.py:377] train in step: 923
I0712 13:43:42.945789 140545052657472 train_utils.py:377] train in step: 924
I0712 13:43:42.977066 140545052657472 train_utils.py:377] train in step: 925
I0712 13:43:43.008053 140545052657472 train_utils.py:377] train in step: 926
I0712 13:43:43.038855 140545052657472 train_utils.py:377] train in step: 927
I0712 13:43:43.070619 140545052657472 train_utils.py:377] train in step: 928
I0712 13:43:43.100785 140545052657472 train_utils.py:377] train in step: 929
I0712 13:43:43.131058 140545052657472 train_utils.py:377] train in step: 930
I0712 13:43:43.162175 140545052657472 train_utils.py:377] train in step: 931
I0712 13:43:43.193643 140545052657472 train_utils.py:377] train in step: 932
I0712 13:43:43.224007 140545052657472 train_utils.py:377] train in step: 933
I0712 13:43:43.254466 140545052657472 train_utils.py:377] train in step: 934
I0712 13:43:43.286073 140545052657472 train_utils.py:377] train in step: 935
I0712 13:43:43.316899 140545052657472 train_utils.py:377] train in step: 936
I0712 13:43:43.347169 140545052657472 train_utils.py:377] train in step: 937
I0712 13:43:43.378181 140545052657472 train_utils.py:377] train in step: 938
I0712 13:43:43.409184 140545052657472 train_utils.py:377] train in step: 939
I0712 13:43:43.447373 140545052657472 train_utils.py:377] train in step: 940
I0712 13:43:43.477738 140545052657472 train_utils.py:377] train in step: 941
I0712 13:43:43.508415 140545052657472 train_utils.py:377] train in step: 942
I0712 13:43:43.539612 140545052657472 train_utils.py:377] train in step: 943
I0712 13:43:43.570709 140545052657472 train_utils.py:377] train in step: 944
I0712 13:43:43.601130 140545052657472 train_utils.py:377] train in step: 945
I0712 13:43:43.633467 140545052657472 train_utils.py:377] train in step: 946
I0712 13:43:43.663976 140545052657472 train_utils.py:377] train in step: 947
I0712 13:43:43.694871 140545052657472 train_utils.py:377] train in step: 948
I0712 13:43:43.725993 140545052657472 train_utils.py:377] train in step: 949
I0712 13:43:43.756645 140545052657472 train_utils.py:377] train in step: 950
I0712 13:43:43.786744 140545052657472 train_utils.py:377] train in step: 951
I0712 13:43:43.817963 140545052657472 train_utils.py:377] train in step: 952
I0712 13:43:43.848998 140545052657472 train_utils.py:377] train in step: 953
I0712 13:43:43.879101 140545052657472 train_utils.py:377] train in step: 954
I0712 13:43:43.910055 140545052657472 train_utils.py:377] train in step: 955
I0712 13:43:43.940660 140545052657472 train_utils.py:377] train in step: 956
I0712 13:43:43.971588 140545052657472 train_utils.py:377] train in step: 957
I0712 13:43:44.002183 140545052657472 train_utils.py:377] train in step: 958
I0712 13:43:44.033427 140545052657472 train_utils.py:377] train in step: 959
I0712 13:43:44.064172 140545052657472 train_utils.py:377] train in step: 960
I0712 13:43:44.095336 140545052657472 train_utils.py:377] train in step: 961
I0712 13:43:44.131130 140545052657472 train_utils.py:377] train in step: 962
I0712 13:43:44.162122 140545052657472 train_utils.py:377] train in step: 963
I0712 13:43:44.194931 140545052657472 train_utils.py:377] train in step: 964
I0712 13:43:44.224203 140545052657472 train_utils.py:377] train in step: 965
I0712 13:43:44.255326 140545052657472 train_utils.py:377] train in step: 966
I0712 13:43:44.286615 140545052657472 train_utils.py:377] train in step: 967
I0712 13:43:44.317233 140545052657472 train_utils.py:377] train in step: 968
I0712 13:43:44.348655 140545052657472 train_utils.py:377] train in step: 969
I0712 13:43:44.378663 140545052657472 train_utils.py:377] train in step: 970
I0712 13:43:44.409744 140545052657472 train_utils.py:377] train in step: 971
I0712 13:43:44.440013 140545052657472 train_utils.py:377] train in step: 972
I0712 13:43:44.471420 140545052657472 train_utils.py:377] train in step: 973
I0712 13:43:44.501919 140545052657472 train_utils.py:377] train in step: 974
I0712 13:43:44.532801 140545052657472 train_utils.py:377] train in step: 975
I0712 13:43:44.563641 140545052657472 train_utils.py:377] train in step: 976
I0712 13:43:44.595080 140545052657472 train_utils.py:377] train in step: 977
I0712 13:43:44.625468 140545052657472 train_utils.py:377] train in step: 978
I0712 13:43:44.656402 140545052657472 train_utils.py:377] train in step: 979
I0712 13:43:44.686522 140545052657472 train_utils.py:377] train in step: 980
I0712 13:43:44.717447 140545052657472 train_utils.py:377] train in step: 981
I0712 13:43:44.748769 140545052657472 train_utils.py:377] train in step: 982
I0712 13:43:44.779410 140545052657472 train_utils.py:377] train in step: 983
I0712 13:43:44.813882 140545052657472 train_utils.py:377] train in step: 984
I0712 13:43:44.851367 140545052657472 train_utils.py:377] train in step: 985
I0712 13:43:44.882673 140545052657472 train_utils.py:377] train in step: 986
I0712 13:43:44.912463 140545052657472 train_utils.py:377] train in step: 987
I0712 13:43:44.943758 140545052657472 train_utils.py:377] train in step: 988
I0712 13:43:44.976133 140545052657472 train_utils.py:377] train in step: 989
I0712 13:43:45.006776 140545052657472 train_utils.py:377] train in step: 990
I0712 13:43:45.037993 140545052657472 train_utils.py:377] train in step: 991
I0712 13:43:45.068662 140545052657472 train_utils.py:377] train in step: 992
I0712 13:43:45.099376 140545052657472 train_utils.py:377] train in step: 993
I0712 13:43:45.130391 140545052657472 train_utils.py:377] train in step: 994
I0712 13:43:45.160727 140545052657472 train_utils.py:377] train in step: 995
I0712 13:43:45.191779 140545052657472 train_utils.py:377] train in step: 996
I0712 13:43:45.222550 140545052657472 train_utils.py:377] train in step: 997
I0712 13:43:45.253483 140545052657472 train_utils.py:377] train in step: 998
I0712 13:43:45.284443 140545052657472 train_utils.py:377] train in step: 999
I0712 13:43:45.315307 140545052657472 train_utils.py:377] train in step: 1000
I0712 13:43:45.318948 140545052657472 checkpoints.py:120] Saving checkpoint at step: 1000
I0712 13:43:45.439659 140545052657472 checkpoints.py:149] Saved checkpoint at trained_models/matching/performer/checkpoint_1000
I0712 13:43:45.483422 140545052657472 train_utils.py:396] train in step: 1000, loss: 0.7008999586105347, acc: 0.4912000000476837
I0712 13:43:48.571135 140545052657472 train_utils.py:411] eval in step: 1000, loss: 0.6927, acc: 0.5150
I0712 13:43:48.575417 140545052657472 train_utils.py:421] Testing...
I0712 13:43:51.740763 140545052657472 train_utils.py:424] test in step: 1000, loss: 0.6938, acc: 0.4900
